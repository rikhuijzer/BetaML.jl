var documenterSearchIndex = {"docs":
[{"location":"index.html#![BLogos](assets/BetaML_logo_30x30.png)-BetaML.jl-Documentation","page":"Index","title":"(Image: BLogos) BetaML.jl Documentation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Welcome to the documentation of the Beta Machine Learning toolkit.","category":"page"},{"location":"index.html#About","page":"Index","title":"About","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML toolkit provides classical algorithms written in the Julia programming language useful to \"learn\" the relationship between some inputs and some outputs, with the objective to make accurate predictions of the output given new inputs (\"supervised machine learning\") or to better understand the structure of the data, perhaps hidden because of the high dimensionality (\"unsupervised machine learning\").","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"While specific packages exist for state-of-the art implementations of these algorithms (see the section \"Alternative Packages\"), thanks to the Just-In-Time compilation nature of Julia, BetaML is reasonably fast for datasets that fits in memory.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside the algorithms themselves, BetaML provides many \"utility\" functions. Because algorithms are all self-contained in the library itself (you are invited to explore their source code by typing @edit functionOfInterest(par1,par2)), the utility functions have APIs that are coordinated with the algorithms, facilitating the \"preparation\" of the data for the analysis, the evaluation of the models or the implementation of several models in chains (pipelines). While BetaML doesn't provide itself tools for hyper-parameters optimisation or complex pipeline building tools, most models have an interface for the MLJ framework that allows it.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside Julia, BetaML can be accessed in R or Python using respectively JuliaCall and PyJulia. See here for a tutorial or the examples for some actual use of the Beta Machine Learning Toolkit in R/Python.","category":"page"},{"location":"index.html#Installation","page":"Index","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML package is now included in the standard Julia register, install it with:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"] add BetaML","category":"page"},{"location":"index.html#Loading-the-module(s)","page":"Index","title":"Loading the module(s)","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"This package is split in several submodules. You can access its functionality either by using the specific submodule of interest and then directly the provided functionality (utilities are re-exported by each of the other submodules, so normally you don't need to implicitly import them) or by using the root module BetaML and then prefix with BetaML. each object/function you want to use, e.g.:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML.Nn\nmyLayer = DenseLayer(2,3)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"or, equivalently,","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML\nres = BetaML.kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])","category":"page"},{"location":"index.html#Usage","page":"Index","title":"Usage","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Documentation for most algorithms can be retrieved using the inline Julia help system (just press the question mark ? and then, on the special help prompt help?>, type the function name).","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"For a list of supported algorithms please look at the individual modules:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"BetaML.Perceptron: The Perceptron, Kernel Perceptron and Pegasos classification algorithms;\nBetaML.Trees: The Decision Trees and Random Forests algorithms for classification or regression (with missing values supported);\nBetaML.Nn: Implementation of Artificial Neural Networks;\nBetaML.Clustering`: Clustering algorithms (Kmeans, Mdedoids, EM/GMM) and missing imputation / collaborative filtering / recommandation systems using clusters;\nBetaML.Utils`: Various utility functions (scale, one-hot, distances, kernels, pca, accuracy/error measures..).","category":"page"},{"location":"index.html#Examples","page":"Index","title":"Examples","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Using an Artificial Neural Network for multinomial categorisation","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"# Load Modules\nusing BetaML.Nn, BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\ny_oh     = oneHotEncoder(y) # Convert to One-hot representation (e.g. 2 => [0 1 0], 3 => [0 0 1])\n\n# Split the data in training/testing sets\n((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) = Utils.partition([x,y,y_oh],[0.8,0.2],shuffle=false)\n(ytrain,ytest)  = dropdims.([ytrain,ytest],dims=2)\n(ntrain, ntest) = size.([xtrain,xtest],1)\n\n# Define the Artificial Neural Network model\nl1   = DenseLayer(4,10,f=relu) # Activation function is ReLU\nl2   = DenseLayer(10,3)        # Activation function is identity by default\nl3   = VectorFunctionLayer(3,3,f=softMax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once\nmynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Multinomial logistic regression Model Sepal\") # Build the NN and use the squared cost (aka MSE) as error function\n\n# Training it (default to ADAM)\nres = train!(mynn,scale(xtrain),ytrain_oh,epochs=100,batchSize=6) # Use optAlg=SGD (Stochastic Gradient Descent) by default\n\n# Test it\nŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function\nŷtest         = predict(mynn,scale(xtest))\ntrainAccuracy = accuracy(ŷtrain,ytrain,tol=1) # 0.983\ntestAccuracy  = accuracy(ŷtest,ytest,tol=1)   # 1.0\n\n# Visualise results\ntestSize = size(ŷtest,1)\nŷtestChosen =  [argmax(ŷtest[i,:]) for i in 1:testSize]\ngroupedbar([ytest ŷtestChosen], label=[\"ytest\" \"ŷtest (est)\"], title=\"True vs estimated categories\") # All records correctly labelled !\nplot(0:res.epochs,res.ϵ_epochs, ylabel=\"epochs\",xlabel=\"error\",legend=nothing,title=\"Avg. error per epoch on the Sepal dataset\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Using the Expectation-Maximisation algorithm for clustering","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML.Clustering, BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\nx        = scale(x) # normalise all dimensions to (μ=0, σ=1)\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\n\n# Get some ranges of minVariance and minCovariance to test\nminVarRange   = collect(0.04:0.05:1.5)\nminCovarRange = collect(0:0.05:1.45)\n\n# Run the gmm(em) algorithm for the various cases...\nsphOut  = [gmm(x,3,mixtures=[SphericalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE) for v in minVarRange, cv in minCovarRange[1:1]]\ndiagOut  = [gmm(x,3,mixtures=[DiagonalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange[1:1]]\nfullOut = [gmm(x,3,mixtures=[FullGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange]\n\n# Get the Bayesian information criterion (AIC is also available)\nsphBIC = [sphOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\ndiagBIC = [diagOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\nfullBIC = [fullOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\n# Compare the accuracy with true categories\nsphAcc  = [accuracy(sphOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\ndiagAcc = [accuracy(diagOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\nfullAcc = [accuracy(fullOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\nplot(minVarRange,[sphBIC diagBIC fullBIC[:,1] fullBIC[:,15] fullBIC[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"BIC\", xlabel=\"minVariance\")\nplot(minVarRange,[sphAcc diagAcc fullAcc[:,1] fullAcc[:,15] fullAcc[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"Accuracies\", xlabel=\"minVariance\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Further examples","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"We also provide some Jupyter notebooks that can be run online without installing anything, so you can start playing with the library in minutes. Finally, you may want to give a look at the \"test\" folder. While the primary reason of the scripts under the \"test\" folder is to provide automatic testing of the BetaML toolkit, they can also be used to see how functions should be called, as virtually all functions provided by BetaML are tested there.","category":"page"},{"location":"index.html#Acknowledgements","page":"Index","title":"Acknowledgements","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The development of this package at the Bureau d'Economie Théorique et Appliquée (BETA, Nancy) was supported by the French National Research Agency through the Laboratory of Excellence ARBRE, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: BLogos)","category":"page"},{"location":"tutorials/A classification task when labels are known: determining the plant species giving floreal measures/tutorial_classification_iris.html","page":"-","title":"-","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/A classification task when labels are known: determining the plant species giving floreal measures/tutorial_classification_iris.jl\"","category":"page"},{"location":"tutorials/A classification task when labels are known: determining the plant species giving floreal measures/tutorial_classification_iris.html#This-is-markdown-title","page":"-","title":"This is markdown title","text":"","category":"section"},{"location":"tutorials/A classification task when labels are known: determining the plant species giving floreal measures/tutorial_classification_iris.html","page":"-","title":"-","text":"This is also markdown","category":"page"},{"location":"tutorials/A classification task when labels are known: determining the plant species giving floreal measures/tutorial_classification_iris.html","page":"-","title":"-","text":"# This is a normal comment","category":"page"},{"location":"tutorials/A classification task when labels are known: determining the plant species giving floreal measures/tutorial_classification_iris.html","page":"-","title":"-","text":"a = 1\nb = a + 1\nprintln(b)","category":"page"},{"location":"tutorials/A classification task when labels are known: determining the plant species giving floreal measures/tutorial_classification_iris.html","page":"-","title":"-","text":"View this file on Github.","category":"page"},{"location":"tutorials/A classification task when labels are known: determining the plant species giving floreal measures/tutorial_classification_iris.html","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/A classification task when labels are known: determining the plant species giving floreal measures/tutorial_classification_iris.html","page":"-","title":"-","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/A regression task: sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"-","title":"-","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/A regression task: sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.jl\"","category":"page"},{"location":"tutorials/A regression task: sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html#This-is-markdown-title","page":"-","title":"This is markdown title","text":"","category":"section"},{"location":"tutorials/A regression task: sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"-","title":"-","text":"This is also markdown","category":"page"},{"location":"tutorials/A regression task: sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"-","title":"-","text":"# This is a normal comment","category":"page"},{"location":"tutorials/A regression task: sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"-","title":"-","text":"a = 1\nb = a + 1\nprintln(b)","category":"page"},{"location":"tutorials/A regression task: sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"-","title":"-","text":"View this file on Github.","category":"page"},{"location":"tutorials/A regression task: sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/A regression task: sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"-","title":"-","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Getting started/betaml_tutorial_getting_started.jl\"","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html#This-is-markdown-title","page":"-","title":"This is markdown title","text":"","category":"section"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"This is also markdown","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"# This is a normal comment","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"a = 1\nb = a + 1\nprintln(b)","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html#Randomness","page":"-","title":"Randomness","text":"","category":"section"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"Most models have some stochastic components and support a rng parameter. By default, the outputs of these models will hence not be absolutelly equal on each run. If you want to be sure that the output of a model remain constant given the same inputs you can pass a fixed Random Number Generator to the rng parameter. Use it with:","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"myAlgorithm(;rng=FIXEDRNG)               # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=StableRNG(SOMEINTEGER)) # always produce the same result (new rng object on each call)","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"In particular, use rng=StableRNG(FIXEDSEED) to retrieve the exacty output as in the documentation or in the unit tests.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"Most of the stochasticity appears in training a model. However in few cases (e.g. decision trees with missing values) some stocasticity appears also in predicting new data with a trained model. In such cases the model doesn't stire the random seed, so that you can choose at predict time to use a fixed or a variable random seed.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"View this file on Github.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Examples.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"Examples.html#Supervised-learning","page":"Examples","title":"Supervised learning","text":"","category":"section"},{"location":"Examples.html#Regression","page":"Examples","title":"Regression","text":"","category":"section"},{"location":"Examples.html#Estimating-the-bike-sharing-demand","page":"Examples","title":"Estimating the bike sharing demand","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Data origin:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"original full dataset (by hour, not used here): https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nsimplified dataset (by day, with some simple scaling): https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec\ndescription: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/exam2019ace.pdf\ndata: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/bikesharing_day.csv.zip","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. Y_t = f(X_t) alone).","category":"page"},{"location":"Examples.html#Classification","page":"Examples","title":"Classification","text":"","category":"section"},{"location":"Examples.html#Unsupervised-lerarning","page":"Examples","title":"Unsupervised lerarning","text":"","category":"section"},{"location":"Examples.html#Notebooks","page":"Examples","title":"Notebooks","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The following notebooks provide runnable examples of the package functionality:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Pegasus classifiers: [Static notebook] - [myBinder]\nDecision Trees and Random Forest regression on Bike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nNeural Networks: [Static notebook] - [myBinder]\nBike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nClustering: [Static notebook] - [myBinder]","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note: the live, runnable computational environment is a temporary new copy made at each connection. The first time after a commit is done on this repository a new environment has to be set (instead of just being copied), and the server may take several minutes.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"This is only if you are the unlucky user triggering the rebuild of the environment after the commit.","category":"page"},{"location":"Nn.html#The-BetaML.Nn-Module","page":"Nn","title":"The BetaML.Nn Module","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Nn","category":"page"},{"location":"Nn.html#BetaML.Nn","page":"Nn","title":"BetaML.Nn","text":"BetaML.Nn module\n\nImplement the functionality required to define an artificial Neural Network, train it with data, forecast data and assess its performances.\n\nCommon type of layers and optimisation algorithms are already provided, but you can define your own ones subclassing respectively the Layer and OptimisationAlgorithm abstract types.\n\nThe module provide the following type or functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition:\n\nDenseLayer: Classical feed-forward layer with user-defined activation function\nDenseNoBiasLayer: Classical layer without the bias parameter\nVectorFunctionLayer: Parameterless layer whose activation function run over the ensable of its nodes rather than on each one individually\nbuildNetwork: Build the chained network and define a cost function\ngetParams(nn): Retrieve current weigthts\ngetGradient(nn): Retrieve the current gradient of the weights\nsetParams!(nn): Update the weigths of the network\nshow(nn): Print a representation of the Neural Network\n\nEach layer can use a default activation function, one of the functions provided in the Utils module (relu, tanh, softmax,...) or you can specify your own function. The derivative of the activation function can be optionally be provided, in such case training will be quicker, altought this difference tends to vanish with bigger datasets. You can alternativly implement your own layers defining a new type as subtype of the abstract type Layer. Each user-implemented layer must define the following methods:\n\nA suitable constructor\nforward(layer,x)\nbackward(layer,x,nextGradient)\ngetParams(layer)\ngetGradient(layer,x,nextGradient)\nsetParams!(layer,w)\nsize(layer)\n\nModel training:\n\ntrainingInfo(nn): Default callback function during training\ntrain!(nn):  Training function\nsingleUpdate!(θ,▽;optAlg): The parameter update made by the specific optimisation algorithm\nSGD: The default optimisation algorithm\nADAM: A faster moment-based optimisation algorithm (added in v0.2.2)\n\nTo define your own optimisation algorithm define a subtype of OptimisationAlgorithm and implement the function singleUpdate!(θ,▽;optAlg) and eventually initOptAlg(⋅) specific for it.\n\nModel predictions and assessment:\n\npredict(nn): Return the output given the data\nloss(nn): Compute avg. network loss on a test set\nUtils.accuracy(ŷ,y): Categorical output accuracy\n\nWhile high-level functions operating on the dataset expect it to be in the standard format (nRecords × nDimensions matrices) it is custom to represent the chain of a neural network as a flow of column vectors, so all low-level operations (operating on a single datapoint) expect both the input and the output as a column vector.\n\n\n\n\n\n","category":"module"},{"location":"Nn.html#Module-Index","page":"Nn","title":"Module Index","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Nn.html#Detailed-API","page":"Nn","title":"Detailed API","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]","category":"page"},{"location":"Nn.html#BetaML.Nn.ADAM","page":"Nn","title":"BetaML.Nn.ADAM","text":"ADAM(;η, λ, β₁, β₂, ϵ)\n\nThe ADAM algorithm, an adaptive moment estimation optimiser.\n\nFields:\n\nη:  Learning rate (stepsize, α in the paper), as a function of the current epoch [def: t -> 0.001 (i.e. fixed)]\nλ:  Multiplicative constant to the learning rate [def: 1]\nβ₁: Exponential decay rate for the first moment estimate [range: ∈ [0,1], def: 0.9]\nβ₂: Exponential decay rate for the second moment estimate [range: ∈ [0,1], def: 0.999]\nϵ:  Epsilon value to avoid division by zero [def: 10^-8]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseLayer","page":"Nn","title":"BetaML.Nn.DenseLayer","text":"DenseLayer\n\nRepresentation of a layer in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nwb: Biases (n)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseNoBiasLayer","page":"Nn","title":"BetaML.Nn.DenseNoBiasLayer","text":"DenseNoBiasLayer\n\nRepresentation of a layer without bias in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.Learnable","page":"Nn","title":"BetaML.Nn.Learnable","text":"Learnable(data)\n\nStructure representing the learnable parameters of a layer or its gradient.\n\nThe learnable parameters of a layers are given in the form of a N-tuple of Array{Float64,N2} where N2 can change (e.g. we can have a layer with the first parameter being a matrix, and the second one being a scalar). We wrap the tuple on its own structure a bit for some efficiency gain, but above all to define standard mathematic operations on the gradients without doing \"type pyracy\" with respect to Base tuples.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NN","page":"Nn","title":"BetaML.Nn.NN","text":"NN\n\nRepresentation of a Neural Network\n\nFields:\n\nlayers:  Array of layers objects\ncf:      Cost function\ndcf:     Derivative of the cost function\ntrained: Control flag for trained networks\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.OptimisationAlgorithm","page":"Nn","title":"BetaML.Nn.OptimisationAlgorithm","text":"OptimisationAlgorithm\n\nAbstract type representing an Optimisation algorithm.\n\nCurrently supported algorithms:\n\nSGD (Stochastic) Gradient Descent\n\nSee ?[Name OF THE ALGORITHM] for their details\n\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate(⋅) (type ?singleUpdate for details).\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.SGD","page":"Nn","title":"BetaML.Nn.SGD","text":"SGD(;η=t -> 1/(1+t), λ=2)\n\nStochastic Gradient Descent algorithm (default)\n\nFields:\n\nη: Learning rate, as a function of the current epoch [def: t -> 1/(1+t)]\nλ: Multiplicative constant to the learning rate [def: 2]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.VectorFunctionLayer","page":"Nn","title":"BetaML.Nn.VectorFunctionLayer","text":"VectorFunctionLayer\n\nRepresentation of a (weightless) VectorFunction layer in the network. Vector function layer expects a vector activation function, i.e. a function taking the whole output of the previous layer in input rather than working on a single node as \"normal\" activation functions. Useful for example for the SoftMax function.\n\nFields:\n\nnₗ: Number of nodes of the previous layer\nn:  Number of nodes in output\nf:  Activation function (vector)\ndf: Derivative of the (vector) activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#Base.size-Tuple{Layer}","page":"Nn","title":"Base.size","text":"size(layer)\n\nSGet the dimensions of the layers in terms of (dimensions in input , dimensions in output)\n\nNotes:\n\nYou need to use import Base.size before defining this function for your layer\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.backward-Tuple{Layer,Any,Any}","page":"Nn","title":"BetaML.Nn.backward","text":"backward(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer inputs\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.buildNetwork-Tuple{Any,Any}","page":"Nn","title":"BetaML.Nn.buildNetwork","text":"buildNetwork(layers,cf;dcf,name)\n\nInstantiate a new Feedforward Neural Network\n\nParameters:\n\nlayers: Array of layers objects\ncf:     Cost function\ndcf:    Derivative of the cost function [def: nothing]\nname:   Name of the network [def: \"Neural Network\"]\n\nNotes:\n\nEven if the network ends with a single output note, the cost function and its derivative should always expect y and ŷ as column vectors.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.forward-Tuple{Layer,Any}","page":"Nn","title":"BetaML.Nn.forward","text":"forward(layer,x)\n\nPredict the output of the layer given the input\n\nParameters:\n\nlayer:  Worker layer\nx:      Input to the layer\n\nReturn:\n\nAn Array{T,1} of the prediction (even for a scalar)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Tuple{Layer,Any,Any}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getParams() and setParams() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{Any,AbstractArray{T,2},AbstractArray{T2,2}}} where T2<:Number where T<:Number","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,xbatch,ybatch)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn:      Worker network\nxbatch:  Input to the network (n,d)\nybatch:  Label input (n,d)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{NN,Union{AbstractArray{T,1}, T},Union{AbstractArray{T2,1}, T2}}} where T2<:Number where T<:Number","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,x,y)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn: Worker network\nx:   Input to the network (d,1)\ny:   Label input (d,1)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{Layer}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(layer)\n\nReturn the number of parameters of a layer.\n\nIt doesn't need to be implemented by each layer type, as it uses getParams().\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(nn) - Return the number of trainable parameters of the neural network.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{Layer}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(layer)\n\nGet the layers current value of its trainable parameters\n\nParameters:\n\nlayer:  Worker layer\n\nReturn:\n\nThe current value of the layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getGradient() and setParams() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(nn)\n\nRetrieve current weigthts\n\nParameters:\n\nnn: Worker network\n\nNotes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.initOptAlg!-Tuple{ADAM}","page":"Nn","title":"BetaML.Nn.initOptAlg!","text":"initOptAlg!(optAlg::ADAM;θ,batchSize,x,y,rng)\n\nInitialize the ADAM algorithm with the parameters m and v as zeros and check parameter bounds\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.initOptAlg!-Tuple{BetaML.Nn.OptimisationAlgorithm}","page":"Nn","title":"BetaML.Nn.initOptAlg!","text":"initOptAlg!(optAlg;θ,batchSize,x,y)\n\nInitialize the optimisation algorithm\n\nParameters:\n\noptAlg:    The Optimisation algorithm to use\nθ:         Current parameters\nbatchSize:    The size of the batch\nx:   The training (input) data\ny:   The training \"labels\" to match\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nOnly a few optimizers need this function and consequently ovverride it. By default it does nothing, so if you want write your own optimizer and don't need to initialise it, you don't have to override this method\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.loss-Tuple{NN,Any,Any}","page":"Nn","title":"BetaML.Nn.loss","text":"loss(fnn,x,y)\n\nCompute avg. network loss on a test set (or a single (1 × d) data point)\n\nParameters:\n\nfnn: Worker network\nx:   Input to the network (n) or (n x d)\ny:   Label input (n) or (n x d)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{Layer,Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":" setParams!(layer,w)\n\nSet the trainable parameters of the layer with the given values\n\nParameters:\n\nlayer: Worker layer\nw:   The new parameters to set (Learnable)\n\nNotes:\n\nThe format of the tuple wrapped by Learnable must be consistent with those of the getParams() and getGradient() functions.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{NN,Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":"setParams!(nn,w)\n\nUpdate weigths of the network\n\nParameters:\n\nnn: Worker network\nw:  The new weights to set\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.show-Tuple{NN}","page":"Nn","title":"BetaML.Nn.show","text":"show(nn)\n\nPrint a representation of the Neural Network (layers, dimensions..)\n\nParameters:\n\nnn: Worker network\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.singleUpdate!-Tuple{Any,Any}","page":"Nn","title":"BetaML.Nn.singleUpdate!","text":"singleUpdate!(θ,▽;nEpoch,nBatch,batchSize,xbatch,ybatch,optAlg)\n\nPerform the parameters update based on the average batch gradient.\n\nParameters:\n\nθ:         Current parameters\n▽:         Average gradient of the batch\nnEpoch:    Count of current epoch\nnBatch:    Count of current batch\nnBatches:  Number of batches per epoch\nxbatch:    Data associated to the current batch\nybatch:    Labels associated to the current batch\noptAlg:    The Optimisation algorithm to use for the update\n\nNotes:\n\nThis function is overridden so that each optimisation algorithm implement their\n\nown version\n\nMost parameters are not used by any optimisation algorithm. They are provided\n\nto support the largest possible class of optimisation algorithms\n\nSome optimisation algorithms may change their internal structure in this function\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.train!-Tuple{NN,Any,Any}","page":"Nn","title":"BetaML.Nn.train!","text":"train!(nn,x,y;epochs,batchSize,sequential,optAlg,verbosity,cb)\n\nTrain a neural network with the given x,y data\n\nParameters:\n\nnn:         Worker network\nx:          Training input to the network (records x dimensions)\ny:          Label input (records x dimensions)\nepochs:     Number of passages over the training set [def: 100]\nbatchSize:  Size of each individual batch [def: min(size(x,1),32)]\nsequential: Wether to run all data sequentially instead of random [def: false]\noptAlg:     The optimisation algorithm to update the gradient at each batch [def: ADAM()]\nverbosity:  A verbosity parameter for the trade off information / efficiency [def: STD]\ncb:         A callback to provide information. [def: trainingInfo]\nrng:        Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn:\n\nA named tuple with the following information\nepochs: Number of epochs actually ran\nϵ_epochs: The average error on each epoch (if verbosity > LOW)\nθ_epochs: The parameters at each epoch (if verbosity > STD)\n\nNotes:\n\nCurrently supported algorithms:\nSGD, the classical (Stochastic) Gradient Descent optimiser\nADAM,  an adaptive moment estimation optimiser\nLook at the individual optimisation algorithm (?[Name OF THE ALGORITHM]) for info on its parameter, e.g. ?SGD for the Stochastic Gradient Descent.\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate!(⋅) (type ?singleUpdate! for details).\nYou can implement your own callback function, altought the one provided by default is already pretty generic (its output depends on the verbosity parameter). See trainingInfo for informations on the cb parameters.\nBoth the callback function and the singleUpdate! function of the optimisation algorithm can be used to stop the training algorithm, respectively returning true or stop=true.\nThe verbosity can be set to any of NONE,LOW,STD,HIGH,FULL.\nThe update is done computing the average gradient for each batch and then calling singleUpdate! to let the optimisation algorithm perform the parameters update\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.trainingInfo-Tuple{Any,Any,Any}","page":"Nn","title":"BetaML.Nn.trainingInfo","text":"trainingInfo(nn,x,y;n,batchSize,epochs,verbosity,nEpoch,nBatch)\n\nDefault callback funtion to display information during training, depending on the verbosity level\n\nParameters:\n\nnn: Worker network\nx:  Batch input to the network (batchSize,d)\ny:  Batch label input (batchSize,d)\nn: Size of the full training set\nnBatches : Number of baches per epoch\nepochs: Number of epochs defined for the training\nverbosity: Verbosity level defined for the training (NONE,LOW,STD,HIGH,FULL)\nnEpoch: Counter of the current epoch\nnBatch: Counter of the current batch\n\n#Notes:\n\nReporting of the error (loss of the network) is expensive. Use verbosity=NONE for better performances\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#The-BetaML.Clustering-Module","page":"Clustering","title":"The BetaML.Clustering Module","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Clustering\n","category":"page"},{"location":"Clustering.html#BetaML.Clustering","page":"Clustering","title":"BetaML.Clustering","text":"Clustering module (WIP)\n\nProvide clustering methods and missing values imputation / collaborative filtering / reccomendation systems using clustering methods as backend.\n\nThe module provides the following functions. Use ?[function] to access their full signature and detailed documentation:\n\ninitRepresentatives(X,K;initStrategy,Z₀): Initialisation strategies for Kmean and Kmedoids\n`kmeans(X,K;dist,initStrategy,Z₀)`: Classical KMean algorithm\n`kmedoids(X,K;dist,initStrategy,Z₀)`: Kmedoids algorithm\n`gmm(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy)`: gmm algorithm over GMM\n`predictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance)`: Fill mixing values / collaborative filtering using gmm as backbone\n\n{Spherical|Diagonal|Full}Gaussian mixtures for gmm / predictMissing are already provided. User defined mixtures can be used defining a struct as subtype of Mixture and implementing for that mixture the following functions:\n\ninitMixtures!(mixtures, X; minVariance, minCovariance, initStrategy)\nlpdf(m,x,mask) (for the e-step)\nupdateParameters!(mixtures, X, pₙₖ; minVariance, minCovariance) (the m-step)\n\n\n\n\n\n","category":"module"},{"location":"Clustering.html#Module-Index","page":"Clustering","title":"Module Index","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Clustering.html#Detailed-API","page":"Clustering","title":"Detailed API","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]","category":"page"},{"location":"Clustering.html#BetaML.Clustering.gmm-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.gmm","text":"gmm(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy)\n\nCompute Expectation-Maximisation algorithm to identify K clusters of X data, i.e. employ a Generative Mixture Model as the underlying probabilistic model.\n\nX can contain missing values in some or all of its dimensions. In such case the learning is done only with the available data. Implemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (n x d) data to clusterise\nK  :           Number of cluster wanted\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: nothing]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: kmeans]\nmaxIter:       Maximum number of iterations [def: -1, i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\npₙₖ:      Matrix of size (N x K) of the probabilities of each point i to belong to cluster j\npₖ:       Probabilities of the categorical distribution (K x 1)\nmixtures: Vector (K x 1) of the estimated underlying distributions\nϵ:        Vector of the discrepancy (matrix norm) between pⱼₓ and the lagged pⱼₓ at each iteration\nlL:       The log-likelihood (without considering the last mixture optimisation)\nBIC:      The Bayesian Information Criterion (lower is better)\nAIC:      The Akaike Information Criterion (lower is better)\n\nNotes:\n\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nReasonable choices for the minVariance/Covariance depends on the mixture. For example 0.25 seems a reasonable value for the SphericalGaussian, 0.05 seems better for the DiagonalGaussian, and FullGaussian seems to prefer either very low values of variance/covariance (e.g. (0.05,0.05) ) or very big but similar ones (e.g. (100,100) ).\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster (expecially if X contains missing values), but kmeans often provides better results.\n\nResources:\n\nPaper describing gmm with missing values\nClass notes from MITx 6.86x (Sec 15.9)\nLimitations of gmm\n\nExample:\n\njulia> clusters = gmm([1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4],3,verbosity=HIGH)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.initMixtures!-Union{Tuple{T}, Tuple{Array{T,1},Any}} where T<:BetaML.Clustering.AbstractGaussian","page":"Clustering","title":"BetaML.Clustering.initMixtures!","text":"initMixtures!(mixtures::Array{T,1}, X; minVariance=0.25, minCovariance=0.0, initStrategy=\"grid\",rng=Random.GLOBAL_RNG)\n\nThe parameter initStrategy can be grid, kmeans or given:\n\ngrid: Uniformly cover the space observed by the data\nkmeans: Use the kmeans algorithm. If the data contains missing values, a first run of predictMissing is done under init=grid to impute the missing values just to allow the kmeans algorithm. Then the em algorithm is used with the output of kmean as init values.\ngiven: Leave the provided set of initial mixtures\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.initRepresentatives-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.initRepresentatives","text":"initRepresentatives(X,K;initStrategy,Z₀)\n\nInitialisate the representatives for a K-Mean or K-Medoids algorithm\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA (K x D) matrix of initial representatives\n\nExample:\n\njulia> Z₀ = initRepresentatives([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.6 38],2,initStrategy=\"given\",Z₀=[1.7 15; 3.6 40])\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmeans-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.kmeans","text":"kmeans(X,K;dist,initStrategy,Z₀)\n\nCompute K-Mean algorithm to identify K clusters of X using Euclidean distance\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmeans([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmedoids-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.kmedoids","text":"kmedoids(X,K;dist,initStrategy,Z₀)\n\nCompute K-Medoids algorithm to identify K clusters of X using distance definition dist\n\nParameters:\n\nX: a (n x d) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach\nshuffle: selecting randomly within the available points [default]\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmedoids([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3,initStrategy=\"grid\")\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{DiagonalGaussian,Any,Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::DiagonalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{FullGaussian,Any,Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::FullGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{SphericalGaussian,Any,Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::SphericalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.predictMissing-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.predictMissing","text":"predictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance)\n\nFill missing entries in a sparse matrix assuming an underlying Gaussian Mixture probabilistic Model (GMM) and implementing an Expectation-Maximisation algorithm.\n\nWhile the name of the function is predictMissing, the function can be used also for system reccomendation / collaborative filtering and GMM-based regressions.\n\nImplemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (N x D) sparse matrix of data to fill according to a GMM model\nK  :           Number of mixtures desired\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: nothing]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: grid]\nmaxIter:       Maximum number of iterations [def: -1, i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\n̂X̂    : The Filled Matrix of size (N x D)\nnFill: The number of items filled\nlL   : The log-likelihood (without considering the last mixture optimisation)\nBIC :  The Bayesian Information Criterion (lower is better)\nAIC :  The Akaike Information Criterion (lower is better)\nNotes:\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster, but kmeans often provides better results.\n\nExample:\n\njulia>  cFOut = predictMissing([1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.predict-Tuple{GMM,Any,Any}","page":"Clustering","title":"MLJModelInterface.predict","text":"predict(m::GMM, fitResults, X) - Given a trained clustering model and some observations, predict the class of the observation\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.predict-Tuple{MissingImputator,Any,Any}","page":"Clustering","title":"MLJModelInterface.predict","text":"predict(m::MissingImputator, fitResults, X) - Given a trained imputator model fill the missing data of some new observations\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.predict-Tuple{Union{KMeans, KMedoids},Any,Any}","page":"Clustering","title":"MLJModelInterface.predict","text":"predict(m::KMeans, fitResults, X) - Given a trained clustering model and some observations, predict the class of the observation\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.transform-Tuple{GMM,Any,Any}","page":"Clustering","title":"MLJModelInterface.transform","text":"transform(m::GMM, fitResults, X) - Given a trained clustering model and some observations, predict the class of the observation\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.transform-Tuple{MissingImputator,Any}","page":"Clustering","title":"MLJModelInterface.transform","text":"transform(m::MissingImputator, X) - Given a matrix with missing value, impute them using an EM algorithm\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.transform-Tuple{Union{KMeans, KMedoids},Any,Any}","page":"Clustering","title":"MLJModelInterface.transform","text":"fit(m::KMeans, fitResults, X) - Given a trained clustering model and some observations, return the distances to each centroids \n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#The-BetaML.Perceptron-Module","page":"Perceptron","title":"The BetaML.Perceptron Module","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Perceptron","category":"page"},{"location":"Perceptron.html#BetaML.Perceptron","page":"Perceptron","title":"BetaML.Perceptron","text":"Perceptron module\n\nProvide linear and kernel classifiers.\n\nSee a runnable example on myBinder\n\nperceptron: Train data using the classical perceptron\nkernelPerceptron: Train data using the kernel perceptron\npegasos: Train data using the pegasos algorithm\npredict: Predict data using parameters from one of the above algorithms\n\nAll algorithms are multiclass, with perceptron and pegasos employing a one-vs-all strategy, while kernelPerceptron employs a one-vs-one approach, and return a \"probability\" for each class in term of a dictionary for each record. Use mode(ŷ) to return a single class prediction per record.\n\nThe binary equivalent algorithms, accepting only {-1,+1} labels, are available as peceptronBinary, kernelPerceptronBinary and pegasosBinary. They are slighly faster as they don't need to be wrapped in the multi-class equivalent and return a more informative output.\n\nThe multi-class versions are available in the MLJ framework as PerceptronClassifier,KernelPerceptronClassifier and PegasosClassifier respectivly.\n\n\n\n\n\n","category":"module"},{"location":"Perceptron.html#Module-Index","page":"Perceptron","title":"Module Index","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Perceptron.html#Detailed-API","page":"Perceptron","title":"Detailed API","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]","category":"page"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptron-Tuple{Any,Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptron","text":"kernelPerceptron(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a multiclass kernel classifier \"perceptron\" algorithm based on x and y.\n\nkernelPerceptron is a (potentially) non-linear perceptron-style classifier employing user-defined kernel funcions. Multiclass is supported using a one-vs-one approach.\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data, in the format of ⨦ 1\nK:        Kernel function to employ. See ?radialKernel or ?polynomialKernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radialKernel]\nT:        Maximum number of iterations (aka \"epochs\") across the whole set (if the set is not fully classified earlier) [def: 100]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: The x data (eventually shuffled if shuffle=true)\ny: The label\nα: The errors associated to each record\nclasses: The labels classes encountered in the training\n\nNotes:\n\nThe trained model can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the KernelPerceptronClassifier\n\nExample:\n\njulia> model  = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷtrain = Perceptron.predict(xtrain,model.x,model.y,model.α, model.classes,K=model.K)\njulia> ϵtrain = error(ytrain, mode(ŷtrain))\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptronBinary-Tuple{Any,Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptronBinary","text":"kernelPerceptronBinary(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a multiclass kernel classifier \"perceptron\" algorithm based on x and y\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data, in the format of ⨦ 1\nK:        Kernel function to employ. See ?radialKernel or ?polynomialKernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radialKernel]\nT:        Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: the x data (eventually shuffled if shuffle=true)\ny: the label\nα: the errors associated to each record\nerrors: the number of errors in the last iteration\nbesterrors: the minimum number of errors in classifying the data ever reached\niterations: the actual number of iterations performed\nseparated: a flag if the data has been successfully separated\n\nNotes:\n\nThe trained data can then be used to make predictions using the function predict(). If the option shuffle has been used, it is important to use there the returned (x,y,α) as these would have been shuffle compared with the original (x,y).\nPlease see @kernelPerceptron for a multi-class version\n\nExample:\n\njulia> model = kernelPerceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasos-Tuple{Any,Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasos","text":"pegasos(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,forceOrigin,returnMeanHyperplane)\n\nTrain the multiclass classifier \"pegasos\" algorithm according to x (features) and y (labels)\n\nPegasos is a linear, gradient-based classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whehter to force θ₀ to remain zero [def: false]\nreturnMeanHyperplane: Whether to return the average hyperplane coefficients instead of the average ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the PegasosClassifier\n\nExample:\n\njulia> model = pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasosBinary-Tuple{Any,Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasosBinary","text":"pegasosBinary(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,forceOrigin)\n\nTrain the peagasos algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:    Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptron-Tuple{AbstractArray{T,2} where T,AbstractArray{T,1} where T}","page":"Perceptron","title":"BetaML.Perceptron.perceptron","text":"perceptron(x,y;θ,θ₀,T,nMsgs,shuffle,forceOrigin,returnMeanHyperplane)\n\nTrain the multiclass classifier \"perceptron\" algorithm  based on x and y (labels).\n\nThe perceptron is a linear classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant                term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set                is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\nreturnMeanHyperplane: Whether to return the average hyperplane coefficients instead of the final ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the PerceptronClassifier\n\nExample:\n\njulia> model = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptronBinary-Tuple{Any,Any}","page":"Perceptron","title":"BetaML.Perceptron.perceptronBinary","text":"perceptronBinary(x,y;θ,θ₀,T,nMsgs,shuffle,forceOrigin)\n\nTrain the binary classifier \"perceptron\" algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant                term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set                is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> model = perceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.predict","page":"Perceptron","title":"BetaML.Perceptron.predict","text":"predict(x,θ,θ₀)\n\nPredict a binary label {-1,1} given the feature vector and the linear coefficients\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\nθ:        The trained parameters\nθ₀:       The trained bias barameter [def: 0]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"function"},{"location":"Perceptron.html#BetaML.Perceptron.predict-NTuple{4,Any}","page":"Perceptron","title":"BetaML.Perceptron.predict","text":"predict(x,xtrain,ytrain,α;K)\n\nPredict a binary label {-1,1} given the feature vector and the training data together with their errors (as trained by a kernel perceptron algorithm)\n\nParameters:\n\nx:      Feature matrix of the training data (n × d)\nxtrain: The feature vectors used for the training\nytrain: The labels of the training set\nα:      The errors associated to each record\nK:      The kernel function used for the training and to be used for the prediction [def: radialKernel]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.predict-Union{Tuple{Tcl}, Tuple{Any,Any,Any,Any,AbstractArray{Tcl,1}}} where Tcl","page":"Perceptron","title":"BetaML.Perceptron.predict","text":"predict(x,xtrain,ytrain,α,classes;K)\n\nPredict a multiclass label given the new feature vector and a trained kernel perceptron model.\n\nParameters:\n\nx:      Feature matrix of the training data (n × d)\nxtrain: A vector of the feature matrix used for training each of the one-vs-one class matches (i.e. model.x)\nytrain: A vector of the label vector used for training each of the one-vs-one class matches (i.e. model.y)\nα:      A vector of the errors associated to each record (i.e. model.α)\nclasses: The overal classes encountered in training (i.e. model.classes)\nK:      The kernel function used for the training and to be used for the prediction [def: radialKernel]\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability (warning: it isn't really a probability, it is just the standardized number of matches \"won\" by this class compared with the other classes)\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\njulia> model  = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷtrain = Perceptron.predict([10 10; 2.2 2.5],model.x,model.y,model.α, model.classes,K=model.K)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.predict-Union{Tuple{Tcl}, Tuple{T}, Tuple{Any,AbstractArray{T,1},AbstractArray{Float64,1},Array{Tcl,1}}} where Tcl where T<:AbstractArray{Float64,1}","page":"Perceptron","title":"BetaML.Perceptron.predict","text":"predict(x,θ,θ₀,classes)\n\nPredict a multiclass label given the feature vector, the linear coefficients and the classes vector\n\nParameters:\n\nx:       Feature matrix of the training data (n × d)\nθ:       Vector of the trained parameters for each one-vs-all model (i.e. model.θ)\nθ₀:      Vector of the trained bias barameter for each one-vs-all model (i.e. model.θ₀)\nclasses: The overal classes encountered in training (i.e. model.classes)\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\n```julia julia> model  = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1]) julia> ŷtrain = predict([10 10; 2.5 2.5],model.θ,model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#The-BetaML.Trees-Module","page":"Trees","title":"The BetaML.Trees Module","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Trees","category":"page"},{"location":"Trees.html#BetaML.Trees","page":"Trees","title":"BetaML.Trees","text":"BetaML.Trees module\n\nImplement the functionality required to build a Decision Tree or a whole Random Forest, predict data and assess its performances.\n\nBoth Decision Trees and Random Forests can be used for regression or classification problems, based on the type of the labels (numerical or not). You can override the automatic selection with the parameter forceClassification=true, typically if your labels are integer representing some categories rather than numbers. For classification problems the output of predictSingle is a dictionary with the key being the labels with non-zero probabilitity and the corresponding value its proobability; for regression it is a numerical value.\n\nPlease be aware that, differently from most other implementations, the Random Forest algorithm collects and averages the probabilities from the trees, rather than just repording the mode, i.e. no information is lost and the output of the forest classifier is still a PMF.\n\nMissing data on features are supported, both on training and on prediction.\n\nThe module provide the following functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition and training:\n\nbuildTree(xtrain,ytrain): Build a single Decision Tree\nbuildForest(xtrain,ytrain): Build a \"forest\" of Decision Trees\n\nModel predictions and assessment:\n\npredict(tree or forest, x): Return the prediction given the feature matrix\noobError(forest,x,y): Return the out-of-bag error estimate\nUtils.accuracy(ŷ,y)): Categorical output accuracy\nUtils.meanRelError(ŷ,y,p): L-p norm based error\n\nFeatures are expected to be in the standard format (nRecords × nDimensions matrices) and the labels (either categorical or numerical) as a nRecords column vector.\n\nAcknowlegdments: originally based on the Josh Gordon's code\n\n\n\n\n\n","category":"module"},{"location":"Trees.html#Module-Index","page":"Trees","title":"Module Index","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Trees.html#Detailed-API","page":"Trees","title":"Detailed API","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]","category":"page"},{"location":"Trees.html#BetaML.Trees.AbstractQuestion","page":"Trees","title":"BetaML.Trees.AbstractQuestion","text":"Question\n\nA question used to partition a dataset.\n\nThis struct just records a 'column number' and a 'column value' (e.g., Green).\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.DecisionNode","page":"Trees","title":"BetaML.Trees.DecisionNode","text":"DecisionNode(question,trueBranch,falseBranch, depth)\n\nA tree's non-terminal node.\n\nConstructor's arguments and struct members:\n\nquestion: The question asked in this node\ntrueBranch: A reference to the \"true\" branch of the trees\nfalseBranch: A reference to the \"false\" branch of the trees\ndepth: The nodes's depth in the tree\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.Forest","page":"Trees","title":"BetaML.Trees.Forest","text":"Forest{Ty}\n\nType representing a Random Forest.\n\nIndividual trees are stored in the array trees. The \"type\" of the forest is given by the type of the labels on which it has been trained.\n\nStruct members:\n\ntrees:        The individual Decision Trees\nisRegression: Whether the forest is to be used for regression jobs or classification\noobData:      For each tree, the rows number if the data that have not being used to train the specific tree\noobError:     The out of bag error (if it has been computed)\nweights:      A weight for each tree depending on the tree's score on the oobData (see buildForest)\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.Leaf","page":"Trees","title":"BetaML.Trees.Leaf","text":"Leaf(y,depth)\n\nA tree's leaf (terminal) node.\n\nConstructor's arguments:\n\ny: The labels assorciated to each record (either numerical or categorical)\ndepth: The nodes's depth in the tree\n\nStruct members:\n\npredictions: Either the relative label's count (i.e. a PMF) or the mean\ndepth: The nodes's depth in the tree\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#Base.print","page":"Trees","title":"Base.print","text":"print(node)\n\nPrint a Decision Tree (textual)\n\n\n\n\n\n","category":"function"},{"location":"Trees.html#BetaML.Trees.buildForest-Union{Tuple{Ty}, Tuple{Any,AbstractArray{Ty,1}}, Tuple{Any,AbstractArray{Ty,1},Any}} where Ty","page":"Trees","title":"BetaML.Trees.buildForest","text":"buildForest(x, y, nTrees; maxDepth, minGain, minRecords, maxFeatures, splittingCriterion, forceClassification)\n\nBuilds (define and train) a \"forest\" of Decision Trees.\n\nParameters:\n\nSee buildTree. The function has all the parameters of bildTree (with the maxFeatures defaulting to √D instead of D) plus the following parameters:\n\nnTrees: Number of trees in the forest [def: 30]\nβ: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction (see later) [def: 0, i.e. uniform weigths]\noob: Whether to coompute the out-of-bag error, an estimation of the generalization accuracy [def: false]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nOutput:\n\nThe function returns a Forest object (see Forest).\nThe forest weights default to array of ones if β ≤ 0 and the oob error to +Inf if oob == false.\n\nNotes :\n\nEach individual decision tree is built using bootstrap over the data, i.e. \"sampling N records with replacement\" (hence, some records appear multiple times and some records do not appear in the specific tree training). The maxFeature injects further variability and reduces the correlation between the forest trees.\nThe predictions of the \"forest\" (using the function predict()) are then the aggregated predictions of the individual trees (from which the name \"bagging\": boostrap aggregating).\nThis function optionally reports a weight distribution of the performances of eanch individual trees, as measured using the records he has not being trained with. These weights can then be (optionally) used in the predict function. The parameter β ≥ 0 regulate the distribution of these weights: larger is β, the greater the importance (hence the weights) attached to the best-performing trees compared to the low-performing ones. Using these weights can significantly improve the forest performances (especially using small forests), however the correct value of β depends on the problem under exam (and the chosen caratteristics of the random forest estimator) and should be cross-validated to avoid over-fitting.\nNote that this function uses multiple threads if these are available. You can check the number of threads available with Threads.nthreads(). To set the number of threads in Julia either set the environmental variable JULIA_NUM_THREADS (before starting Julia) or start Julia with the command line option --threads (most integrated development editors for Julia already set the number of threads to 4).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.buildTree-Union{Tuple{Ty}, Tuple{Any,AbstractArray{Ty,1}}} where Ty","page":"Trees","title":"BetaML.Trees.buildTree","text":"buildTree(x, y, depth; maxDepth, minGain, minRecords, maxFeatures, splittingCriterion, forceClassification)\n\nBuilds (define and train) a Decision Tree.\n\nGiven a dataset of features x and the corresponding dataset of labels y, recursivelly build a decision tree by finding at each node the best question to split the data untill either all the dataset is separated or a terminal condition is reached. The given tree is then returned.\n\nParameters:\n\nx: The dataset's features (N × D)\ny: The dataset's labels (N × 1)\nmaxDepth: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: N, i.e. no limits]\nminGain: The minimum information gain to allow for a node's partition [def: 0]\nminRecords:  The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmaxFeatures: The maximum number of (random) features to consider at each partitioning [def: D, i.e. look at all features]\nsplittingCriterion: Either gini, entropy or variance (see infoGain ) [def: gini for categorical labels (classification task) and variance for numerical labels(regression task)]\nforceClassification: Weather to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: false]\nrng: Random Number Generator ((see FIXEDSEED)) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nMissing data (in the feature dataset) are supported.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.findBestSplit-Union{Tuple{Ty}, Tuple{Any,AbstractArray{Ty,1},Any}} where Ty","page":"Trees","title":"BetaML.Trees.findBestSplit","text":"findBestSplit(x,y;maxFeatures,splittingCriterion)\n\nFind the best possible split of the database.\n\nFind the best question to ask by iterating over every feature / value and calculating the information gain.\n\nParameters:\n\nx: The feature dataset\ny: The labels dataset\nmaxFeatures: Maximum number of (random) features to look up for the \"best split\"\nsplittingCriterion: The metric to define the \"impurity\" of the labels\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.infoGain-Tuple{Any,Any,Any}","page":"Trees","title":"BetaML.Trees.infoGain","text":"infoGain(left, right, parentUncertainty; splittingCriterion)\n\nCompute the information gain of a specific partition.\n\nCompare the \"information gain\" my measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items.\n\nParameters:\n\nleftY:  Child #1 labels\nrightY: Child #2 labels\nparentUncertainty: \"Impurity\" of the labels of the parent node\nsplittingCriterion: Metric to adopt to determine the \"impurity\" (see below)\n\nYou can use your own function as the metric. We provide the following built-in metrics:\n\ngini (categorical)\nentropy (categorical)\nvariance (numerical)\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.match-Union{Tuple{Tx}, Tuple{BetaML.Trees.Question{Tx},Any}} where Tx","page":"Trees","title":"BetaML.Trees.match","text":"match(question, x)\n\nReturn a dicotomic answer of a question when applied to a given feature record.\n\nIt compares the feature value in the given record to the value stored in the question. Numerical features are compared in terms of disequality (\">=\"), while categorical features are compared in terms of equality (\"==\").\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.oobError-Union{Tuple{Ty}, Tuple{BetaML.Trees.Forest{Ty},Any,Any}} where Ty","page":"Trees","title":"BetaML.Trees.oobError","text":"oobError(forest,x,y)\n\nComute the Out-Of-Bag error, an estimation of the validation error.\n\nThis function is called at time of train the forest if the parameter oob is true, or can be used later to get the oob error on an already trained forest.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.partition-Union{Tuple{Tx}, Tuple{BetaML.Trees.Question{Tx},Any,Any}} where Tx","page":"Trees","title":"BetaML.Trees.partition","text":"partition(question,x)\n\nDicotomically partitions a dataset x given a question.\n\nFor each row in the dataset, check if it matches the question. If so, add it to 'true rows', otherwise, add it to 'false rows'. Rows with missing values on the question column are assigned randomly proportionally to the assignment of the non-missing rows.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predict-Union{Tuple{Ty}, Tuple{BetaML.Trees.Forest{Ty},Any}} where Ty","page":"Trees","title":"BetaML.Trees.predict","text":"predict(forest,x)\n\nPredict the labels of a feature dataset.\n\nFor each record of the dataset and each tree of the \"forest\", recursivelly traverse the tree to find the prediction most opportune for the given record. If the labels the tree has been trained with are numeric, the prediction is also numeric (the mean of the different trees predictions, in turn the mean of the labels of the training records ended in that leaf node). If the labels were categorical, the prediction is a dictionary with the probabilities of each item and in such case the probabilities of the different trees are averaged to compose the forest predictions. This is a bit different than most other implementations where the mode instead is reported.\n\nIn the first case (numerical predictions) use meanRelError(ŷ,y) to assess the mean relative error, in the second case you can use accuracy(ŷ,y).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predict-Union{Tuple{Ty}, Tuple{Tx}, Tuple{Union{BetaML.Trees.DecisionNode{Tx}, BetaML.Trees.Leaf{Ty}},Any}} where Ty where Tx","page":"Trees","title":"BetaML.Trees.predict","text":"predict(tree,x)\n\nPredict the labels of a feature dataset.\n\nFor each record of the dataset, recursivelly traverse the tree to find the prediction most opportune for the given record. If the labels the tree has been trained with are numeric, the prediction is also numeric. If the labels were categorical, the prediction is a dictionary with the probabilities of each item.\n\nIn the first case (numerical predictions) use meanRelError(ŷ,y) to assess the mean relative error, in the second case you can use accuracy(ŷ,y).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predictSingle-Union{Tuple{Ty}, Tuple{BetaML.Trees.Forest{Ty},Any}} where Ty","page":"Trees","title":"BetaML.Trees.predictSingle","text":"predictSingle(forest,x)\n\nPredict the label of a single feature record. See predict.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predictSingle-Union{Tuple{Ty}, Tuple{Tx}, Tuple{Union{BetaML.Trees.DecisionNode{Tx}, BetaML.Trees.Leaf{Ty}},Any}} where Ty where Tx","page":"Trees","title":"BetaML.Trees.predictSingle","text":"predictSingle(tree,x)\n\nPredict the label of a single feature record. See predict.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.updateTreesWeights!-Union{Tuple{Ty}, Tuple{BetaML.Trees.Forest{Ty},Any,Any}} where Ty","page":"Trees","title":"BetaML.Trees.updateTreesWeights!","text":"updateTreesWeights!(forest,x,y;β)\n\nUpdate the weights of each tree (to use in the prediction of the forest) based on the error of the individual tree computed on the records on which it has not been trained. As training a forest is expensive, this function can be used to \"just\" upgrade the trees weights using different betas, without retraining the model.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#The-BetaML.Utils-Module","page":"Utils","title":"The BetaML.Utils Module","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Utils\n","category":"page"},{"location":"Utils.html#BetaML.Utils","page":"Utils","title":"BetaML.Utils","text":"Utils module\n\nProvide shared utility functions for various machine learning algorithms. You don't usually need to import from this module, as each other module (Nn, Perceptron, Clusters,...) reexport it.\n\n\n\n\n\n","category":"module"},{"location":"Utils.html#Module-Index","page":"Utils","title":"Module Index","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Utils.html#Detailed-API","page":"Utils","title":"Detailed API","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]","category":"page"},{"location":"Utils.html#BetaML.Utils.FIXEDRNG","page":"Utils","title":"BetaML.Utils.FIXEDRNG","text":"FIXEDRNG\n\nFixed ring to allow reproducible results\n\nUse it with:\n\nmyAlgorithm(;rng=FIXEDRNG)             # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=StableRNG(FIXEDSEED)) # always produce the same result (new rng object on each call)\n\n\n\n\n\n","category":"constant"},{"location":"Utils.html#BetaML.Utils.FIXEDSEED","page":"Utils","title":"BetaML.Utils.FIXEDSEED","text":"FIXEDSEED\n\nFixed seed to allow reproducible results. This is the seed used to obtain the same results under unit tests.\n\nUse it with:\n\nmyAlgorithm(;rng=FIXEDRNG)             # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=StableRNG(FIXEDSEED)) # always produce the same result (new rng object on each call)\n\n\n\n\n\n","category":"constant"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{AbstractArray{T,1},AbstractArray{T,1}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y;ignoreLabels=false) - Categorical error (T vs T)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Array{Dict{T,Float64},1},Array{T,1}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Array{T,1},Int64}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic prediction of a single datapoint (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Array{T,2},Array{Int64,1}}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic predictions of a dataset (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.reshape-Union{Tuple{T}, Tuple{T,Vararg{Any,N} where N}} where T<:Number","page":"Utils","title":"Base.reshape","text":"reshape(myNumber, dims..) - Reshape a number as a n dimensional Array \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractArray{T,1},AbstractArray{T,1}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;ignoreLabels=false) - Categorical accuracy between two vectors (T vs T). If \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Array{Dict{T,Float64},1},Array{T,1}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: A narray where each item is the estimated probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Array{T,1},Int64}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint (PMF vs Int).\n\nUse the parameter tol [def: 1] to determine the tollerance of the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Array{T,2},Array{Int64,1}}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol,ignoreLabels)\n\nCategorical accuracy with probabilistic predictions of a dataset (PMF vs Int).\n\nParameters:\n\nŷ: An (N,K) matrix of probabilities that each hat y_n record with n in 1N  being of category k with k in 1K.\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\nignoreLabels: Whether to ignore the specific label order in y. Useful for unsupervised learning algorithms where the specific label order don't make sense [def: false]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Dict{T,Float64},T}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: The returned probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.aic-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.aic","text":"aic(lL,k) -  Akaike information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.autoJacobian-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.autoJacobian","text":"autoJacobian(f,x;nY)\n\nEvaluate the Jacobian using AD in the form of a (nY,nX) madrix of first derivatives\n\nParameters:\n\nf: The function to compute the Jacobian\nx: The input to the function where the jacobian has to be computed\nnY: The number of outputs of the function f [def: length(f(x))]\n\nReturn values:\n\nAn Array{Float64,2} of the locally evaluated Jacobian\n\nNotes:\n\nThe nY parameter is optional. If provided it avoids having to compute f(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.batch-Tuple{Integer,Integer}","page":"Utils","title":"BetaML.Utils.batch","text":"batch(n,bSize;sequential=false,rng)\n\nReturn a vector of bSize vectors of indeces from 1 to n. Randomly unless the optional parameter sequential is used.\n\nExample:\n\njulia julia> Utils.batch(6,2,sequential=true) 3-element Array{Array{Int64,1},1}:  [1, 2]  [3, 4]  [5, 6]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.bic-Tuple{Any,Any,Any}","page":"Utils","title":"BetaML.Utils.bic","text":"bic(lL,k,n) -  Bayesian information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.celu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.celu","text":"celu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.classCounts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.classCounts","text":"classCounts(x)\n\nReturn a dictionary that counts the number of each unique item (rows) in a dataset.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.colsWithMissing-Tuple{Any}","page":"Utils","title":"BetaML.Utils.colsWithMissing","text":"colsWithMissing(x)\n\nRetuyrn an array with the ids of the columns where there is at least a missing value.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.cosine_distance-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.cosine_distance","text":"Cosine distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.crossEntropy-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.crossEntropy","text":"crossEntropy(ŷ, y; weight)\n\nCompute the (weighted) cross-entropy between the predicted and the sampled probability distributions.\n\nTo be used in classification problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dcelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dcelu","text":"dcelu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.delu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.delu","text":"delu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dmish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dmish","text":"dmish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dplu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dplu","text":"dplu(x;α=0.1,c=1) \n\nPiecewise Linear Unit derivative \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.drelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.drelu","text":"drelu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsigmoid","text":"dsigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftmax","text":"dsoftmax(x; β=1) \n\nDerivative of the softmax function \n\nhttps://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftplus","text":"dsoftplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dtanh-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dtanh","text":"dtanh(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.elu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.elu","text":"elu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.entropy-Tuple{Any}","page":"Utils","title":"BetaML.Utils.entropy","text":"entropy(x)\n\nCalculate the entropy for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Gini_impurity\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getPermutations-Union{Tuple{AbstractArray{T,1}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.getPermutations","text":"getPermutations(v::AbstractArray{T,1};keepStructure=false)\n\nReturn a vector of either (a) all possible permutations (uncollected) or (b) just those based on the unique values of the vector\n\nUseful to measure accuracy where you don't care about the actual name of the labels, like in unsupervised classifications (e.g. clustering)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getScaleFactors-Tuple{Any}","page":"Utils","title":"BetaML.Utils.getScaleFactors","text":"getScaleFactors(x;skip)\n\nReturn the scale factors (for each dimensions) in order to scale a matrix X (n,d) such that each dimension has mean 0 and variance 1.\n\nParameters\n\nx: the (n × d) dimension matrix to scale on each dimension d\nskip: an array of dimension index to skip the scaling [def: []]\n\nReturn\n\nA touple whose first elmement is the shift and the second the multiplicative\n\nterm to make the scale.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.gini-Tuple{Any}","page":"Utils","title":"BetaML.Utils.gini","text":"gini(x)\n\nCalculate the Gini Impurity for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Information_gain\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerDecoder-Union{Tuple{T}, Tuple{Any,AbstractArray{T,1}}} where T","page":"Utils","title":"BetaML.Utils.integerDecoder","text":"integerDecoder(y,target::AbstractVector{T};unique)\n\nDecode an array of integers to an array of T corresponding to the elements of target\n\nParameters:\n\ny: The vector to decode\ntarget: The vector of elements to use for the encoding\nunique: Wether target is already made of unique elements [def: true]\n\nReturn:\n\nA vector of length(y) elements corresponding to the (unique) target elements at the position y\n\nExample:\n\njulia> integerDecoder([1, 2, 2, 3, 2, 1],[\"aa\",\"cc\",\"bb\"]) # out: [\"aa\",\"cc\",\"cc\",\"bb\",\"cc\",\"aa\"]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerEncoder-Tuple{AbstractArray{T,1} where T}","page":"Utils","title":"BetaML.Utils.integerEncoder","text":"integerEncoder(y;unique)\n\nEncode an array of T to an array of integers using the their position in the unique vector if the input array\n\nParameters:\n\ny: The vector to encode\nunique: Wether the vector to encode is already made of unique elements [def: false]\n\nReturn:\n\nA vector of [1,length(Y)] integers corresponding to the position of each element in the \"unique\" version of the original input\n\nNote:\n\nAttention that while this function creates a ordered (and sortable) set, it is up to the user to be sure that this \"property\" is not indeed used in his code if the unencoded data is indeed unordered.\n\nExample:\n\njulia> integerEncoder([\"aa\",\"cc\",\"cc\",\"bb\",\"cc\",\"aa\"]) # out: [1, 2, 2, 3, 2, 1]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.issortable-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T","page":"Utils","title":"BetaML.Utils.issortable","text":"Return wheather an array is sortable, i.e. has methos issort defined\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l1_distance-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.l1_distance","text":"L1 norm distance (aka Manhattan Distance)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2_distance-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.l2_distance","text":"Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2²_distance-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.l2²_distance","text":"Squared Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.lse-Tuple{Any}","page":"Utils","title":"BetaML.Utils.lse","text":"LogSumExp for efficiently computing log(sum(exp.(x))) \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.makeMatrix-Tuple{AbstractArray}","page":"Utils","title":"BetaML.Utils.makeMatrix","text":"Transform an Array{T,1} in an Array{T,2} and leave unchanged Array{T,2}.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.meanDicts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.meanDicts","text":"meanDicts(dicts)\n\nCompute the mean of the values of an array of dictionaries.\n\nGiven dicts an array of dictionaries, meanDicts first compute the union of the keys and then average the values. If the original valueas are probabilities (non-negative items summing to 1), the result is also a probability distribution.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.meanRelError-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.meanRelError","text":"meanRelError(ŷ,y;normDim=true,normRec=true,p=1)\n\nCompute the mean relative error (l-1 based by default) between ŷ and y.\n\nThere are many ways to compute a mean relative error. In particular, if normRec (normDim) is set to true, the records (dimensions) are normalised, in the sense that it doesn't matter if a record (dimension) is bigger or smaller than the others, the relative error is first computed for each record (dimension) and then it is averaged. With both normDim and normRec set to false the function returns the relative mean error; with both set to true (default) it returns the mean relative error (i.e. with p=1 the \"mean absolute percentage error (MAPE)\") The parameter p [def: 1] controls the p-norm used to define the error.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.mish","text":"mish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractArray{Dict{T,Float64},N} where N}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.mode","text":"mode(dicts)\n\nGiven a vector of dictionaries representing probabilities it returns the mode of each element in terms of the key\n\nUse it to return a unique value from a multiclass classifier returning probabilities.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.oneHotEncoder","page":"Utils","title":"BetaML.Utils.oneHotEncoder","text":"oneHotEncoder(y,d;count)\n\nEncode arrays (or arrays of arrays) of integer data as 0/1 matrices\n\nParameters:\n\ny: The data to convert (integer, array or array of arrays of integers)\nd: The number of dimensions in the output matrik. [def: maximum(maximum.(Y))]\ncount: Wether to count multiple instances on the same dimension/record or indicate just presence. [def: false]\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.partition-Union{Tuple{T}, Tuple{AbstractArray{T,1},AbstractArray{Float64,1}}} where T<:AbstractArray","page":"Utils","title":"BetaML.Utils.partition","text":"partition(data,parts;shuffle=true)\n\nPartition (by rows) one or more matrices according to the shares in parts.\n\nParameters\n\ndata: A matrix/vector or a vector of matrices/vectors\nparts: A vector of the required shares (must sum to 1)\nshufle: Whether to randomly shuffle the matrices (preserving the relative order between matrices)\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nExample:\n\njulia julia> x = [1:10 11:20] julia> y = collect(31:40) julia> ((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.7,0.3])\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.pca-Tuple{Any}","page":"Utils","title":"BetaML.Utils.pca","text":"pca(X;K,error)\n\nPerform Principal Component Analysis returning the matrix reprojected among the dimensions of maximum variance.\n\nParameters:\n\nX : The (N,D) data to reproject\nK : The number of dimensions to maintain (with K<=D) [def: nothing]\nerror: The maximum approximation error that we are willing to accept [def: 0.05]\n\nReturn:\n\nA named tuple with:\nX: The reprojected (NxK) matrix with the column dimensions organized in descending order of of the proportion of explained variance\nK: The number of dimensions retieved\nerror: The actual proportion of variance not explained in the reprojected dimensions\nP: The (D,K) matrix of the eigenvectors associated to the K-largest eigenvalues used to reproject the data matrix\nexplVarByDim: An array of dimensions D with the share of the cumulative variance explained by dimensions (the last element being always 1.0)\n\nNotes:\n\nIf K is provided, the parameter error has no effect.\nIf one doesn't know a priori the error that she/he is willling to accept, nor the wished number of dimensions, he/she can run this pca function with out = pca(X,K=size(X,2)) (i.e. with K=D), analise the proportions of explained cumulative variance by dimensions in out.explVarByDim, choose the number of dimensions K according to his/her needs and finally pick from the reprojected matrix only the number of dimensions needed, i.e. out.X[:,1:K].\n\nExample:\n\njulia> X = [1 10 100; 1.1 15 120; 0.95 23 90; 0.99 17 120; 1.05 8 90; 1.1 12 95]\n6×3 Array{Float64,2}:\n 1.0   10.0  100.0\n 1.1   15.0  120.0\n 0.95  23.0   90.0\n 0.99  17.0  120.0\n 1.05   8.0   90.0\n 1.1   12.0   95.0\n julia> X = pca(X,error=0.05).X\n6×2 Array{Float64,2}:\n  3.1783   100.449\n  6.80764  120.743\n 16.8275    91.3551\n  8.80372  120.878\n  1.86179   90.3363\n  5.51254   95.5965\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.plu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.plu","text":"plu(x;α=0.1,c=1) \n\nPiecewise Linear Unit \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.polynomialKernel-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.polynomialKernel","text":"Polynomial kernel parametrised with c=0 and d=2 (i.e. a quadratic kernel). For other cᵢ and dᵢ use K = (x,y) -> polynomialKernel(x,y,c=cᵢ,d=dᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.radialKernel-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.radialKernel","text":"Radial Kernel (aka RBF kernel) parametrised with γ=1/2. For other gammas γᵢ use K = (x,y) -> radialKernel(x,y,γ=γᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.relu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.relu","text":"relu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.scale","page":"Utils","title":"BetaML.Utils.scale","text":"scale(x,scaleFactors;rev)\n\nPerform a linear scaling of x using scaling factors scaleFactors.\n\nParameters\n\nx: The (n × d) dimension matrix to scale on each dimension d\nscalingFactors: A tuple of the constant and multiplicative scaling factor\n\nrespectively [def: the scaling factors needed to scale x to mean 0 and variance 1]\n\nrev: Whether to invert the scaling [def: false]\n\nReturn\n\nThe scaled matrix\n\nNotes:\n\nAlso available scale!(x,scaleFactors) for in-place scaling.\nRetrieve the scale factors with the getScaleFactors() function\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.sigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.sigmoid","text":"sigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softmax","text":"softmax (x; β=1) \n\nThe input x is a vector. Return a PMF\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softplus","text":"softplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.squaredCost-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.squaredCost","text":"squaredCost(ŷ,y)\n\nCompute the squared costs between a vector of prediction and one of observations as (1/2)*norm(y - ŷ)^2.\n\nAside the 1/2 term correspond to the squared l-2 norm distance and when it is averaged on multiple datapoints corresponds to the Mean Squared Error. It is mostly used for regression problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.sterling-Tuple{BigInt,BigInt}","page":"Utils","title":"BetaML.Utils.sterling","text":"Sterling number: number of partitions of a set of n elements in k sets \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.variance-Tuple{Any}","page":"Utils","title":"BetaML.Utils.variance","text":"variance(x) - population variance\n\n\n\n\n\n","category":"method"}]
}
