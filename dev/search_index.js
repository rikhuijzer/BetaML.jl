var documenterSearchIndex = {"docs":
[{"location":"index.html#![BLogos](assets/BetaML_logo_30x30.png)-BetaML.jl-Documentation","page":"Index","title":"(Image: BLogos) BetaML.jl Documentation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Welcome to the documentation of the Beta Machine Learning toolkit.","category":"page"},{"location":"index.html#About","page":"Index","title":"About","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML toolkit provides classical algorithms written in the Julia programming language useful to \"learn\" the relationship between some inputs and some outputs, with the objective to make accurate predictions of the output given new inputs (\"supervised machine learning\") or to better understand the structure of the data, perhaps hidden because of the high dimensionality (\"unsupervised machine learning\").","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"While specific packages exist for state-of-the art implementations of these algorithms (see the section \"Alternative Packages\"), thanks to the Just-In-Time compilation nature of Julia, BetaML is reasonably fast for datasets that fits in memory.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside the algorithms themselves, BetaML provides many \"utility\" functions. Because algorithms are all self-contained in the library itself (you are invited to explore their source code by typing @edit functionOfInterest(par1,par2)), the utility functions have APIs that are coordinated with the algorithms, facilitating the \"preparation\" of the data for the analysis, the evaluation of the models or the implementation of several models in chains (pipelines). While BetaML doesn't provide itself tools for hyper-parameters optimisation or complex pipeline building tools, most models have an interface for the MLJ framework that allows it.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Aside Julia, BetaML can be accessed in R or Python using respectively JuliaCall and PyJulia. See here for a tutorial or the examples for some actual use of the Beta Machine Learning Toolkit in R/Python.","category":"page"},{"location":"index.html#Installation","page":"Index","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The BetaML package is now included in the standard Julia register, install it with:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"] add BetaML","category":"page"},{"location":"index.html#Loading-the-module(s)","page":"Index","title":"Loading the module(s)","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"This package is split in several submodules, but all modules are re-exported at the root module level. This means that you can access their functionality by simply using BetaML.","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML\nmyLayer = DenseLayer(2,3) # DenseLayer is defined in the Nn submodule\nres     = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1]) # kernelPerceptron is defined in the Perceptron module\n@edit DenseLayer(2,3)     # Open a text editor with to the relevant source code","category":"page"},{"location":"index.html#Usage","page":"Index","title":"Usage","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"Documentation for most algorithms can be retrieved using the inline Julia help system (just press the question mark ? and then, on the special help prompt help?>, type the function name).","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"For a list of supported algorithms please look at the individual modules:","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"BetaML.Perceptron: The Perceptron, Kernel Perceptron and Pegasos classification algorithms;\nBetaML.Trees: The Decision Trees and Random Forests algorithms for classification or regression (with missing values supported);\nBetaML.Nn: Implementation of Artificial Neural Networks;\nBetaML.Clustering`: Clustering algorithms (Kmeans, Mdedoids, EM/GMM) and missing imputation / collaborative filtering / recommandation systems using clusters;\nBetaML.Utils`: Various utility functions (scale, one-hot, distances, kernels, pca, accuracy/error measures..).","category":"page"},{"location":"index.html#Examples","page":"Index","title":"Examples","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"See the tutorial page for a more hand-to-hand to the below and other examples","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Using an Artificial Neural Network for multinomial categorisation","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"# Load Modules\nusing BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\ny_oh     = oneHotEncoder(y) # Convert to One-hot representation (e.g. 2 => [0 1 0], 3 => [0 0 1])\n\n# Split the data in training/testing sets\n((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,y,y_oh],[0.8,0.2],shuffle=false)\n(ntrain, ntest) = size.([xtrain,xtest],1)\n\n# Define the Artificial Neural Network model\nl1   = DenseLayer(4,10,f=relu) # Activation function is ReLU\nl2   = DenseLayer(10,3)        # Activation function is identity by default\nl3   = VectorFunctionLayer(3,3,f=softMax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once\nmynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Multinomial logistic regression Model Sepal\") # Build the NN and use the squared cost (aka MSE) as error function\n\n# Training it (default to ADAM)\nres = train!(mynn,scale(xtrain),ytrain_oh,epochs=100,batchSize=6) # Use optAlg=SGD (Stochastic Gradient Descent) by default\n\n# Test it\nŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function\nŷtest         = predict(mynn,scale(xtest))\ntrainAccuracy = accuracy(ŷtrain,ytrain,tol=1) # 0.983\ntestAccuracy  = accuracy(ŷtest,ytest,tol=1)   # 1.0\n\n# Visualise results\ntestSize    = size(ŷtest,1)\nŷtestChosen =  [argmax(ŷtest[i,:]) for i in 1:testSize]\ngroupedbar([ytest ŷtestChosen], label=[\"ytest\" \"ŷtest (est)\"], title=\"True vs estimated categories\") # All records correctly labelled !\nplot(0:res.epochs,res.ϵ_epochs, ylabel=\"epochs\",xlabel=\"error\",legend=nothing,title=\"Avg. error per epoch on the Sepal dataset\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Using the Expectation-Maximisation algorithm for clustering","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"using BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(pathof(BetaML)),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\nx        = scale(x) # normalise all dimensions to (μ=0, σ=1)\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\n\n# Get some ranges of minVariance and minCovariance to test\nminVarRange   = collect(0.04:0.05:1.5)\nminCovarRange = collect(0:0.05:1.45)\n\n# Run the gmm(em) algorithm for the various cases...\nsphOut  = [gmm(x,3,mixtures=[SphericalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE) for v in minVarRange, cv in minCovarRange[1:1]]\ndiagOut  = [gmm(x,3,mixtures=[DiagonalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange[1:1]]\nfullOut = [gmm(x,3,mixtures=[FullGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange]\n\n# Get the Bayesian information criterion (AIC is also available)\nsphBIC = [sphOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\ndiagBIC = [diagOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\nfullBIC = [fullOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\n# Compare the accuracy with true categories\nsphAcc  = [accuracy(sphOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\ndiagAcc = [accuracy(diagOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\nfullAcc = [accuracy(fullOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\nplot(minVarRange,[sphBIC diagBIC fullBIC[:,1] fullBIC[:,15] fullBIC[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"BIC\", xlabel=\"minVariance\")\nplot(minVarRange,[sphAcc diagAcc fullAcc[:,1] fullAcc[:,15] fullAcc[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"Accuracies\", xlabel=\"minVariance\")","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"Further examples","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"We also provide some Jupyter notebooks that can be run online without installing anything, so you can start playing with the library in minutes. Finally, you may want to give a look at the \"test\" folder. While the primary reason of the scripts under the \"test\" folder is to provide automatic testing of the BetaML toolkit, they can also be used to see how functions should be called, as virtually all functions provided by BetaML are tested there.","category":"page"},{"location":"index.html#Acknowledgements","page":"Index","title":"Acknowledgements","text":"","category":"section"},{"location":"index.html","page":"Index","title":"Index","text":"The development of this package at the Bureau d'Economie Théorique et Appliquée (BETA, Nancy) was supported by the French National Research Agency through the Laboratory of Excellence ARBRE, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).","category":"page"},{"location":"index.html","page":"Index","title":"Index","text":"(Image: BLogos)","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Getting started/betaml_tutorial_getting_started.jl\"","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html#This-is-markdown-title","page":"-","title":"This is markdown title","text":"","category":"section"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"This is also markdown","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"# This is a normal comment","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"a = 1\nb = a + 1\nprintln(b)","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html#Randomness","page":"-","title":"Randomness","text":"","category":"section"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"Most models have some stochastic components and support a rng parameter. By default, the outputs of these models will hence not be absolutelly equal on each run. If you want to be sure that the output of a model remain constant given the same inputs you can pass a fixed Random Number Generator to the rng parameter. Use it with:","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"myAlgorithm(;rng=FIXEDRNG)               # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=StableRNG(SOMEINTEGER)) # always produce the same result (new rng object on each call)","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"In particular, use rng=StableRNG(FIXEDSEED) to retrieve the exacty output as in the documentation or in the unit tests.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"Most of the stochasticity appears in training a model. However in few cases (e.g. decision trees with missing values) some stocasticity appears also in predicting new data with a trained model. In such cases the model doesn't stire the random seed, so that you can choose at predict time to use a fixed or a variable random seed.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"View this file on Github.","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/Getting started/betaml_tutorial_getting_started.html","page":"-","title":"-","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"EditURL = \"https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.jl\"","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html#A-regression-task:-the-prediction-of-bike-sharing-demand","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"","category":"section"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Data origin:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"original full dataset (by hour, not used here): https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nsimplified dataset (by day, with some simple scaling): https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec\ndescription: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/exam2019ace.pdf\ndata: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/bikesharing_day.csv.zip","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. Y_t = f(X_t) alone).","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html#Library-and-data-loading","page":"A regression task: the prediction of  bike  sharing demand","title":"Library and data loading","text":"","category":"section"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We first load all the packages we are going to use","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"using  LinearAlgebra, Random, Statistics, DataFrames, CSV, Plots, Pipe, BenchmarkTools, BetaML\nimport Distributions: Uniform\nimport DecisionTree, Flux ## For comparisions","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here we load the data from a csv provided by the BataML package","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"baseDir = joinpath(dirname(pathof(BetaML)),\"..\",\"docs\",\"src\",\"tutorials\",\"A regression task - sharing bike demand prediction\")\ndata    = CSV.File(joinpath(baseDir,\"data\",\"bike_sharing_day.csv\"),delim=',') |> DataFrame\ndescribe(data)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The variable we want to learn to predict is cnt, the total demand of bikes for a given day. Even if it is indeed an integer, we treat it as a continuous variable, so each single prediction will be a scalar Y in mathbbR.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot(data.cnt, title=\"Daily bike sharing rents (2Y)\", label=nothing)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html#Decision-Trees","page":"A regression task: the prediction of  bike  sharing demand","title":"Decision Trees","text":"","category":"section"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We start our regression task with Decision Trees","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html#Data-preparation","page":"A regression task: the prediction of  bike  sharing demand","title":"Data preparation","text":"","category":"section"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The first step is to prepare the data for the analysis. This indeed depends already on the model we want to employ, as some models \"accept\" everything as input, no matter if the data is numerical or categorical, if it has missing values or not... while other models are instead much more exigents, and require more work to \"clean up\" our dataset. Here we start using  Decision Tree and Random Forest models that belong to the first group, so the only things we have to do is to select the variables in input (the \"feature matrix\", we wil lindicate it with \"X\") and those representing our output (the values we want to learn to predict, we call them \"y\"):","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"x    = convert(Matrix,hcat(data[:,[:instant,:season,:yr,:mnth,:holiday,:weekday,:workingday,:weathersit,:temp,:atemp,:hum,:windspeed]]))\ny    = data[:,16]","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now split the dataset between the data we will use for training the algorithm (xtrain/ytrain), those for selecting the hyperparameters (xval/yval) and finally those for testing the quality of the algoritm with the optimal hyperparameters (xtest/ytest). We use the partition function specifying the share we want to use for these three different subsets, here 75%, 12.5% and 12.5 respectively. As our data represents indeed a time serie, we want our model to be able to predict future demand of bike sharing from past, observed rented bikes, so we do not shuffle the datasets as it would be the default.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)\n(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now \"tune\" our model so-called hyperparameters, i.e. choose the best exogenous parameters of our algorithm, where \"best\" refers to some minimisation of a \"loss\" function between the true and the predicted value. To make the comparision we use a specific \"validation\" subset of data (xval and yval).","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"BetaML doesn't have a dedicated function for hyperparameters optimisation, but it is easy to write some custom julia code, at least for a simple grid-based \"search\". Indeed one of the main reasons that a dedicated function exists in other Machine Learning libraries is that loops in other languages are slow, but this is not a problem in julia, so we can retain the flexibility to write the kind of hyperparameter tuning that best fits our needs.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Below is an example of a possible such function. Note there are more \"elegant\" ways to code it, but this one does the job. We will see the various functions inside tuneHyperParameters() in a moment. For now let's going just to observe that tuneHyperParameters just loops over all the possible hyperparameters and selects the one where the error between xval and yval is minimised. For the meaning of the various hyperparameter, consult the documentation of the buildTree and buildForest functions. The function uses multiple threads, so we calls generateParallelRngs() (in the BetaML.Utils submodule) to generate thread-safe random number generators and locks the comparision step.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"function tuneHyperParameters(model,xtrain,ytrain,xval,yval;maxDepthRange=15:15,maxFeaturesRange=size(xtrain,2):size(xtrain,2),nTreesRange=20:20,βRange=0:0,minRecordsRange=2:2,repetitions=5,rng=Random.GLOBAL_RNG)\n    # We start with an infinitely high error\n    bestRme         = +Inf\n    bestMaxDepth    = 1\n    bestMaxFeatures = 1\n    bestMinRecords  = 2\n    bestNTrees      = 1\n    bestβ           = 0\n    compLock        = ReentrantLock()\n\n    # Generate one random number generator per thread\n    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generateParallelRngs\n    rngs = generateParallelRngs(rng,Threads.nthreads())\n\n    # We loop over all possible hyperparameter combinations...\n    parLengths = (length(maxDepthRange),length(maxFeaturesRange),length(minRecordsRange),length(nTreesRange),length(βRange))\n    Threads.@threads for ij in CartesianIndices(parLengths) ## This to avoid many nested for loops\n           (maxDepth,maxFeatures,minRecords,nTrees,β)   = (maxDepthRange[Tuple(ij)[1]], maxFeaturesRange[Tuple(ij)[2]], minRecordsRange[Tuple(ij)[3]], nTreesRange[Tuple(ij)[4]], βRange[Tuple(ij)[5]]) ## The specific hyperparameters of this nested loop\n           tsrng = rngs[Threads.threadid()] ## The random number generator is specific for each thread..\n           joinedIndx = LinearIndices(parLengths)[ij]\n           # And here we make the seeding depending on the id of the loop, not the thread: hence we get the same results indipendently of the number of threads\n           Random.seed!(tsrng,masterSeed+joinedIndx*10)\n           totAttemptError = 0.0\n           # We run several repetitions with the same hyperparameter combination to account for stochasticity...\n           for r in 1:repetitions\n              if model == \"DecisionTree\"\n                 # Here we train the Decition Tree model\n                 myTrainedModel = buildTree(xtrain,ytrain, maxDepth=maxDepth,maxFeatures=maxFeatures,minRecords=minRecords,rng=tsrng)\n              else\n                 # Here we train the Random Forest model\n                 myTrainedModel = buildForest(xtrain,ytrain,nTrees,maxDepth=maxDepth,maxFeatures=maxFeatures,minRecords=minRecords,β=β,rng=tsrng)\n              end\n              # Here we make prediciton with this trained model and we compute its error\n              ŷval   = predict(myTrainedModel, xval,rng=tsrng)\n              rmeVal = meanRelError(ŷval,yval,normRec=false)\n              totAttemptError += rmeVal\n           end\n           avgAttemptedDepthError = totAttemptError / repetitions\n           begin\n               lock(compLock) ## This step can't be run in parallel...\n               try\n                   # Select this specific combination of hyperparameters if the error is the lowest\n                   if avgAttemptedDepthError < bestRme\n                     bestRme         = avgAttemptedDepthError\n                     bestMaxDepth    = maxDepth\n                     bestMaxFeatures = maxFeatures\n                     bestNTrees      = nTrees\n                     bestβ           = β\n                     bestMinRecords  = minRecords\n                   end\n               finally\n                   unlock(compLock)\n               end\n           end\n    end\n    return (bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords,bestNTrees,bestβ)\nend","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now run the hyperparameter optimisation function with some \"reasonable\" ranges. To obtain replicable results we call tuneHyperParameters with rng=copy(FIXEDRNG), where FIXEDRNG is a fixed-seeded random number generator guaranteed to maintain the same stream of random numbers even between different julia versions. That's also what we use for our unit tests.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords) = tuneHyperParameters(\"DecisionTree\",xtrain,ytrain,xval,yval,\n           maxDepthRange=3:7,maxFeaturesRange=10:12,minRecordsRange=2:6,repetitions=10,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Now that we have found the \"optimal\" hyperparameters we can build (\"train\") our model using them:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"myTree = buildTree(xtrain,ytrain, maxDepth=bestMaxDepth, maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the time and memory usage of the training step of a decision tree:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime  buildTree(xtrain,ytrain, maxDepth=bestMaxDepth, maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Individual decision trees are blazing fast, among the fastest algorithms we could use.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The above buildTreefunction produces a DecisionTree object that can be used to make predictions given some new features, i.e. given some X matrix of (number of observations x dimensions), predict the corresponding Y vector of scalers in R.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest) = predict.([myTree], [xtrain,xval,xtest])","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Note that the above code uses the \"dot syntax\" to \"broadcast\" predict() over an array of label matrices. It is exactly equivalent to:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrain = predict(myTree, xtrain)\nŷval   = predict(myTree, xval)\nŷtest  = predict(myTree, xtest)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now compute the relative mean error for the training, the validation and the test set. The meanRelError is a very flexible error function. Without additional parameter, it computes, as the name says, the mean relative error, also known as the \"mean absolute percentage error\" (MAPE)](https://en.wikipedia.org/wiki/Meanabsolutepercentageerror)\") between an estimated and a true vector. However it can also compute the _relative mean error (as we do here), or use a p-norm higher than 1. The mean relative error enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the relative mean error the same relative error on larger observations (or dimensions) weights more. In this exercise we use the later, as our data has clearly some outlier days with very small rents, and we care more of avoiding our customers finding empty bike racks than having unrented bikes on the rack. Targeting a low mean average error would push all our predicitons down to try accomodate the low-level predicitons (to avoid a large relative error), and that's not what we want.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"For example let'c consider the following example:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"y     = [30,28,27,3,32,38]\nŷpref = [32,30,28,10,31,40]\nŷbad  = [29,25,24,5,28,35]","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Here ŷpref is an ipotetical output of a model that minimise the relative mean error, while ŷbad minimise the mean realative error","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"meanRelError.([ŷbad, ŷpref],[y,y],normRec=true) ## Mean relative error","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"meanRelError.([ŷbad, ŷpref],[y,y],normRec=false) ## Relative mean error","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot([y ŷbad ŷpref], colour=[:black :red :green], label=[\"obs\" \"bad est\" \"good est\"])","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can then compute the relative mean error for the decision tree","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can plot the true labels vs the estimated one for the three subsets...","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (DT)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (DT)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (DT)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Or we can visualise the true vs estimated bike shared on a temporal base. First on the full period (2 years) ...","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (DT)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and then focusing on the testing period","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (DT)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The predictions aren't so bad in this case, however decision trees are highly instable, and the output could have depended just from the specific initial random seed.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html#Random-Forests","page":"A regression task: the prediction of  bike  sharing demand","title":"Random Forests","text":"","category":"section"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Rather than trying to solve this problem using a single Decision Tree model, let's not try to use a Random Forest model. Random forests average the results of many different decision trees and provide a more \"stable\" result. Being made of many decision trees, random forests are hovever more computationally expensive to train, but luckily they tend to self-tune (or self-regularise). In particular the default maxDepth andmaxFeatures` shouldn't need tuning. We still tune however the model for other parameters, and in particular the β parameter, a prerogative of BetaML Random Forests that allows to assign more weigth to the best performing trees in the forest. It may be particularly important if there are many outliers in the data we don't want to \"learn\" from.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"minRecordsRange=[2,4,5]; nTreesRange=60:10:80; βRange=100:100:300\n(bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords,bestNTrees,bestβ) = tuneHyperParameters(\"RandomForest\",xtrain,ytrain,xval,yval,\n        maxDepthRange=size(xtrain,1):size(xtrain,1),maxFeaturesRange=Int(round(sqrt(size(xtrain,2)))):Int(round(sqrt(size(xtrain,2)))),\n        minRecordsRange=minRecordsRange,nTreesRange=nTreesRange,βRange=βRange,repetitions=5,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As for decision trees, once the hyper-parameters of the model are tuned we wan refit the model using the optimal parameters.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"myForest = buildForest(xtrain,ytrain, bestNTrees, maxDepth=bestMaxDepth,maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,β=bestβ,oob=true,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's now benchmark of the training of BetaML Random Forest model","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime buildForest(xtrain,ytrain, bestNTrees, maxDepth=bestMaxDepth,maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,β=bestβ,oob=true,rng=copy(FIXEDRNG));\nnothing #hide","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random forests are evidently slower than individual decision trees (approximatly by a factor  number of trees/number of threads), but are still relativly fast","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random forests support the so-called \"out-of-bag\" error, an estimation of the error that we would have when the model is applied on a testing sample. However in this case the oob reported is much smaller than the testing error we will find. This is due to the fact that the division between training/validation and testing in this exercise is not random, but has a temporal basis. It seems that in this example the data in validation/testing follows a different pattern/variance than those in training (in probabilistic terms, they are not i.i.d.).","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"oobError, trueTestMeanRelativeError  = myForest.oobError,meanRelError(ŷtest,ytest,normRec=true)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest)         = predict.([myForest], [xtrain,xval,xtest])\n(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"In this case we found an error very similar to the one employing a single decision tree. Let's print the observed data vs the estimated one using the random forest and then along the temporal axis:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (RF)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (RF)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (RF)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Full period plot (2 years):","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (RF)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Focus on the testing period:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (RF)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html#Comparison-with-DecisionTree.jl-random-forest","page":"A regression task: the prediction of  bike  sharing demand","title":"Comparison with DecisionTree.jl random forest","text":"","category":"section"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now compare our results with those obtained employing the same model in the DecisionTree package, using the default suggested hyperparameters:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"# Hyperparameters of the DecisionTree.jl random forest model\n\n\nn_subfeatures=-1; n_trees=bestNTrees; partial_sampling=1; max_depth=26\nmin_samples_leaf=bestMinRecords; min_samples_split=bestMinRecords; min_purity_increase=0.0; seed=3","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We train the model..","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"model = DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),\n                     n_subfeatures,\n                     n_trees,\n                     partial_sampling,\n                     max_depth,\n                     min_samples_leaf,\n                     min_samples_split,\n                     min_purity_increase;\n                     rng = seed)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"And we generate predictions and measure their error","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(ŷtrain,ŷval,ŷtest) = DecisionTree.apply_forest.([model],[xtrain,xval,xtest]);\nnothing #hide","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the DecisionTrees.jl Random Forest training","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime DecisionTree.apply_forest.([model],[xtrain,xval,xtest])","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Nothing to say, DecisionTrees.jl makes a unbelivable good job in optimising the algorithm. It is several order of magnitude faster than BetaML Random forests","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"However the error on the test set remains relativly high. The very low error level on the training set is a sign that it overspecialised on the training set, and we should have instead running a dedicated hyperparameter optimisation for the model.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Finally we plot the DecisionTree.jl predictions alongside the observed value:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (DT.jl RF)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, focusing on the testing data:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc  = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (DT.jl RF)\")\n\n### Conclusions of Decision Trees / Random Forests methods","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The error obtained employing DecisionTree.jl is significantly larger than those obtained with the BetaML random forest model, altought to be fair with DecisionTrees.jl we didn't tuned its hyper-parameters. Also, DecisionTree.jl random forest model is much faster. This is partially due by the fact that internally DecisionTree.jl models optimise the algorithm by sorting the observations. BetaML trees/forests don't employ this optimisation and hence it can work with true categorical data for which ordering is not defined. An other explanation of this difference in speed is that BetaML Ransom Forest models accept missing values within the feature matrix. To sum up, BetaML random forests are ideal algorithms when we want to obtain good predictions in the most simpler way, even without tuning the hyperparameters, and without spending time in cleaning (\"munging\") the feature matrix, as they accept almost \"any kind\" of data as it is.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html#Neural-Networks","page":"A regression task: the prediction of  bike  sharing demand","title":"Neural Networks","text":"","category":"section"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"BetaML provides only deep forward neural networks, artificial neural network units where the individual \"nodes\" are arranged in layers, from the input layer, where each unit holds the input coordinate, through various hidden layer transformations, until the actual output of the model:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(Image: Neural Networks)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"In this layerwise computation, each unit in a particular layer takes input from all the preceding layer units and it has its own parameters that are adjusted to perform the overall computation. The training of the network consists in retrieving the coefficients that minimise a loss function betwenn the output of the model and the known data. In particular, a deep (feedforward) neural network refers to a neural network that contains not only the input and output layers, but also hidden layers in between.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Neural networks accept only numerical inputs. We hence need to convert all categorical data in numerical units. A common approach is to use the so-called \"one-hot-encoding\" where the catagorical values are converted into indicator variables (0/1), one for each possible value. This can be done in BetaML using the oneHotEncoder function:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"seasonDummies  = convert(Array{Float64,2},oneHotEncoder(data[:,:season]))\nweatherDummies = convert(Array{Float64,2},oneHotEncoder(data[:,:weathersit]))\nwdayDummies    = convert(Array{Float64,2},oneHotEncoder(data[:,:weekday] .+ 1 ))\n\n# We compose the feature matrix with the new dimensions obtained from the oneHotEncoder functions\nx = hcat(Matrix{Float64}(data[:,[:instant,:yr,:mnth,:holiday,:workingday,:temp,:atemp,:hum,:windspeed]]),\n         seasonDummies,\n         weatherDummies,\n         wdayDummies)\ny = data[:,16]","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As usual, we split the data in training, validation and testing sets","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)\n(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"An other common operation with neural networks is to scale the feature vectors (X) and the labels (Y). The BetaML scale() function, by default, scale the data such that each dimension has mean 0 and variance 1. Note that we can provide the function with different scale factors or specify the columns not to scale (e.g. those resulting from the one-hot encoding). Finally we can reverse the scaling (this is useful to retrieve the unscaled features from a model trained with scaled ones).","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"colsNotToScale = [2;4;5;10:23]\nxScaleFactors   = getScaleFactors(xtrain,skip=colsNotToScale)\nyScaleFactors   = ([0],[0.001]) # getScaleFactors(ytrain) # This just divide by 1000. Using full scaling of Y we may get negative demand.\nxtrainScaled    = scale(xtrain,xScaleFactors)\nxvalScaled      = scale(xval,xScaleFactors)\nxtestScaled     = scale(xtest,xScaleFactors)\nytrainScaled    = scale(ytrain,yScaleFactors)\nyvalScaled      = scale(yval,yScaleFactors)\nytestScaled     = scale(ytest,yScaleFactors)\nD               = size(xtrain,2)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As before, we select the best hyperparameters by using the validation set (it may take a while)...","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"function tuneHyperParameters(xtrain,ytrain,xval,yval;epochRange=50:50,hiddenLayerSizeRange=12:12,repetitions=5,rng=Random.GLOBAL_RNG)\n    # We start with an infinititly high error\n    bestRme         = +Inf\n    bestEpoch       = 0\n    bestSize        = 0\n    compLock        = ReentrantLock()\n\n    # Generate one random number generator per thread\n    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generateParallelRngs\n    rngs       = generateParallelRngs(rng,Threads.nthreads())\n\n    # We loop over all possible hyperparameter combinations...\n    parLengths = (length(epochRange),length(hiddenLayerSizeRange))\n    Threads.@threads for ij in CartesianIndices(parLengths)\n       (epoch,hiddenLayerSize)   = (epochRange[Tuple(ij)[1]], hiddenLayerSizeRange[Tuple(ij)[2]])\n       tsrng = rngs[Threads.threadid()]\n       joinedIndx = LinearIndices(parLengths)[ij]\n       # And here we make the seeding depending on the i of the loop, not the thread: hence we get the same results indipendently of the number of threads\n       Random.seed!(tsrng,masterSeed+joinedIndx*10)\n       totAttemptError = 0.0\n       println(\"Testing epochs $epoch, layer size $hiddenLayerSize ...\")\n       # We run several repetitions with the same hyperparameter combination to account for stochasticity...\n       for r in 1:repetitions\n           l1   = DenseLayer(D,hiddenLayerSize,f=relu,rng=tsrng) # Activation function is ReLU\n           l2   = DenseLayer(hiddenLayerSize,hiddenLayerSize,f=identity,rng=tsrng)\n           l3   = DenseLayer(hiddenLayerSize,1,f=relu,rng=tsrng)\n           mynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Bike sharing regression model\") # Build the NN and use the squared cost (aka MSE) as error function\n           # Training it (default to ADAM)\n           res  = train!(mynn,xtrain,ytrain,epochs=epoch,batchSize=8,optAlg=ADAM(),verbosity=NONE, rng=tsrng) # Use optAlg=SGD() to use Stochastic Gradient Descent\n           ŷval = predict(mynn,xval)\n           rmeVal  = meanRelError(ŷval,yval,normRec=false)\n           totAttemptError += rmeVal\n       end\n       avgRme = totAttemptError / repetitions\n       begin\n           lock(compLock) ## This step can't be run in parallel...\n           try\n               # Select this specific combination of hyperparameters if the error is the lowest\n               if avgRme < bestRme\n                 bestRme    = avgRme\n                 bestEpoch  = epoch\n                 bestSize   = hiddenLayerSize\n               end\n           finally\n               unlock(compLock)\n           end\n       end\n    end\n    return (bestRme=bestRme,bestEpoch=bestEpoch,bestSize=bestSize)\nend\n\nepochsToTest     = [100,400]\nhiddenLayerSizes = [5,15,30]\n(bestRme,bestEpoch,bestSize) = tuneHyperParameters(xtrainScaled,ytrainScaled,xvalScaled,yvalScaled;epochRange=epochsToTest,hiddenLayerSizeRange=hiddenLayerSizes,repetitions=3,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now build our feed-forward neaural network. We create three layers, the first layers will always have a input size equal to the dimensions of our data (the number of columns), and the output layer, for a simple regression where the predictions are scalars, it will always be one. There are already several kind of layers available (and you can build your own kind by defining a new struct and implementing a few functions. See the Nn module documentation for details). Here we use only dense layers, those found in typycal feed-fordward neural networks. For each layer, on top of its size (in \"neurons\") we can specify an activation function. Here we use the relu for the two terminal layers (this will guarantee that our predictions are always positive) and identity for the hidden layer. Again, consult the Nn module documentation for other activation layers already defined, or use any function of your choice. Initial weight parameters can also be specified if needed. By default DenseLayer use the so-called Xavier initialisation.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"l1   = DenseLayer(D,bestSize,f=relu,rng=copy(FIXEDRNG)) # Activation function is ReLU\nl2   = DenseLayer(bestSize,bestSize,f=identity,rng=copy(FIXEDRNG))\nl3   = DenseLayer(bestSize,1,f=relu,rng=copy(FIXEDRNG))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Finally we \"chain\" the layer together and we assign a final loss function (agian, you can provide your own, if those available in BetaML don't suit your needs) in order to compose the \"neural network\" object.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Bike sharing regression model\") ## Build the NN and use the squared cost (aka MSE) as error function","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The above neural network will use automatic differentiation (using the Zygote package) to compute the gradient to minimise in the training step. Using manual differentiaiton, for the layers that support it, is however really simple. The network below is exactly equivalent to the one above, except it avoids automatic differentiation:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"mynnManual = buildNetwork([\n        DenseLayer(D,bestSize,f=relu,df=drelu,rng=copy(FIXEDRNG)),\n        DenseLayer(bestSize,bestSize,f=identity,df=didentity,rng=copy(FIXEDRNG)),\n        DenseLayer(bestSize,1,f=relu,df=drelu,rng=copy(FIXEDRNG))\n    ], squaredCost, name=\"Bike sharing regression model\", dcf=dSquaredCost)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We can now re-do the training with the best hyperparameters. Several optimisation algorithms are available, and each accepts different parameters, like the learning rate for the Stochastic Gradient Descent algorithm (used by default) or the exponential decay rates for the  moments estimates for the ADAM algorithm (that we use here, with the default parameters).","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"println(\"Final training of $bestEpoch epochs, with layer size $bestSize ...\")\nres  = train!(mynn,xtrainScaled,ytrainScaled,epochs=bestEpoch,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG)) ## Use optAlg=SGD() to use Stochastic Gradient Descent","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Let's benchmark the BetaML neural network training","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime train!(mynnManual,xtrainScaled,ytrainScaled,epochs=bestEpoch,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG), verbosity=NONE)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"As we can see the model training is one order of magnitude slower than random forests, altought the memory requirement is approximatly the same","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"To obtain the neural network predictions we apply the function predict to the feature matrix X for which we want to generate previsions, and then, in order to obtain the unscaled unscaled estimates we use the scale function applied to the scaled values with the original scaling factors and the parameter rev set to true. Note the usage of the pipe operator to avoid ugly function1(function2(function3(...))) nested calls:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrain = @pipe predict(mynn,xtrainScaled) |> scale(_, yScaleFactors,rev=true)\nŷval   = @pipe predict(mynn,xvalScaled)   |> scale(_, yScaleFactors,rev=true)\nŷtest  = @pipe predict(mynn,xtestScaled)  |> scale(_ ,yScaleFactors,rev=true)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(mreTrain, mreVal, mreTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"The error is much lower. Let's plot our predictions:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Again, we can start by plotting the estimated vs the observed value:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrain,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (NN)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷval,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (NN)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtest,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (NN)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now plot across the time dimension, first plotting the whole period (2 years):","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period  (NN)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"...and then focusing on the testing data","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc  = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (NN)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html#Comparison-with-Flux","page":"A regression task: the prediction of  bike  sharing demand","title":"Comparison with Flux","text":"","category":"section"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We now to apply the same Neural Network model using the Flux framework, a dedicated neural network library. reusing the optimal parameters that we did found in tuneHyperParameters","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We fix the default random number generator so that the Flux example gives a reproducible output","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Random.seed!(123)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We define the Flux neural network model and load it with data...","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"l1         = Flux.Dense(D,bestSize,Flux.relu)\nl2         = Flux.Dense(bestSize,bestSize,identity)\nl3         = Flux.Dense(bestSize,1,Flux.relu)\nFlux_nn    = Flux.Chain(l1,l2,l3)\nloss(x, y) = Flux.mse(Flux_nn(x), y)\nps         = Flux.params(Flux_nn)\nnndata     = Flux.Data.DataLoader((xtrainScaled', ytrainScaled'), batchsize=8,shuffle=true)\n\nFlux_nn2   = deepcopy(Flux_nn)      ## A copy for the time benchmarking\nps2        = Flux.params(Flux_nn2)  ## A copy for the time benchmarking","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We do the training of the Flux model...","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Flux.@epochs bestEpoch Flux.train!(loss, ps, nndata, Flux.ADAM(0.001, (0.9, 0.8)))","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and we benchmark it..","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"@btime begin for i in 1:bestEpoch Flux.train!(loss, ps2, nndata, Flux.ADAM(0.001, (0.9, 0.8))) end end","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"On this small example the speed of Flux is on the same order than BetaML (the actual difference seems to depend on the specific RNG seed), however I suspect that Flux scales much better with larger networks and/or data.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"We obtain the estimates...","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainf = @pipe Flux_nn(xtrainScaled')' |> scale(_,yScaleFactors,rev=true)\nŷvalf   = @pipe Flux_nn(xvalScaled')'   |> scale(_,yScaleFactors,rev=true)\nŷtestf  = @pipe Flux_nn(xtestScaled')'  |> scale(_,yScaleFactors,rev=true)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"..and we compute the mean relative errors..","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"(mreTrain, mreVal, mreTest) = meanRelError.([ŷtrainf,ŷvalf,ŷtestf],[ytrain,yval,ytest],normRec=false)","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":".. finding an error not significantly different than the one obtained from BetaML.Nn.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Plots:","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytrain,ŷtrainf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in training period (Flux.NN)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(yval,ŷvalf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in validation period (Flux.NN)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"scatter(ytest,ŷtestf,xlabel=\"daily rides\",ylabel=\"est. daily rides\",label=nothing,title=\"Est vs. obs in testing period (Flux.NN)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"ŷtrainfullf = vcat(ŷtrainf,fill(missing,nval+ntest))\nŷvalfullf   = vcat(fill(missing,ntrain), ŷvalf, fill(missing,ntest))\nŷtestfullf  = vcat(fill(missing,ntrain+nval), ŷtestf)\nplot(data[:,:dteday],[data[:,:cnt] ŷtrainfullf ŷvalfullf ŷtestfullf], label=[\"obs\" \"train\" \"val\" \"test\"], legend=:topleft, ylabel=\"daily rides\", title=\"Daily bike sharing demand observed/estimated across the\\n whole 2-years period (Flux.NN)\")","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"stc = 620\nendc = size(x,1)\nplot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfullf[stc:endc] ŷtestfullf[stc:endc]], label=[\"obs\" \"val\" \"test\"], legend=:bottomleft, ylabel=\"Daily rides\", title=\"Focus on the testing period (Flux.NN)\")\n\n\n### Conclusions of Neural Network models","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"If we strive for the most accurate predictions, deep neural networks uysually offer the best solution. However they are computationally expensive, so with limited resourses we may get better results by fine tuning and running many repetitions of \"simpler\" decision trees or even random forest models than a large naural network with insufficient hyperparameter tuning. Also, we shoudl consider that decision trees/random forests are much simple to work with.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"That said, specialised neural network libraries, like Flux, allow to use GPU and specialised hardware letting neural networks to scale with very big datasets.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"Still, for small and medium datasets, BetaML provides simpler yet customisable solutions that are accurate and fast.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"View this file on Github.","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"","category":"page"},{"location":"tutorials/A regression task - sharing bike demand prediction/betaml_tutorial_regression_sharingBikes.html","page":"A regression task: the prediction of  bike  sharing demand","title":"A regression task: the prediction of  bike  sharing demand","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Examples.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"Examples.html#Supervised-learning","page":"Examples","title":"Supervised learning","text":"","category":"section"},{"location":"Examples.html#Regression","page":"Examples","title":"Regression","text":"","category":"section"},{"location":"Examples.html#Estimating-the-bike-sharing-demand","page":"Examples","title":"Estimating the bike sharing demand","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Data origin:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"original full dataset (by hour, not used here): https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\nsimplified dataset (by day, with some simple scaling): https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec\ndescription: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/exam2019ace.pdf\ndata: https://www.hds.utc.fr/~tdenoeux/dokuwiki/media/en/bikesharing_day.csv.zip","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. Y_t = f(X_t) alone).","category":"page"},{"location":"Examples.html#Classification","page":"Examples","title":"Classification","text":"","category":"section"},{"location":"Examples.html#Unsupervised-lerarning","page":"Examples","title":"Unsupervised lerarning","text":"","category":"section"},{"location":"Examples.html#Notebooks","page":"Examples","title":"Notebooks","text":"","category":"section"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"The following notebooks provide runnable examples of the package functionality:","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Pegasus classifiers: [Static notebook] - [myBinder]\nDecision Trees and Random Forest regression on Bike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nNeural Networks: [Static notebook] - [myBinder]\nBike sharing demand forecast (daily data): [Static notebook] - [myBinder]\nClustering: [Static notebook] - [myBinder]","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"Note: the live, runnable computational environment is a temporary new copy made at each connection. The first time after a commit is done on this repository a new environment has to be set (instead of just being copied), and the server may take several minutes.","category":"page"},{"location":"Examples.html","page":"Examples","title":"Examples","text":"This is only if you are the unlucky user triggering the rebuild of the environment after the commit.","category":"page"},{"location":"Nn.html#The-BetaML.Nn-Module","page":"Nn","title":"The BetaML.Nn Module","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Nn","category":"page"},{"location":"Nn.html#BetaML.Nn","page":"Nn","title":"BetaML.Nn","text":"BetaML.Nn module\n\nImplement the functionality required to define an artificial Neural Network, train it with data, forecast data and assess its performances.\n\nCommon type of layers and optimisation algorithms are already provided, but you can define your own ones subclassing respectively the Layer and OptimisationAlgorithm abstract types.\n\nThe module provide the following type or functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition:\n\nDenseLayer: Classical feed-forward layer with user-defined activation function\nDenseNoBiasLayer: Classical layer without the bias parameter\nVectorFunctionLayer: Parameterless layer whose activation function run over the ensable of its nodes rather than on each one individually\nbuildNetwork: Build the chained network and define a cost function\ngetParams(nn): Retrieve current weigthts\ngetGradient(nn): Retrieve the current gradient of the weights\nsetParams!(nn): Update the weigths of the network\nshow(nn): Print a representation of the Neural Network\n\nEach layer can use a default activation function, one of the functions provided in the Utils module (relu, tanh, softmax,...) or you can specify your own function. The derivative of the activation function can be optionally be provided, in such case training will be quicker, altought this difference tends to vanish with bigger datasets. You can alternativly implement your own layers defining a new type as subtype of the abstract type Layer. Each user-implemented layer must define the following methods:\n\nA suitable constructor\nforward(layer,x)\nbackward(layer,x,nextGradient)\ngetParams(layer)\ngetGradient(layer,x,nextGradient)\nsetParams!(layer,w)\nsize(layer)\n\nModel training:\n\ntrainingInfo(nn): Default callback function during training\ntrain!(nn):  Training function\nsingleUpdate!(θ,▽;optAlg): The parameter update made by the specific optimisation algorithm\nSGD: The default optimisation algorithm\nADAM: A faster moment-based optimisation algorithm (added in v0.2.2)\n\nTo define your own optimisation algorithm define a subtype of OptimisationAlgorithm and implement the function singleUpdate!(θ,▽;optAlg) and eventually initOptAlg(⋅) specific for it.\n\nModel predictions and assessment:\n\npredict(nn): Return the output given the data\nloss(nn): Compute avg. network loss on a test set\nUtils.accuracy(ŷ,y): Categorical output accuracy\n\nWhile high-level functions operating on the dataset expect it to be in the standard format (nRecords × nDimensions matrices) it is custom to represent the chain of a neural network as a flow of column vectors, so all low-level operations (operating on a single datapoint) expect both the input and the output as a column vector.\n\n\n\n\n\n","category":"module"},{"location":"Nn.html#Module-Index","page":"Nn","title":"Module Index","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Nn.html#Detailed-API","page":"Nn","title":"Detailed API","text":"","category":"section"},{"location":"Nn.html","page":"Nn","title":"Nn","text":"Modules = [Nn]","category":"page"},{"location":"Nn.html#BetaML.Nn.ADAM","page":"Nn","title":"BetaML.Nn.ADAM","text":"ADAM(;η, λ, β₁, β₂, ϵ)\n\nThe ADAM algorithm, an adaptive moment estimation optimiser.\n\nFields:\n\nη:  Learning rate (stepsize, α in the paper), as a function of the current epoch [def: t -> 0.001 (i.e. fixed)]\nλ:  Multiplicative constant to the learning rate [def: 1]\nβ₁: Exponential decay rate for the first moment estimate [range: ∈ [0,1], def: 0.9]\nβ₂: Exponential decay rate for the second moment estimate [range: ∈ [0,1], def: 0.999]\nϵ:  Epsilon value to avoid division by zero [def: 10^-8]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseLayer","page":"Nn","title":"BetaML.Nn.DenseLayer","text":"DenseLayer\n\nRepresentation of a layer in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nwb: Biases (n)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseNoBiasLayer","page":"Nn","title":"BetaML.Nn.DenseNoBiasLayer","text":"DenseNoBiasLayer\n\nRepresentation of a layer without bias in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.Learnable","page":"Nn","title":"BetaML.Nn.Learnable","text":"Learnable(data)\n\nStructure representing the learnable parameters of a layer or its gradient.\n\nThe learnable parameters of a layers are given in the form of a N-tuple of Array{Float64,N2} where N2 can change (e.g. we can have a layer with the first parameter being a matrix, and the second one being a scalar). We wrap the tuple on its own structure a bit for some efficiency gain, but above all to define standard mathematic operations on the gradients without doing \"type pyracy\" with respect to Base tuples.\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NN","page":"Nn","title":"BetaML.Nn.NN","text":"NN\n\nRepresentation of a Neural Network\n\nFields:\n\nlayers:  Array of layers objects\ncf:      Cost function\ndcf:     Derivative of the cost function\ntrained: Control flag for trained networks\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.OptimisationAlgorithm","page":"Nn","title":"BetaML.Nn.OptimisationAlgorithm","text":"OptimisationAlgorithm\n\nAbstract type representing an Optimisation algorithm.\n\nCurrently supported algorithms:\n\nSGD (Stochastic) Gradient Descent\n\nSee ?[Name OF THE ALGORITHM] for their details\n\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate(⋅) (type ?singleUpdate for details).\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.SGD","page":"Nn","title":"BetaML.Nn.SGD","text":"SGD(;η=t -> 1/(1+t), λ=2)\n\nStochastic Gradient Descent algorithm (default)\n\nFields:\n\nη: Learning rate, as a function of the current epoch [def: t -> 1/(1+t)]\nλ: Multiplicative constant to the learning rate [def: 2]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.VectorFunctionLayer","page":"Nn","title":"BetaML.Nn.VectorFunctionLayer","text":"VectorFunctionLayer\n\nRepresentation of a (weightless) VectorFunction layer in the network. Vector function layer expects a vector activation function, i.e. a function taking the whole output of the previous layer in input rather than working on a single node as \"normal\" activation functions. Useful for example for the SoftMax function.\n\nFields:\n\nnₗ: Number of nodes of the previous layer\nn:  Number of nodes in output\nf:  Activation function (vector)\ndf: Derivative of the (vector) activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#Base.size-Tuple{Layer}","page":"Nn","title":"Base.size","text":"size(layer)\n\nSGet the dimensions of the layers in terms of (dimensions in input , dimensions in output)\n\nNotes:\n\nYou need to use import Base.size before defining this function for your layer\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.backward-Tuple{Layer, Any, Any}","page":"Nn","title":"BetaML.Nn.backward","text":"backward(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer inputs\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.buildNetwork-Tuple{Any, Any}","page":"Nn","title":"BetaML.Nn.buildNetwork","text":"buildNetwork(layers,cf;dcf,name)\n\nInstantiate a new Feedforward Neural Network\n\nParameters:\n\nlayers: Array of layers objects\ncf:     Cost function\ndcf:    Derivative of the cost function [def: nothing]\nname:   Name of the network [def: \"Neural Network\"]\n\nNotes:\n\nEven if the network ends with a single output note, the cost function and its derivative should always expect y and ŷ as column vectors.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.forward-Tuple{Layer, Any}","page":"Nn","title":"BetaML.Nn.forward","text":"forward(layer,x)\n\nPredict the output of the layer given the input\n\nParameters:\n\nlayer:  Worker layer\nx:      Input to the layer\n\nReturn:\n\nAn Array{T,1} of the prediction (even for a scalar)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Tuple{Layer, Any, Any}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getParams() and setParams() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{Any, AbstractMatrix{T}, AbstractMatrix{T2}}} where {T<:Number, T2<:Number}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,xbatch,ybatch)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn:      Worker network\nxbatch:  Input to the network (n,d)\nybatch:  Label input (n,d)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{NN, Union{AbstractVector{T}, T}, Union{AbstractVector{T2}, T2}}} where {T<:Number, T2<:Number}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,x,y)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn: Worker network\nx:   Input to the network (d,1)\ny:   Label input (d,1)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{Layer}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(layer)\n\nReturn the number of parameters of a layer.\n\nIt doesn't need to be implemented by each layer type, as it uses getParams().\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(nn) - Return the number of trainable parameters of the neural network.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{Layer}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(layer)\n\nGet the layers current value of its trainable parameters\n\nParameters:\n\nlayer:  Worker layer\n\nReturn:\n\nThe current value of the layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getGradient() and setParams() functions. Note that starting from BetaML 0.2.2 this tuple needs to be wrapped in its Learnable type.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(nn)\n\nRetrieve current weigthts\n\nParameters:\n\nnn: Worker network\n\nNotes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.initOptAlg!-Tuple{ADAM}","page":"Nn","title":"BetaML.Nn.initOptAlg!","text":"initOptAlg!(optAlg::ADAM;θ,batchSize,x,y,rng)\n\nInitialize the ADAM algorithm with the parameters m and v as zeros and check parameter bounds\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.initOptAlg!-Tuple{BetaML.Nn.OptimisationAlgorithm}","page":"Nn","title":"BetaML.Nn.initOptAlg!","text":"initOptAlg!(optAlg;θ,batchSize,x,y)\n\nInitialize the optimisation algorithm\n\nParameters:\n\noptAlg:    The Optimisation algorithm to use\nθ:         Current parameters\nbatchSize:    The size of the batch\nx:   The training (input) data\ny:   The training \"labels\" to match\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nOnly a few optimizers need this function and consequently ovverride it. By default it does nothing, so if you want write your own optimizer and don't need to initialise it, you don't have to override this method\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.loss-Tuple{NN, Any, Any}","page":"Nn","title":"BetaML.Nn.loss","text":"loss(fnn,x,y)\n\nCompute avg. network loss on a test set (or a single (1 × d) data point)\n\nParameters:\n\nfnn: Worker network\nx:   Input to the network (n) or (n x d)\ny:   Label input (n) or (n x d)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{Layer, Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":" setParams!(layer,w)\n\nSet the trainable parameters of the layer with the given values\n\nParameters:\n\nlayer: Worker layer\nw:   The new parameters to set (Learnable)\n\nNotes:\n\nThe format of the tuple wrapped by Learnable must be consistent with those of the getParams() and getGradient() functions.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{NN, Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":"setParams!(nn,w)\n\nUpdate weigths of the network\n\nParameters:\n\nnn: Worker network\nw:  The new weights to set\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.show-Tuple{NN}","page":"Nn","title":"BetaML.Nn.show","text":"show(nn)\n\nPrint a representation of the Neural Network (layers, dimensions..)\n\nParameters:\n\nnn: Worker network\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.singleUpdate!-Tuple{Any, Any}","page":"Nn","title":"BetaML.Nn.singleUpdate!","text":"singleUpdate!(θ,▽;nEpoch,nBatch,batchSize,xbatch,ybatch,optAlg)\n\nPerform the parameters update based on the average batch gradient.\n\nParameters:\n\nθ:         Current parameters\n▽:         Average gradient of the batch\nnEpoch:    Count of current epoch\nnBatch:    Count of current batch\nnBatches:  Number of batches per epoch\nxbatch:    Data associated to the current batch\nybatch:    Labels associated to the current batch\noptAlg:    The Optimisation algorithm to use for the update\n\nNotes:\n\nThis function is overridden so that each optimisation algorithm implement their\n\nown version\n\nMost parameters are not used by any optimisation algorithm. They are provided\n\nto support the largest possible class of optimisation algorithms\n\nSome optimisation algorithms may change their internal structure in this function\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.train!-Tuple{NN, Any, Any}","page":"Nn","title":"BetaML.Nn.train!","text":"train!(nn,x,y;epochs,batchSize,sequential,optAlg,verbosity,cb)\n\nTrain a neural network with the given x,y data\n\nParameters:\n\nnn:         Worker network\nx:          Training input to the network (records x dimensions)\ny:          Label input (records x dimensions)\nepochs:     Number of passages over the training set [def: 100]\nbatchSize:  Size of each individual batch [def: min(size(x,1),32)]\nsequential: Wether to run all data sequentially instead of random [def: false]\noptAlg:     The optimisation algorithm to update the gradient at each batch [def: ADAM()]\nverbosity:  A verbosity parameter for the trade off information / efficiency [def: STD]\ncb:         A callback to provide information. [def: trainingInfo]\nrng:        Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn:\n\nA named tuple with the following information\nepochs: Number of epochs actually ran\nϵ_epochs: The average error on each epoch (if verbosity > LOW)\nθ_epochs: The parameters at each epoch (if verbosity > STD)\n\nNotes:\n\nCurrently supported algorithms:\nSGD, the classical (Stochastic) Gradient Descent optimiser\nADAM,  an adaptive moment estimation optimiser\nLook at the individual optimisation algorithm (?[Name OF THE ALGORITHM]) for info on its parameter, e.g. ?SGD for the Stochastic Gradient Descent.\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate!(⋅) (type ?singleUpdate! for details).\nYou can implement your own callback function, altought the one provided by default is already pretty generic (its output depends on the verbosity parameter). See trainingInfo for informations on the cb parameters.\nBoth the callback function and the singleUpdate! function of the optimisation algorithm can be used to stop the training algorithm, respectively returning true or stop=true.\nThe verbosity can be set to any of NONE,LOW,STD,HIGH,FULL.\nThe update is done computing the average gradient for each batch and then calling singleUpdate! to let the optimisation algorithm perform the parameters update\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.trainingInfo-Tuple{Any, Any, Any}","page":"Nn","title":"BetaML.Nn.trainingInfo","text":"trainingInfo(nn,x,y;n,batchSize,epochs,verbosity,nEpoch,nBatch)\n\nDefault callback funtion to display information during training, depending on the verbosity level\n\nParameters:\n\nnn: Worker network\nx:  Batch input to the network (batchSize,d)\ny:  Batch label input (batchSize,d)\nn: Size of the full training set\nnBatches : Number of baches per epoch\nepochs: Number of epochs defined for the training\nverbosity: Verbosity level defined for the training (NONE,LOW,STD,HIGH,FULL)\nnEpoch: Counter of the current epoch\nnBatch: Counter of the current batch\n\n#Notes:\n\nReporting of the error (loss of the network) is expensive. Use verbosity=NONE for better performances\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#The-BetaML.Clustering-Module","page":"Clustering","title":"The BetaML.Clustering Module","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Clustering\n","category":"page"},{"location":"Clustering.html#BetaML.Clustering","page":"Clustering","title":"BetaML.Clustering","text":"Clustering module (WIP)\n\nProvide clustering methods and missing values imputation / collaborative filtering / reccomendation systems using clustering methods as backend.\n\nThe module provides the following functions. Use ?[function] to access their full signature and detailed documentation:\n\ninitRepresentatives(X,K;initStrategy,Z₀): Initialisation strategies for Kmean and Kmedoids\n`kmeans(X,K;dist,initStrategy,Z₀)`: Classical KMean algorithm\n`kmedoids(X,K;dist,initStrategy,Z₀)`: Kmedoids algorithm\n`gmm(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy)`: gmm algorithm over GMM\n`predictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance)`: Fill mixing values / collaborative filtering using gmm as backbone\n\n{Spherical|Diagonal|Full}Gaussian mixtures for gmm / predictMissing are already provided. User defined mixtures can be used defining a struct as subtype of Mixture and implementing for that mixture the following functions:\n\ninitMixtures!(mixtures, X; minVariance, minCovariance, initStrategy)\nlpdf(m,x,mask) (for the e-step)\nupdateParameters!(mixtures, X, pₙₖ; minVariance, minCovariance) (the m-step)\n\n\n\n\n\n","category":"module"},{"location":"Clustering.html#Module-Index","page":"Clustering","title":"Module Index","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Clustering.html#Detailed-API","page":"Clustering","title":"Detailed API","text":"","category":"section"},{"location":"Clustering.html","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]","category":"page"},{"location":"Clustering.html#BetaML.Clustering.gmm-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.gmm","text":"gmm(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy)\n\nCompute Expectation-Maximisation algorithm to identify K clusters of X data, i.e. employ a Generative Mixture Model as the underlying probabilistic model.\n\nX can contain missing values in some or all of its dimensions. In such case the learning is done only with the available data. Implemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (n x d) data to clusterise\nK  :           Number of cluster wanted\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: nothing]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: kmeans]\nmaxIter:       Maximum number of iterations [def: -1, i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\npₙₖ:      Matrix of size (N x K) of the probabilities of each point i to belong to cluster j\npₖ:       Probabilities of the categorical distribution (K x 1)\nmixtures: Vector (K x 1) of the estimated underlying distributions\nϵ:        Vector of the discrepancy (matrix norm) between pⱼₓ and the lagged pⱼₓ at each iteration\nlL:       The log-likelihood (without considering the last mixture optimisation)\nBIC:      The Bayesian Information Criterion (lower is better)\nAIC:      The Akaike Information Criterion (lower is better)\n\nNotes:\n\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nReasonable choices for the minVariance/Covariance depends on the mixture. For example 0.25 seems a reasonable value for the SphericalGaussian, 0.05 seems better for the DiagonalGaussian, and FullGaussian seems to prefer either very low values of variance/covariance (e.g. (0.05,0.05) ) or very big but similar ones (e.g. (100,100) ).\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster (expecially if X contains missing values), but kmeans often provides better results.\n\nResources:\n\nPaper describing gmm with missing values\nClass notes from MITx 6.86x (Sec 15.9)\nLimitations of gmm\n\nExample:\n\njulia> clusters = gmm([1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4],3,verbosity=HIGH)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.initMixtures!-Union{Tuple{T}, Tuple{Vector{T}, Any}} where T<:BetaML.Clustering.AbstractGaussian","page":"Clustering","title":"BetaML.Clustering.initMixtures!","text":"initMixtures!(mixtures::Array{T,1}, X; minVariance=0.25, minCovariance=0.0, initStrategy=\"grid\",rng=Random.GLOBAL_RNG)\n\nThe parameter initStrategy can be grid, kmeans or given:\n\ngrid: Uniformly cover the space observed by the data\nkmeans: Use the kmeans algorithm. If the data contains missing values, a first run of predictMissing is done under init=grid to impute the missing values just to allow the kmeans algorithm. Then the em algorithm is used with the output of kmean as init values.\ngiven: Leave the provided set of initial mixtures\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.initRepresentatives-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.initRepresentatives","text":"initRepresentatives(X,K;initStrategy,Z₀)\n\nInitialisate the representatives for a K-Mean or K-Medoids algorithm\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA (K x D) matrix of initial representatives\n\nExample:\n\njulia> Z₀ = initRepresentatives([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.6 38],2,initStrategy=\"given\",Z₀=[1.7 15; 3.6 40])\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmeans-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.kmeans","text":"kmeans(X,K;dist,initStrategy,Z₀)\n\nCompute K-Mean algorithm to identify K clusters of X using Euclidean distance\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmeans([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmedoids-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.kmedoids","text":"kmedoids(X,K;dist,initStrategy,Z₀)\n\nCompute K-Medoids algorithm to identify K clusters of X using distance definition dist\n\nParameters:\n\nX: a (n x d) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Whether to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach\nshuffle: selecting randomly within the available points [default]\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmedoids([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3,initStrategy=\"grid\")\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{DiagonalGaussian, Any, Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::DiagonalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{FullGaussian, Any, Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::FullGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{SphericalGaussian, Any, Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::SphericalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.predictMissing-Tuple{Any, Any}","page":"Clustering","title":"BetaML.Clustering.predictMissing","text":"predictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance)\n\nFill missing entries in a sparse matrix assuming an underlying Gaussian Mixture probabilistic Model (GMM) and implementing an Expectation-Maximisation algorithm.\n\nWhile the name of the function is predictMissing, the function can be used also for system reccomendation / collaborative filtering and GMM-based regressions.\n\nImplemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (N x D) sparse matrix of data to fill according to a GMM model\nK  :           Number of mixtures desired\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: nothing]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: grid]\nmaxIter:       Maximum number of iterations [def: -1, i.e. ∞]\nrng:           Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturns:\n\nA named touple of:\n̂X̂    : The Filled Matrix of size (N x D)\nnFill: The number of items filled\nlL   : The log-likelihood (without considering the last mixture optimisation)\nBIC :  The Bayesian Information Criterion (lower is better)\nAIC :  The Akaike Information Criterion (lower is better)\nNotes:\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support grid, kmeans or given. grid is faster, but kmeans often provides better results.\n\nExample:\n\njulia>  cFOut = predictMissing([1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.predict-Tuple{GMMFitter, Any, Any}","page":"Clustering","title":"MLJModelInterface.predict","text":"predict(m::GMM, fitResults, X) - Given a trained clustering model, predict the class of the observations used for training\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.predict-Tuple{Union{KMeans, KMedoids}, Any, Any}","page":"Clustering","title":"MLJModelInterface.predict","text":"predict(m::KMeans, fitResults, X) - Given a trained clustering model and some observations, predict the class of the observation\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.transform-Tuple{MissingImputator, Any, Any}","page":"Clustering","title":"MLJModelInterface.transform","text":"transform(m::MissingImputator, fitResults, X) - Given a trained imputator model fill the missing data of some new observations\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#MLJModelInterface.transform-Tuple{Union{KMeans, KMedoids}, Any, Any}","page":"Clustering","title":"MLJModelInterface.transform","text":"fit(m::KMeans, fitResults, X) - Given a trained clustering model and some observations, return the distances to each centroids \n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#The-BetaML.Perceptron-Module","page":"Perceptron","title":"The BetaML.Perceptron Module","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Perceptron","category":"page"},{"location":"Perceptron.html#BetaML.Perceptron","page":"Perceptron","title":"BetaML.Perceptron","text":"Perceptron module\n\nProvide linear and kernel classifiers.\n\nSee a runnable example on myBinder\n\nperceptron: Train data using the classical perceptron\nkernelPerceptron: Train data using the kernel perceptron\npegasos: Train data using the pegasos algorithm\npredict: Predict data using parameters from one of the above algorithms\n\nAll algorithms are multiclass, with perceptron and pegasos employing a one-vs-all strategy, while kernelPerceptron employs a one-vs-one approach, and return a \"probability\" for each class in term of a dictionary for each record. Use mode(ŷ) to return a single class prediction per record.\n\nThe binary equivalent algorithms, accepting only {-1,+1} labels, are available as peceptronBinary, kernelPerceptronBinary and pegasosBinary. They are slighly faster as they don't need to be wrapped in the multi-class equivalent and return a more informative output.\n\nThe multi-class versions are available in the MLJ framework as PerceptronClassifier,KernelPerceptronClassifier and PegasosClassifier respectivly.\n\n\n\n\n\n","category":"module"},{"location":"Perceptron.html#Module-Index","page":"Perceptron","title":"Module Index","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Perceptron.html#Detailed-API","page":"Perceptron","title":"Detailed API","text":"","category":"section"},{"location":"Perceptron.html","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]","category":"page"},{"location":"Perceptron.html#BetaML.Api.predict","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,θ,θ₀)\n\nPredict a binary label {-1,1} given the feature vector and the linear coefficients\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\nθ:        The trained parameters\nθ₀:       The trained bias barameter [def: 0]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"function"},{"location":"Perceptron.html#BetaML.Api.predict-NTuple{4, Any}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,xtrain,ytrain,α;K)\n\nPredict a binary label {-1,1} given the feature vector and the training data together with their errors (as trained by a kernel perceptron algorithm)\n\nParameters:\n\nx:      Feature matrix of the training data (n × d)\nxtrain: The feature vectors used for the training\nytrain: The labels of the training set\nα:      The errors associated to each record\nK:      The kernel function used for the training and to be used for the prediction [def: radialKernel]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Union{Tuple{Tcl}, Tuple{Any, Any, Any, Any, AbstractVector{Tcl}}} where Tcl","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,xtrain,ytrain,α,classes;K)\n\nPredict a multiclass label given the new feature vector and a trained kernel perceptron model.\n\nParameters:\n\nx:      Feature matrix of the training data (n × d)\nxtrain: A vector of the feature matrix used for training each of the one-vs-one class matches (i.e. model.x)\nytrain: A vector of the label vector used for training each of the one-vs-one class matches (i.e. model.y)\nα:      A vector of the errors associated to each record (i.e. model.α)\nclasses: The overal classes encountered in training (i.e. model.classes)\nK:      The kernel function used for the training and to be used for the prediction [def: radialKernel]\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability (warning: it isn't really a probability, it is just the standardized number of matches \"won\" by this class compared with the other classes)\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\njulia> model  = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷtrain = Perceptron.predict([10 10; 2.2 2.5],model.x,model.y,model.α, model.classes,K=model.K)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Api.predict-Union{Tuple{Tcl}, Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{Float64}, Vector{Tcl}}} where {T<:AbstractVector{Float64}, Tcl}","page":"Perceptron","title":"BetaML.Api.predict","text":"predict(x,θ,θ₀,classes)\n\nPredict a multiclass label given the feature vector, the linear coefficients and the classes vector\n\nParameters:\n\nx:       Feature matrix of the training data (n × d)\nθ:       Vector of the trained parameters for each one-vs-all model (i.e. model.θ)\nθ₀:      Vector of the trained bias barameter for each one-vs-all model (i.e. model.θ₀)\nclasses: The overal classes encountered in training (i.e. model.classes)\n\nReturn :\n\nŷ: Vector of dictionaries label=>probability\n\nNotes:\n\nUse mode(ŷ) if you want a single predicted label per record\n\nExample:\n\n```julia julia> model  = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1]) julia> ŷtrain = predict([10 10; 2.5 2.5],model.θ,model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptron-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptron","text":"kernelPerceptron(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a multiclass kernel classifier \"perceptron\" algorithm based on x and y.\n\nkernelPerceptron is a (potentially) non-linear perceptron-style classifier employing user-defined kernel funcions. Multiclass is supported using a one-vs-one approach.\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data, in the format of ⨦ 1\nK:        Kernel function to employ. See ?radialKernel or ?polynomialKernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radialKernel]\nT:        Maximum number of iterations (aka \"epochs\") across the whole set (if the set is not fully classified earlier) [def: 100]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: The x data (eventually shuffled if shuffle=true)\ny: The label\nα: The errors associated to each record\nclasses: The labels classes encountered in the training\n\nNotes:\n\nThe trained model can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the KernelPerceptronClassifier\n\nExample:\n\njulia> model  = kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷtrain = Perceptron.predict(xtrain,model.x,model.y,model.α, model.classes,K=model.K)\njulia> ϵtrain = error(ytrain, mode(ŷtrain))\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptronBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptronBinary","text":"kernelPerceptronBinary(x,y;K,T,α,nMsgs,shuffle)\n\nTrain a multiclass kernel classifier \"perceptron\" algorithm based on x and y\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data, in the format of ⨦ 1\nK:        Kernel function to employ. See ?radialKernel or ?polynomialKernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radialKernel]\nT:        Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done\nshuffle:  Whether to randomly shuffle the data at each iteration [def: false]\nrng:      Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nx: the x data (eventually shuffled if shuffle=true)\ny: the label\nα: the errors associated to each record\nerrors: the number of errors in the last iteration\nbesterrors: the minimum number of errors in classifying the data ever reached\niterations: the actual number of iterations performed\nseparated: a flag if the data has been successfully separated\n\nNotes:\n\nThe trained data can then be used to make predictions using the function predict(). If the option shuffle has been used, it is important to use there the returned (x,y,α) as these would have been shuffle compared with the original (x,y).\nPlease see @kernelPerceptron for a multi-class version\n\nExample:\n\njulia> model = kernelPerceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasos-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasos","text":"pegasos(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,forceOrigin,returnMeanHyperplane)\n\nTrain the multiclass classifier \"pegasos\" algorithm according to x (features) and y (labels)\n\nPegasos is a linear, gradient-based classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whehter to force θ₀ to remain zero [def: false]\nreturnMeanHyperplane: Whether to return the average hyperplane coefficients instead of the average ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the PegasosClassifier\n\nExample:\n\njulia> model = pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasosBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasosBinary","text":"pegasosBinary(x,y;θ,θ₀,λ,η,T,nMsgs,shuffle,forceOrigin)\n\nTrain the peagasos algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:    Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> pegasos([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptron-Tuple{AbstractMatrix{T} where T, AbstractVector{T} where T}","page":"Perceptron","title":"BetaML.Perceptron.perceptron","text":"perceptron(x,y;θ,θ₀,T,nMsgs,shuffle,forceOrigin,returnMeanHyperplane)\n\nTrain the multiclass classifier \"perceptron\" algorithm  based on x and y (labels).\n\nThe perceptron is a linear classifier. Multiclass is supported using a one-vs-all approach.\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, can be in any format (string, integers..)\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant                term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set                is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done [def: 0]\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\nreturnMeanHyperplane: Whether to return the average hyperplane coefficients instead of the final ones  [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The weights of the classifier\nθ₀:         The weight of the classifier associated to the constant term\nclasses:    The classes (unique values) of y\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\nThis model is available in the MLJ framework as the PerceptronClassifier\n\nExample:\n\njulia> model = perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\njulia> ŷ     = predict([2.1 3.1; 7.3 5.2], model.θ, model.θ₀, model.classes)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptronBinary-Tuple{Any, Any}","page":"Perceptron","title":"BetaML.Perceptron.perceptronBinary","text":"perceptronBinary(x,y;θ,θ₀,T,nMsgs,shuffle,forceOrigin)\n\nTrain the binary classifier \"perceptron\" algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant                term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set                is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nshuffle:     Whether to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Whether to force θ₀ to remain zero [def: false]\nrng:         Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> model = perceptronBinary([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#The-BetaML.Trees-Module","page":"Trees","title":"The BetaML.Trees Module","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Trees","category":"page"},{"location":"Trees.html#BetaML.Trees","page":"Trees","title":"BetaML.Trees","text":"BetaML.Trees module\n\nImplement the functionality required to build a Decision Tree or a whole Random Forest, predict data and assess its performances.\n\nBoth Decision Trees and Random Forests can be used for regression or classification problems, based on the type of the labels (numerical or not). You can override the automatic selection with the parameter forceClassification=true, typically if your labels are integer representing some categories rather than numbers. For classification problems the output of predictSingle is a dictionary with the key being the labels with non-zero probabilitity and the corresponding value its proobability; for regression it is a numerical value.\n\nPlease be aware that, differently from most other implementations, the Random Forest algorithm collects and averages the probabilities from the trees, rather than just repording the mode, i.e. no information is lost and the output of the forest classifier is still a PMF.\n\nMissing data on features are supported, both on training and on prediction.\n\nThe module provide the following functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition and training:\n\nbuildTree(xtrain,ytrain): Build a single Decision Tree\nbuildForest(xtrain,ytrain): Build a \"forest\" of Decision Trees\n\nModel predictions and assessment:\n\npredict(tree or forest, x): Return the prediction given the feature matrix\noobError(forest,x,y): Return the out-of-bag error estimate\nUtils.accuracy(ŷ,y)): Categorical output accuracy\nUtils.meanRelError(ŷ,y,p): L-p norm based error\n\nFeatures are expected to be in the standard format (nRecords × nDimensions matrices) and the labels (either categorical or numerical) as a nRecords column vector.\n\nAcknowlegdments: originally based on the Josh Gordon's code\n\n\n\n\n\n","category":"module"},{"location":"Trees.html#Module-Index","page":"Trees","title":"Module Index","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Trees.html#Detailed-API","page":"Trees","title":"Detailed API","text":"","category":"section"},{"location":"Trees.html","page":"Trees","title":"Trees","text":"Modules = [Trees]","category":"page"},{"location":"Trees.html#BetaML.Trees.AbstractQuestion","page":"Trees","title":"BetaML.Trees.AbstractQuestion","text":"Question\n\nA question used to partition a dataset.\n\nThis struct just records a 'column number' and a 'column value' (e.g., Green).\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.DecisionNode","page":"Trees","title":"BetaML.Trees.DecisionNode","text":"DecisionNode(question,trueBranch,falseBranch, depth)\n\nA tree's non-terminal node.\n\nConstructor's arguments and struct members:\n\nquestion: The question asked in this node\ntrueBranch: A reference to the \"true\" branch of the trees\nfalseBranch: A reference to the \"false\" branch of the trees\ndepth: The nodes's depth in the tree\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.Forest","page":"Trees","title":"BetaML.Trees.Forest","text":"Forest{Ty}\n\nType representing a Random Forest.\n\nIndividual trees are stored in the array trees. The \"type\" of the forest is given by the type of the labels on which it has been trained.\n\nStruct members:\n\ntrees:        The individual Decision Trees\nisRegression: Whether the forest is to be used for regression jobs or classification\noobData:      For each tree, the rows number if the data that have not being used to train the specific tree\noobError:     The out of bag error (if it has been computed)\nweights:      A weight for each tree depending on the tree's score on the oobData (see buildForest)\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#BetaML.Trees.Leaf","page":"Trees","title":"BetaML.Trees.Leaf","text":"Leaf(y,depth)\n\nA tree's leaf (terminal) node.\n\nConstructor's arguments:\n\ny: The labels assorciated to each record (either numerical or categorical)\ndepth: The nodes's depth in the tree\n\nStruct members:\n\npredictions: Either the relative label's count (i.e. a PMF) or the mean\ndepth: The nodes's depth in the tree\n\n\n\n\n\n","category":"type"},{"location":"Trees.html#Base.print","page":"Trees","title":"Base.print","text":"print(node)\n\nPrint a Decision Tree (textual)\n\n\n\n\n\n","category":"function"},{"location":"Trees.html#BetaML.Api.partition-Union{Tuple{Tx}, Tuple{BetaML.Trees.Question{Tx}, Any, Any}} where Tx","page":"Trees","title":"BetaML.Api.partition","text":"partition(question,x)\n\nDicotomically partitions a dataset x given a question.\n\nFor each row in the dataset, check if it matches the question. If so, add it to 'true rows', otherwise, add it to 'false rows'. Rows with missing values on the question column are assigned randomly proportionally to the assignment of the non-missing rows.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Api.predict-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Api.predict","text":"predict(forest,x)\n\nPredict the labels of a feature dataset.\n\nFor each record of the dataset and each tree of the \"forest\", recursivelly traverse the tree to find the prediction most opportune for the given record. If the labels the tree has been trained with are numeric, the prediction is also numeric (the mean of the different trees predictions, in turn the mean of the labels of the training records ended in that leaf node). If the labels were categorical, the prediction is a dictionary with the probabilities of each item and in such case the probabilities of the different trees are averaged to compose the forest predictions. This is a bit different than most other implementations where the mode instead is reported.\n\nIn the first case (numerical predictions) use meanRelError(ŷ,y) to assess the mean relative error, in the second case you can use accuracy(ŷ,y).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Api.predict-Union{Tuple{Ty}, Tuple{Tx}, Tuple{Union{DecisionNode{Tx}, Leaf{Ty}}, Any}} where {Tx, Ty}","page":"Trees","title":"BetaML.Api.predict","text":"predict(tree,x)\n\nPredict the labels of a feature dataset.\n\nFor each record of the dataset, recursivelly traverse the tree to find the prediction most opportune for the given record. If the labels the tree has been trained with are numeric, the prediction is also numeric. If the labels were categorical, the prediction is a dictionary with the probabilities of each item.\n\nIn the first case (numerical predictions) use meanRelError(ŷ,y) to assess the mean relative error, in the second case you can use accuracy(ŷ,y).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.buildForest-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.buildForest","text":"buildForest(x, y, nTrees; maxDepth, minGain, minRecords, maxFeatures, splittingCriterion, forceClassification)\n\nBuilds (define and train) a \"forest\" of Decision Trees.\n\nParameters:\n\nSee buildTree. The function has all the parameters of bildTree (with the maxFeatures defaulting to √D instead of D) plus the following parameters:\n\nnTrees: Number of trees in the forest [def: 30]\nβ: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction (see later) [def: 0, i.e. uniform weigths]\noob: Whether to coompute the out-of-bag error, an estimation of the generalization accuracy [def: false]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nOutput:\n\nThe function returns a Forest object (see Forest).\nThe forest weights default to array of ones if β ≤ 0 and the oob error to +Inf if oob == false.\n\nNotes :\n\nEach individual decision tree is built using bootstrap over the data, i.e. \"sampling N records with replacement\" (hence, some records appear multiple times and some records do not appear in the specific tree training). The maxFeature injects further variability and reduces the correlation between the forest trees.\nThe predictions of the \"forest\" (using the function predict()) are then the aggregated predictions of the individual trees (from which the name \"bagging\": boostrap aggregating).\nThis function optionally reports a weight distribution of the performances of eanch individual trees, as measured using the records he has not being trained with. These weights can then be (optionally) used in the predict function. The parameter β ≥ 0 regulate the distribution of these weights: larger is β, the greater the importance (hence the weights) attached to the best-performing trees compared to the low-performing ones. Using these weights can significantly improve the forest performances (especially using small forests), however the correct value of β depends on the problem under exam (and the chosen caratteristics of the random forest estimator) and should be cross-validated to avoid over-fitting.\nNote that this function uses multiple threads if these are available. You can check the number of threads available with Threads.nthreads(). To set the number of threads in Julia either set the environmental variable JULIA_NUM_THREADS (before starting Julia) or start Julia with the command line option --threads (most integrated development editors for Julia already set the number of threads to 4).\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.buildTree-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}}} where Ty","page":"Trees","title":"BetaML.Trees.buildTree","text":"buildTree(x, y, depth; maxDepth, minGain, minRecords, maxFeatures, splittingCriterion, forceClassification)\n\nBuilds (define and train) a Decision Tree.\n\nGiven a dataset of features x and the corresponding dataset of labels y, recursivelly build a decision tree by finding at each node the best question to split the data untill either all the dataset is separated or a terminal condition is reached. The given tree is then returned.\n\nParameters:\n\nx: The dataset's features (N × D)\ny: The dataset's labels (N × 1)\nmaxDepth: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: N, i.e. no limits]\nminGain: The minimum information gain to allow for a node's partition [def: 0]\nminRecords:  The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmaxFeatures: The maximum number of (random) features to consider at each partitioning [def: D, i.e. look at all features]\nsplittingCriterion: Either gini, entropy or variance (see infoGain ) [def: gini for categorical labels (classification task) and variance for numerical labels(regression task)]\nforceClassification: Weather to force a classification task even if the labels are numerical (typically when labels are integers encoding some feature rather than representing a real cardinal measure) [def: false]\nrng: Random Number Generator ((see FIXEDSEED)) [deafult: Random.GLOBAL_RNG]\n\nNotes:\n\nMissing data (in the feature dataset) are supported.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.findBestSplit-Union{Tuple{Ty}, Tuple{Any, AbstractVector{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.findBestSplit","text":"findBestSplit(x,y;maxFeatures,splittingCriterion)\n\nFind the best possible split of the database.\n\nFind the best question to ask by iterating over every feature / value and calculating the information gain.\n\nParameters:\n\nx: The feature dataset\ny: The labels dataset\nmaxFeatures: Maximum number of (random) features to look up for the \"best split\"\nsplittingCriterion: The metric to define the \"impurity\" of the labels\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.infoGain-Tuple{Any, Any, Any}","page":"Trees","title":"BetaML.Trees.infoGain","text":"infoGain(left, right, parentUncertainty; splittingCriterion)\n\nCompute the information gain of a specific partition.\n\nCompare the \"information gain\" my measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items.\n\nParameters:\n\nleftY:  Child #1 labels\nrightY: Child #2 labels\nparentUncertainty: \"Impurity\" of the labels of the parent node\nsplittingCriterion: Metric to adopt to determine the \"impurity\" (see below)\n\nYou can use your own function as the metric. We provide the following built-in metrics:\n\ngini (categorical)\nentropy (categorical)\nvariance (numerical)\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.match-Union{Tuple{Tx}, Tuple{BetaML.Trees.Question{Tx}, Any}} where Tx","page":"Trees","title":"BetaML.Trees.match","text":"match(question, x)\n\nReturn a dicotomic answer of a question when applied to a given feature record.\n\nIt compares the feature value in the given record to the value stored in the question. Numerical features are compared in terms of disequality (\">=\"), while categorical features are compared in terms of equality (\"==\").\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.oobError-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any, Any}} where Ty","page":"Trees","title":"BetaML.Trees.oobError","text":"oobError(forest,x,y)\n\nComute the Out-Of-Bag error, an estimation of the validation error.\n\nThis function is called at time of train the forest if the parameter oob is true, or can be used later to get the oob error on an already trained forest.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predictSingle-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any}} where Ty","page":"Trees","title":"BetaML.Trees.predictSingle","text":"predictSingle(forest,x)\n\nPredict the label of a single feature record. See predict.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.predictSingle-Union{Tuple{Ty}, Tuple{Tx}, Tuple{Union{DecisionNode{Tx}, Leaf{Ty}}, Any}} where {Tx, Ty}","page":"Trees","title":"BetaML.Trees.predictSingle","text":"predictSingle(tree,x)\n\nPredict the label of a single feature record. See predict.\n\n\n\n\n\n","category":"method"},{"location":"Trees.html#BetaML.Trees.updateTreesWeights!-Union{Tuple{Ty}, Tuple{Forest{Ty}, Any, Any}} where Ty","page":"Trees","title":"BetaML.Trees.updateTreesWeights!","text":"updateTreesWeights!(forest,x,y;β)\n\nUpdate the weights of each tree (to use in the prediction of the forest) based on the error of the individual tree computed on the records on which it has not been trained. As training a forest is expensive, this function can be used to \"just\" upgrade the trees weights using different betas, without retraining the model.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#The-BetaML.Utils-Module","page":"Utils","title":"The BetaML.Utils Module","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Utils\n","category":"page"},{"location":"Utils.html#BetaML.Utils","page":"Utils","title":"BetaML.Utils","text":"Utils module\n\nProvide shared utility functions for various machine learning algorithms. You don't usually need to import from this module, as each other module (Nn, Perceptron, Clusters,...) reexport it.\n\n\n\n\n\n","category":"module"},{"location":"Utils.html#Module-Index","page":"Utils","title":"Module Index","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Utils.html#Detailed-API","page":"Utils","title":"Detailed API","text":"","category":"section"},{"location":"Utils.html","page":"Utils","title":"Utils","text":"Modules = [Utils]","category":"page"},{"location":"Utils.html#BetaML.Utils.FIXEDSEED","page":"Utils","title":"BetaML.Utils.FIXEDSEED","text":"FIXEDSEED\n\nFixed seed to allow reproducible results. This is the seed used to obtain the same results under unit tests.\n\nUse it with:\n\nmyAlgorithm(;rng=FIXEDRNG)             # always produce the same sequence of results on each run of the script (\"pulling\" from the same rng object on different calls)\nmyAlgorithm(;rng=copy(FIXEDRNG)        # always produce the same result (new rng object on each call)\n\n\n\n\n\n","category":"constant"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y;ignoreLabels=false) - Categorical error (T vs T)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Array{Dict{T, Float64}, 1}, Vector{T}}} where T","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic predictions of a dataset (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic prediction of a single datapoint (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.reshape-Union{Tuple{T}, Tuple{T, Vararg{Any, N} where N}} where T<:Number","page":"Utils","title":"Base.reshape","text":"reshape(myNumber, dims..) - Reshape a number as a n dimensional Array \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Api.partition-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{Float64}}} where T<:AbstractArray","page":"Utils","title":"BetaML.Api.partition","text":"partition(data,parts;shuffle=true)\n\nPartition (by rows) one or more matrices according to the shares in parts.\n\nParameters\n\ndata: A matrix/vector or a vector of matrices/vectors\nparts: A vector of the required shares (must sum to 1)\nshufle: Whether to randomly shuffle the matrices (preserving the relative order between matrices)\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]\n\nExample:\n\njulia julia> x = [1:10 11:20] julia> y = collect(31:40) julia> ((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.7,0.3])\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;ignoreLabels=false) - Categorical accuracy between two vectors (T vs T). If \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Array{Dict{T, Float64}, 1}, Vector{T}}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic predictions of a dataset given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: A narray where each item is the estimated probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Dict{T, Float64}, T}} where T","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint given in terms of a dictionary of probabilities (Dict{T,Float64} vs T).\n\nParameters:\n\nŷ: The returned probability mass function in terms of a Dictionary(Item1 => Prob1, Item2 => Prob2, ...)\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Matrix{T}, Vector{Int64}}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol,ignoreLabels)\n\nCategorical accuracy with probabilistic predictions of a dataset (PMF vs Int).\n\nParameters:\n\nŷ: An (N,K) matrix of probabilities that each hat y_n record with n in 1N  being of category k with k in 1K.\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\nignoreLabels: Whether to ignore the specific label order in y. Useful for unsupervised learning algorithms where the specific label order don't make sense [def: false]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint (PMF vs Int).\n\nUse the parameter tol [def: 1] to determine the tollerance of the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.aic-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.aic","text":"aic(lL,k) -  Akaike information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.autoJacobian-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.autoJacobian","text":"autoJacobian(f,x;nY)\n\nEvaluate the Jacobian using AD in the form of a (nY,nX) madrix of first derivatives\n\nParameters:\n\nf: The function to compute the Jacobian\nx: The input to the function where the jacobian has to be computed\nnY: The number of outputs of the function f [def: length(f(x))]\n\nReturn values:\n\nAn Array{Float64,2} of the locally evaluated Jacobian\n\nNotes:\n\nThe nY parameter is optional. If provided it avoids having to compute f(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.batch-Tuple{Integer, Integer}","page":"Utils","title":"BetaML.Utils.batch","text":"batch(n,bSize;sequential=false,rng)\n\nReturn a vector of bSize vectors of indeces from 1 to n. Randomly unless the optional parameter sequential is used.\n\nExample:\n\njulia julia> Utils.batch(6,2,sequential=true) 3-element Array{Array{Int64,1},1}:  [1, 2]  [3, 4]  [5, 6]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.bic-Tuple{Any, Any, Any}","page":"Utils","title":"BetaML.Utils.bic","text":"bic(lL,k,n) -  Bayesian information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.celu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.celu","text":"celu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.classCounts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.classCounts","text":"classCounts(x)\n\nReturn a dictionary that counts the number of each unique item (rows) in a dataset.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.colsWithMissing-Tuple{Any}","page":"Utils","title":"BetaML.Utils.colsWithMissing","text":"colsWithMissing(x)\n\nRetuyrn an array with the ids of the columns where there is at least a missing value.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.cosine_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.cosine_distance","text":"Cosine distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.crossEntropy-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.crossEntropy","text":"crossEntropy(ŷ, y; weight)\n\nCompute the (weighted) cross-entropy between the predicted and the sampled probability distributions.\n\nTo be used in classification problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dcelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dcelu","text":"dcelu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.delu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.delu","text":"delu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dmish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dmish","text":"dmish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dplu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dplu","text":"dplu(x;α=0.1,c=1) \n\nPiecewise Linear Unit derivative \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.drelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.drelu","text":"drelu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsigmoid","text":"dsigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftmax","text":"dsoftmax(x; β=1) \n\nDerivative of the softmax function \n\nhttps://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftplus","text":"dsoftplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dtanh-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dtanh","text":"dtanh(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.elu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.elu","text":"elu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.entropy-Tuple{Any}","page":"Utils","title":"BetaML.Utils.entropy","text":"entropy(x)\n\nCalculate the entropy for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Gini_impurity\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.generateParallelRngs-Tuple{Random.AbstractRNG, Integer}","page":"Utils","title":"BetaML.Utils.generateParallelRngs","text":"generateParallelRngs(rng::AbstractRNG, n::Integer;reSeed=false)\n\nFor multi-threaded models, return n independent random number generators (one per thread) to be used in threaded computations.\n\nNote that each ring is a copy of the original random ring. This means that code that use these RNGs will not change the original RNG state.\n\nUse it with rngs = generateParallelRngs(rng,Threads.nthreads()) to have a separate rng per thread. By default the function doesn't re-seed the RNG, as you may want to have a loop index based re-seeding strategy rather than a threadid-based one (to guarantee the same result independently of the number of threads). If you prefer, you can instead re-seed the RNG here (using the parameter reSeed=true), such that each thread has a different seed. Be aware however that the stream  of number generated will depend from the number of threads at run time.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getPermutations-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.getPermutations","text":"getPermutations(v::AbstractArray{T,1};keepStructure=false)\n\nReturn a vector of either (a) all possible permutations (uncollected) or (b) just those based on the unique values of the vector\n\nUseful to measure accuracy where you don't care about the actual name of the labels, like in unsupervised classifications (e.g. clustering)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getScaleFactors-Tuple{Any}","page":"Utils","title":"BetaML.Utils.getScaleFactors","text":"getScaleFactors(x;skip)\n\nReturn the scale factors (for each dimensions) in order to scale a matrix X (n,d) such that each dimension has mean 0 and variance 1.\n\nParameters\n\nx: the (n × d) dimension matrix to scale on each dimension d\nskip: an array of dimension index to skip the scaling [def: []]\n\nReturn\n\nA touple whose first elmement is the shift and the second the multiplicative\n\nterm to make the scale.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.gini-Tuple{Any}","page":"Utils","title":"BetaML.Utils.gini","text":"gini(x)\n\nCalculate the Gini Impurity for a list of items (or rows).\n\nSee: https://en.wikipedia.org/wiki/Decisiontreelearning#Information_gain\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerDecoder-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T","page":"Utils","title":"BetaML.Utils.integerDecoder","text":"integerDecoder(x,factors::AbstractVector{T};unique)\n\nDecode an array of integers to an array of T corresponding to the elements of factors\n\nParameters:\n\nx: The vector to decode\nfactors: The vector of elements to use for the encoding\nunique: Wether factors is already made of unique elements [def: true]\n\nReturn:\n\nA vector of length(x) elements corresponding to the (unique) factors elements at the position x\n\nExample:\n\njulia> integerDecoder([1, 2, 2, 3, 2, 1],[\"aa\",\"cc\",\"bb\"]) # out: [\"aa\",\"cc\",\"cc\",\"bb\",\"cc\",\"aa\"]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.integerEncoder-Tuple{AbstractVector{T} where T}","page":"Utils","title":"BetaML.Utils.integerEncoder","text":"integerEncoder(x;factors=unique(x))\n\nEncode an array of T to an array of integers using the their position in factor vector (default to the unique vector of the input array)\n\nParameters:\n\nx: The vector to encode\nfactors: The vector of factors whose position is the result of the encoding [def: unique(x)]\n\nReturn:\n\nA vector of [1,length(x)] integers corresponding to the position of each element in the factors vector`\n\nNote:\n\nAttention that while this function creates a ordered (and sortable) set, it is up to the user to be sure that this \"property\" is not indeed used in his code if the unencoded data is indeed unordered.\n\nExample:\n\njulia> integerEncoder([\"a\",\"e\",\"b\",\"e\"],factors=[\"a\",\"b\",\"c\",\"d\",\"e\"]) # out: [1,5,2,5]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.issortable-Union{Tuple{AbstractArray{T, N}}, Tuple{N}, Tuple{T}} where {T, N}","page":"Utils","title":"BetaML.Utils.issortable","text":"Return wheather an array is sortable, i.e. has methos issort defined\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l1_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l1_distance","text":"L1 norm distance (aka Manhattan Distance)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l2_distance","text":"Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2²_distance-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.l2²_distance","text":"Squared Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.lse-Tuple{Any}","page":"Utils","title":"BetaML.Utils.lse","text":"LogSumExp for efficiently computing log(sum(exp.(x))) \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.makeMatrix-Tuple{AbstractArray}","page":"Utils","title":"BetaML.Utils.makeMatrix","text":"Transform an Array{T,1} in an Array{T,2} and leave unchanged Array{T,2}.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.meanDicts-Tuple{Any}","page":"Utils","title":"BetaML.Utils.meanDicts","text":"meanDicts(dicts)\n\nCompute the mean of the values of an array of dictionaries.\n\nGiven dicts an array of dictionaries, meanDicts first compute the union of the keys and then average the values. If the original valueas are probabilities (non-negative items summing to 1), the result is also a probability distribution.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.meanRelError-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.meanRelError","text":"meanRelError(ŷ,y;normDim=true,normRec=true,p=1)\n\nCompute the mean relative error (l-1 based by default) between ŷ and y.\n\nThere are many ways to compute a mean relative error. In particular, if normRec (normDim) is set to true, the records (dimensions) are normalised, in the sense that it doesn't matter if a record (dimension) is bigger or smaller than the others, the relative error is first computed for each record (dimension) and then it is averaged. With both normDim and normRec set to false the function returns the relative mean error; with both set to true (default) it returns the mean relative error (i.e. with p=1 the \"mean absolute percentage error (MAPE)\") The parameter p [def: 1] controls the p-norm used to define the error.\n\nThe mean relative error enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the relative mean error the same relative error on larger observations (or dimensions) weights more.\n\nFor example, given y = [1,44,3] and ŷ = [2,45,2], the mean relative error meanRelError(ŷ,y) is 0.452, while the relative mean error meanRelError(ŷ,y, normRec=false) is \"only\" 0.0625.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.mish","text":"mish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mode-Union{Tuple{AbstractArray{Dict{T, Float64}, N} where N}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.mode","text":"mode(dicts)\n\nGiven a vector of dictionaries representing probabilities it returns the mode of each element in terms of the key\n\nUse it to return a unique value from a multiclass classifier returning probabilities.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.oneHotEncoder-Union{Tuple{Union{AbstractVector{T}, T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.oneHotEncoder","text":"oneHotEncoder(x;d,factors,count)\n\nEncode arrays (or arrays of arrays) of categorical data as matrices of one column per factor.\n\nThe case of arrays of arrays is for when at each record you have more than one categorical output. You can then decide to encode just the presence of the factors or their counting\n\nParameters:\n\nx: The data to convert (array or array of arrays)\nd: The number of dimensions in the output matrix [def: maximum(x) for integers and length(factors) otherwise]\nfactors: The factors from which to encode [def: 1:d for integer x or unique(x) otherwise]\ncount: Wether to count multiple instances on the same dimension/record (true) or indicate just presence. [def: false]\n\nExamples\n\njulia> oneHotEncoder([\"a\",\"c\",\"c\"],factors=[\"a\",\"b\",\"c\",\"d\"])\n3×4 Matrix{Int64}:\n 1  0  0  0\n 0  0  1  0\n 0  0  1  0\njulia> oneHotEncoder([2,4,4])\n3×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  0  1\n 0  0  0  1\n julia> oneHotEncoder([[2,2,1],[2,4,4]],count=true)\n2×4 Matrix{Int64}:\n 1  2  0  0\n 0  1  0  2\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.pca-Tuple{Any}","page":"Utils","title":"BetaML.Utils.pca","text":"pca(X;K,error)\n\nPerform Principal Component Analysis returning the matrix reprojected among the dimensions of maximum variance.\n\nParameters:\n\nX : The (N,D) data to reproject\nK : The number of dimensions to maintain (with K<=D) [def: nothing]\nerror: The maximum approximation error that we are willing to accept [def: 0.05]\n\nReturn:\n\nA named tuple with:\nX: The reprojected (NxK) matrix with the column dimensions organized in descending order of of the proportion of explained variance\nK: The number of dimensions retieved\nerror: The actual proportion of variance not explained in the reprojected dimensions\nP: The (D,K) matrix of the eigenvectors associated to the K-largest eigenvalues used to reproject the data matrix\nexplVarByDim: An array of dimensions D with the share of the cumulative variance explained by dimensions (the last element being always 1.0)\n\nNotes:\n\nIf K is provided, the parameter error has no effect.\nIf one doesn't know a priori the error that she/he is willling to accept, nor the wished number of dimensions, he/she can run this pca function with out = pca(X,K=size(X,2)) (i.e. with K=D), analise the proportions of explained cumulative variance by dimensions in out.explVarByDim, choose the number of dimensions K according to his/her needs and finally pick from the reprojected matrix only the number of dimensions needed, i.e. out.X[:,1:K].\n\nExample:\n\njulia> X = [1 10 100; 1.1 15 120; 0.95 23 90; 0.99 17 120; 1.05 8 90; 1.1 12 95]\n6×3 Array{Float64,2}:\n 1.0   10.0  100.0\n 1.1   15.0  120.0\n 0.95  23.0   90.0\n 0.99  17.0  120.0\n 1.05   8.0   90.0\n 1.1   12.0   95.0\n julia> X = pca(X,error=0.05).X\n6×2 Array{Float64,2}:\n  3.1783   100.449\n  6.80764  120.743\n 16.8275    91.3551\n  8.80372  120.878\n  1.86179   90.3363\n  5.51254   95.5965\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.plu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.plu","text":"plu(x;α=0.1,c=1) \n\nPiecewise Linear Unit \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.polynomialKernel-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.polynomialKernel","text":"Polynomial kernel parametrised with c=0 and d=2 (i.e. a quadratic kernel). For other cᵢ and dᵢ use K = (x,y) -> polynomialKernel(x,y,c=cᵢ,d=dᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.radialKernel-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.radialKernel","text":"Radial Kernel (aka RBF kernel) parametrised with γ=1/2. For other gammas γᵢ use K = (x,y) -> radialKernel(x,y,γ=γᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.relu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.relu","text":"relu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.scale","page":"Utils","title":"BetaML.Utils.scale","text":"scale(x,scaleFactors;rev)\n\nPerform a linear scaling of x using scaling factors scaleFactors.\n\nParameters\n\nx: The (n × d) dimension matrix to scale on each dimension d\nscalingFactors: A tuple of the constant and multiplicative scaling factor\n\nrespectively [def: the scaling factors needed to scale x to mean 0 and variance 1]\n\nrev: Whether to invert the scaling [def: false]\n\nReturn\n\nThe scaled matrix\n\nNotes:\n\nAlso available scale!(x,scaleFactors) for in-place scaling.\nRetrieve the scale factors with the getScaleFactors() function\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.sigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.sigmoid","text":"sigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.singleUnique-Union{Tuple{Union{AbstractArray{T, N} where N, T}}, Tuple{T}} where T","page":"Utils","title":"BetaML.Utils.singleUnique","text":"singleUnique(x) Return the unique values of x whether x is an array of arrays, an array or a scalar\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softmax","text":"softmax (x; β=1) \n\nThe input x is a vector. Return a PMF\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softplus","text":"softplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.squaredCost-Tuple{Any, Any}","page":"Utils","title":"BetaML.Utils.squaredCost","text":"squaredCost(ŷ,y)\n\nCompute the squared costs between a vector of prediction and one of observations as (1/2)*norm(y - ŷ)^2.\n\nAside the 1/2 term, it correspond to the squared l-2 norm distance and when it is averaged on multiple datapoints corresponds to the Mean Squared Error (MSE). It is mostly used for regression problems.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.sterling-Tuple{BigInt, BigInt}","page":"Utils","title":"BetaML.Utils.sterling","text":"Sterling number: number of partitions of a set of n elements in k sets \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.variance-Tuple{Any}","page":"Utils","title":"BetaML.Utils.variance","text":"variance(x) - population variance\n\n\n\n\n\n","category":"method"}]
}
