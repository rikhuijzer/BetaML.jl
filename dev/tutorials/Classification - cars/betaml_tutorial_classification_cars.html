<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A classification task when labels are known - determining the country of origin of cars given the cars characteristics · BetaML.jl Documentation</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">BetaML.jl Documentation</span></div><form class="docs-search" action="../../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li><li><span class="tocitem">Tutorials</span><ul><li><input class="collapse-toggle" id="menuitem-7-1" type="checkbox"/><label class="tocitem" for="menuitem-7-1"><span class="docs-label">Getting started</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Getting started/betaml_tutorial_getting_started.html">-</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-2" type="checkbox"/><label class="tocitem" for="menuitem-7-2"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-3" type="checkbox" checked/><label class="tocitem" for="menuitem-7-3"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a><ul class="internal"><li><a class="tocitem" href="#Library-and-data-loading"><span>Library and data loading</span></a></li><li><a class="tocitem" href="#Random-Forests"><span>Random Forests</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-4" type="checkbox"/><label class="tocitem" for="menuitem-7-4"><span class="docs-label">Clusterisation - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Clusterisation - Iris/betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris tdataset)</a></li></ul></li></ul></li><li><a class="tocitem" href="../../Examples.html">Examples</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">Classification - cars</a></li><li class="is-active"><a href="betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Classification - cars/betaml_tutorial_classification_cars.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="classification_tutorial"><a class="docs-heading-anchor" href="#classification_tutorial">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a><a id="classification_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#classification_tutorial" title="Permalink"></a></h1><p>In this exercise we have some car technical characteristics (mpg, horsepower,weight, model year...) and the country of origin and we want to create a model such that the country of origin can be accurately predicted given the technical characteristics.</p><p>Data origin:</p><ul><li>dataset description: <a href="https://archive.ics.uci.edu/ml/datasets/auto+mpg">https://archive.ics.uci.edu/ml/datasets/auto+mpg</a></li><li>data source we use here: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data">https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data</a></li></ul><p>field description</p><ol><li>mpg:           continuous</li><li>cylinders:     multi-valued discrete</li><li>displacement:  continuous</li><li>horsepower:    continuous</li><li>weight:        continuous</li><li>acceleration:  continuous</li><li>model year:    multi-valued discrete</li><li>origin:        multi-valued discrete</li><li>car name:      string (unique for each instance) - not used here</li></ol><h2 id="Library-and-data-loading"><a class="docs-heading-anchor" href="#Library-and-data-loading">Library and data loading</a><a id="Library-and-data-loading-1"></a><a class="docs-heading-anchor-permalink" href="#Library-and-data-loading" title="Permalink"></a></h2><p>We load a buch of packages that we&#39;ll use during this tutorial..</p><pre><code class="language-julia">using Random, HTTP, CSV, DataFrames, BenchmarkTools, BetaML
import DecisionTree, Flux
import Pipe: @pipe</code></pre><p>To load the data from the internet our workflow is (1) Retrieve the data -&gt; (2) Clean it -&gt; (3) Load it -&gt; (4) Output it as a DataFrame</p><p>For step 1 we use HTTP.get(), for step (2) we use <code>replace!</code>, for steps (3) and (4) we uses the CSV package, and we use the &quot;pip&quot; <code>|&gt;</code> operator to chain these operations:</p><pre><code class="language-julia">urlDataOriginal = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original&quot;
data = @pipe HTTP.get(urlDataOriginal).body                                                |&gt;
             replace!(_, UInt8(&#39;\t&#39;) =&gt; UInt8(&#39; &#39;))                                        |&gt;
             CSV.File(_, delim=&#39; &#39;, missingstring=&quot;NA&quot;, ignorerepeated=true, header=false) |&gt;
             DataFrame;</code></pre><p>This results in a table where the rows are the observations (the various cars) and the column the fields. All BetaML models expect this layout. As the dataset is ordered, we randomly shuffle the data. Note that we pass to shuffle <code>copy(FIXEDRNG)</code> as the random nuber generator in order to obtain reproducible output.</p><pre><code class="language-julia">data[shuffle(copy(FIXEDRNG),axes(data, 1)), :]
describe(data)</code></pre><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Int64</th><th>Type</th></tr></thead><tbody><p>9 rows × 7 columns</p><tr><th>1</th><td>Column1</td><td>23.5146</td><td>9.0</td><td>23.0</td><td>46.6</td><td>8</td><td>Union{Missing, Float64}</td></tr><tr><th>2</th><td>Column2</td><td>5.47537</td><td>3.0</td><td>4.0</td><td>8.0</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>Column3</td><td>194.78</td><td>68.0</td><td>151.0</td><td>455.0</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>Column4</td><td>105.082</td><td>46.0</td><td>95.0</td><td>230.0</td><td>6</td><td>Union{Missing, Float64}</td></tr><tr><th>5</th><td>Column5</td><td>2979.41</td><td>1613.0</td><td>2822.5</td><td>5140.0</td><td>0</td><td>Float64</td></tr><tr><th>6</th><td>Column6</td><td>15.5197</td><td>8.0</td><td>15.5</td><td>24.8</td><td>0</td><td>Float64</td></tr><tr><th>7</th><td>Column7</td><td>75.9212</td><td>70.0</td><td>76.0</td><td>82.0</td><td>0</td><td>Float64</td></tr><tr><th>8</th><td>Column8</td><td>1.56897</td><td>1.0</td><td>1.0</td><td>3.0</td><td>0</td><td>Float64</td></tr><tr><th>9</th><td>Column9</td><td></td><td>amc ambassador brougham</td><td></td><td>vw rabbit custom</td><td>0</td><td>String</td></tr></tbody></table><p>Columns 1 to 7 contain  characteristics of the car, while column 8 encodes the country or origin (&quot;1&quot; -&gt; US, &quot;2&quot; -&gt; EU, &quot;3&quot; -&gt; Japan) that we want to be able to predict. Columns 9 contains the car name, but we are not going to use this information in this tutorial. Note also that some fields have missing data. Our first step is hence to divide the dataset in features (the x) and the labels (the y) we want to predict. The <code>x</code> is then a Julia standard <code>Matrix</code> of 406 rows by 7 columns and the <code>y</code> is a vector of the 406 observations:</p><pre><code class="language-julia">x     = Matrix{Union{Missing,Float64}}(data[:,1:7]);
y     = Vector{Int64}(data[:,8]);</code></pre><p>Some algorithms that we will use today don&#39;t like missing data, so we need to <em>impute</em> them. Foir this we are using the <a href="../../Clustering.html#BetaML.Clustering.predictMissing-Tuple{Any, Any}"><code>predictMissing</code></a> function provided by the <a href="../../Clustering.html#BetaML.Clustering"><code>Clustering</code></a> sub-module. Internally it uses a Gaussian Mixture Model to assign to the missing walue of a given record an average of the values of the non-missing records weighted for how much close they are to our specific record.</p><pre><code class="language-julia">xFull = predictMissing(x,3,rng=copy(FIXEDRNG)).X̂;</code></pre><pre class="documenter-example-output">Iter. 1:	Var. of the post  20.10554158364678 	  Log-likelihood -12110.417616527317</pre><p>Further, some models don&#39;t work with categorical data as such, so we need to represent our y as a matrix with a separate column for each possible value (the so called &quot;one-hot&quot; representation). To encode as one-hot we use the function <a href="../../Utils.html#BetaML.Utils.oneHotEncoder-Union{Tuple{Union{AbstractVector{T}, T}}, Tuple{T}} where T"><code>oneHotEncoder</code></a> in submodule <a href="../../Utils.html#BetaML.Utils"><code>BetaML.Utils</code></a></p><pre><code class="language-julia">y_oh  = oneHotEncoder(y); ## Convert to One-hot representation (e.g. 2 =&gt; [0 1 0], 3 =&gt; [0 0 1])</code></pre><p>In supervised machine learning it is good practice to partition the available data in a <em>training</em>, <em>validation</em>, and <em>test</em> subsets, where the first one is used to train the ML algorithm, the second one to train any eventual &quot;hyperparameters&quot; of the algorithm and the <em>test</em> subset is finally used to evaluate the quality of the algorithm. Here, for brevity, we use only the <em>train</em> and the <em>test</em> subsets, implicitly assuming we already know the best hyperparameters. Please refer to the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a> for examples of how to use the validation subset to train the hyperparameters. We use then the <a href="../../Trees.html#BetaML.Api.partition-Union{Tuple{Tx}, Tuple{BetaML.Trees.Question{Tx}, Any, Any}} where Tx"><code>partition</code></a> function, where we can specify the different data to partition (that must have the same number of observations) and the shares of observation that we want in each subset.</p><pre><code class="language-julia">((xtrain,xtest),(xtrainFull,xtestFull),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,xFull,y,y_oh],[0.8,1-0.8],rng=copy(FIXEDRNG));</code></pre><h2 id="Random-Forests"><a class="docs-heading-anchor" href="#Random-Forests">Random Forests</a><a id="Random-Forests-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Forests" title="Permalink"></a></h2><p>We are now ready to use our first model, the Random Forests (in the <a href="../../Trees.html#BetaML.Trees"><code>BetaML.Trees</code></a> sub<em>module) <a href="tutorials/Classification - cars/@ref BetaML.Trees">Random Forests</a>. Random Forests build a &quot;forest&quot; of decision trees models and then use their averaged predictions to make a overall prediction out of a feature matrix. To &quot;build&quot; the forest model (i.e. to &quot;train&quot; it) we need to give the model the training feature matrix and the associated &quot;true&quot; training labels, and we need to specify the number of trees to use (this is an example of hyperparameters). Here we use 30 individual decision trees. As the labels are encoded using integers,  we need also to use the parameter <code>forceClassification=true</code> otherwide the model would undergo a _regression</em> job.</p><pre><code class="language-julia">myForest       = buildForest(xtrain,ytrain,30, rng=copy(FIXEDRNG),forceClassification=true);</code></pre><p>To obtain the predicted values, we can simply use the function <a href="../../Perceptron.html#BetaML.Api.predict"><code>BetaML.Trees.predict</code></a> with our <code>myForest</code> model and either the training or testing data.</p><pre><code class="language-julia">ŷtrain,ŷtest   = predict.(Ref(myForest), [xtrain,xtest],rng=copy(FIXEDRNG));</code></pre><p>Finally we can measure the <em>accuracy</em> of our predictions with the <a href="../../Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T"><code>accuracy</code></a> function:</p><pre><code class="language-julia">trainAccuracy,testAccuracy  = accuracy.([parse.(Int64,mode(ŷtrain)),parse.(Int64,mode(ŷtest))],[ytrain,ytest])</code></pre><pre class="documenter-example-output">2-element Vector{Float64}:
 0.9969230769230769
 0.8024691358024691</pre><p>The predictions are quite good, for the training set the algoritm predicted almost all cars&#39; origins correctly, while for the testing set (i.e. those records that has <strong>not</strong> been used to train the algorithm), the correct prediction level is still quite high, at 80% When we benchmark the resourse used (time and memory) we find that Random Forests remain pretty fast, expecially when we compare them with neural networks (see later)</p><pre><code class="language-julia">@btime buildForest(xtrain,ytrain,30, rng=copy(FIXEDRNG),forceClassification=true);</code></pre><pre class="documenter-example-output">  355.478 ms (780963 allocations: 196.30 MiB)</pre><h3 id="Comparision-with-DecisionTree.jl"><a class="docs-heading-anchor" href="#Comparision-with-DecisionTree.jl">Comparision with DecisionTree.jl</a><a id="Comparision-with-DecisionTree.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Comparision-with-DecisionTree.jl" title="Permalink"></a></h3><p>DecisionTrees.jl random forests are similar in usage: we first &quot;build&quot; (train) the forest and we then make predictions out of the trained model. The main difference is that the model requires data with nonmissing values, so we are going to use the <code>xtrainFull</code> and <code>xtestFull</code> feature labels we created earlier:</p><pre><code class="language-julia"># We train the model...
model = DecisionTree.build_forest(ytrain, xtrainFull,-1,30,rng=123)
# ..and we generate predictions and measure their error
(ŷtrain,ŷtest) = DecisionTree.apply_forest.([model],[xtrainFull,xtestFull]);
(trainAccuracy,testAccuracy) = accuracy.([ŷtrain,ŷtest],[ytrain,ytest])</code></pre><pre class="documenter-example-output">2-element Vector{Float64}:
 0.9969230769230769
 0.7530864197530864</pre><p>While the accuracy on the training set is exactly the same as for <code>BetaML</code> random forets, <code>DecisionTree.jl</code> random forests are slighly less accurate in the testing sample. Where however <code>DecisionTrees.jl</code> excell is in the efficiency: they are extremelly fast and memory parse, even if here to this benchmark we should add the resources need to impute the missing values. Also, one of the reasons DecisionTrees are such efficient is that internally they sort the data to avoid repeated comparision, but in this way they work only with features that are sortable, while BetaML random forests accept virtually any kind of input without the need of adapt it.</p><pre><code class="language-julia">@btime  DecisionTree.build_forest(ytrain, xtrainFull,-1,30,rng=123);</code></pre><pre class="documenter-example-output">  3.182 ms (10875 allocations: 1.52 MiB)</pre><h3 id="Neural-network"><a class="docs-heading-anchor" href="#Neural-network">Neural network</a><a id="Neural-network-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-network" title="Permalink"></a></h3><p>Neural networks (NN) can be very powerfull, but have two &quot;inconvenients&quot; compared with random forests: First, are a bit &quot;picky&quot;. We need to do a bit of work to provide data in specific format. Note that this is <em>not</em> feature engineering. One of the advantages on neural network is that for the most this is not needed for neural networks. However we still need to &quot;clean&quot; the data. One issue is that NN don&#39;t like missing data. So we need to provide them with the feature matrix &quot;clean&quot; of missing data. Secondly, they work only with numerical data. So we need to use the one-hot encoding we saw earlier.</p><pre><code class="language-julia">#Further, they work best if the features are scaled such that each feature has mean zero and standard deviation 1. We can achieve it with the function [`scale`](@ref) or, as in this case, [`getScaleFactors`](@ref).
xScaleFactors   = getScaleFactors(xtrainFull)
D               = size(xtrainFull,2)
classes         = unique(y)
nCl             = length(classes)</code></pre><pre class="documenter-example-output">3</pre><p>The second &quot;inconvenient&quot; of NN i that, while not requiring feature engineering, they stil lneed a bit of practice on the way to build the network. It&#39;s not as simple as <code>train(model,x,y)</code>. We need here to specify how we want our layers, <em>chain</em> the layers together and then decide a <em>loss</em> overall function. Only when we done these steps, we have the model ready for training. Here we define 3 <a href="../../Nn.html#BetaML.Nn.DenseLayer"><code>DenseLayer</code></a> zwhere, for each of them, we specify the number of neurons in input (the first layer being equal to the dimensions of the data), the output layer (for a classification task, the last layer output size beying equal to the number of classes) and an _activation function for each layer (default the <code>identity</code> function).</p><pre><code class="language-julia">ls   = 80
l1   = DenseLayer(D,ls,f=relu,rng=copy(FIXEDRNG)) ## Activation function is ReLU
l2   = DenseLayer(ls,ls,f=relu,rng=copy(FIXEDRNG))
l3   = DenseLayer(ls,nCl,f=relu,rng=copy(FIXEDRNG))</code></pre><pre class="documenter-example-output">DenseLayer([-0.17152304682574077 -0.2458887996935557 … -0.1809986944509141 0.15856831110153013; -0.07125500994061709 -0.03334429781363102 … -0.16852619929260615 -0.19829755136926608; 0.09090827333599694 -0.008985446286453647 … -0.1993457323421579 0.16251427157907766], [-0.1631142011905145, -0.18252959792804174, 0.16273954026172094], BetaML.Utils.relu, nothing)</pre><p>For a classification the last layer is a <a href="../../Nn.html#BetaML.Nn.VectorFunctionLayer"><code>VectorFunctionLayer</code></a> that has no learnable parameters but whose activation function is applied to the ensemble of the neurons, rather than individually on each neuron. In particular, for classification we pass the <a href="../../Utils.html#BetaML.Utils.softmax-Tuple{Any}"><code>BetaML.Utils.softmax</code></a> function whose output has the same size as the input (and the number of classes to predict), but we can use the <code>VectorFunctionLayer</code> with any function, including the <a href="../../Utils.html#BetaML.Utils.pool1d"><code>pool1d</code></a> function to create a &quot;pooling&quot; layer (using maximum, mean or whatever other subfunction we pass to <code>pool1d</code>)</p><pre><code class="language-julia">l4   = VectorFunctionLayer(nCl,f=softmax) ## Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once</code></pre><pre class="documenter-example-output">VectorFunctionLayer(3, 3, BetaML.Utils.softmax, nothing)</pre><p>Finally we <em>chain</em> the layers and assign a loss function</p><pre><code class="language-julia">mynn = buildNetwork([l1,l2,l3],squaredCost,name=&quot;Multinomial logistic regression Model Cars&quot;) ## Build the NN and use the squared cost (aka MSE) as error function (crossEntropy could also be used)</code></pre><pre class="documenter-example-output">NN(AbstractLayer[DenseLayer([-0.16753359293154152 -0.08396611435645382 … 0.05347934439237256 0.13799847104685847; -0.06959768993523258 -0.2159497981141119 … 0.1803476756640992 0.11876779285191691; … ; -0.26122655256300037 0.10937214263174544 … 0.20817809082352795 0.16347763041445562; 0.05480302100503903 -0.22405872517130204 … 0.01537086620234912 -0.2535846266786569], [-0.05786849661797441, -0.21111495911326197, -0.1862311347142502, 0.18660544475070173, 0.16807257314981666, -0.14998562928637318, -0.12728613491899823, -0.18461677105731672, -0.19122062560019715, -0.1831865410437677  …  0.23857000235487913, -0.007081627637399057, 0.14544076996085625, 0.11511842136552952, 0.06353015862225225, -0.21269147385986734, -0.26108049860533494, -0.2454214698237953, -0.21261295737424285, -0.22259462168894284], BetaML.Utils.relu, nothing), DenseLayer([-0.12353827630961124 -0.061916113983484455 … 0.007554433081341655 -0.13610387416173572; -0.05132092316102099 -0.15924009842806428 … 0.16162831959412724 -0.18487352846246305; … ; -0.19262690822324854 0.08065036832664349 … 0.052722371460339645 0.05319278358501697; 0.04041142216945379 -0.16521957307453103 … -0.05653041905221359 0.16992517555817585], [0.13495566033921388, -0.0819197049446383, 0.17063692814357992, -0.13198898536887, -0.0023063383935972293, -0.1212310790099031, -0.11523779448414907, -0.15358474765839691, 0.09501689736921423, -0.11646844866580806  …  -0.18313564188199127, -0.10610357217398922, 0.016107298141093324, 0.19052389693482571, -0.1429275767760467, -0.1266955179828183, -0.05351936094532944, -0.17625143165725632, 0.11703021685473977, -0.17622543591703171], BetaML.Utils.relu, nothing), DenseLayer([-0.17152304682574077 -0.2458887996935557 … -0.1809986944509141 0.15856831110153013; -0.07125500994061709 -0.03334429781363102 … -0.16852619929260615 -0.19829755136926608; 0.09090827333599694 -0.008985446286453647 … -0.1993457323421579 0.16251427157907766], [-0.1631142011905145, -0.18252959792804174, 0.16273954026172094], BetaML.Utils.relu, nothing)], BetaML.Utils.squaredCost, nothing, false, &quot;Multinomial logistic regression Model Cars&quot;)</pre><p>Now we can train our network using the function <a href="../../Nn.html#BetaML.Nn.train!-Tuple{NN, Any, Any}"><code>train!</code></a>. It has many options, have a look at the documentation for all the possible arguments. Note that we trained the network based on the scaled feature matrix</p><pre><code class="language-julia">res = train!(mynn,scale(xtrainFull,xScaleFactors),ytrain_oh,epochs=300,batchSize=16,rng=copy(FIXEDRNG)) ## Use optAlg=SGD() to use Stochastic Gradient Descent instead</code></pre><pre class="documenter-example-output">(epochs = 300, ϵ_epochs = [0.5205909381547231, 0.45745290172506825, 0.4534218545260661, 0.4513078027416439, 0.4485243071127398, 0.44659423162583994, 0.4454099144881782, 0.44448146683147377, 0.4426721461661249, 0.4428378927086748  …  0.4122661822192426, 0.40992889142921424, 0.4073980003479463, 0.40638211628178206, 0.40610244295569653, 0.4057987016790151, 0.40602275440790997, 0.4059872573599337, 0.4062988578268179, 0.40616735336958615], θ_epochs = Any[])</pre><p>Once trained, we can predict the label. As the trained was based on the scaled feature matrix, so must be for the predictions</p><pre><code class="language-julia">(ŷtrain,ŷtest)  = predict.(Ref(mynn),[scale(xtrainFull,xScaleFactors),scale(xtestFull,xScaleFactors)])
(trainAccuracy,testAccuracy) = accuracy.([ŷtrain,ŷtest],[ytrain,ytest])</code></pre><pre class="documenter-example-output">2-element Vector{Float64}:
 0.9753846153846154
 0.8765432098765432</pre><p>With neural networks the tesst accuracy improves of 7 percentual points. However this come with a large computational cost, at the training takes now several seconds:</p><pre><code class="language-julia">@btime train!(mynn,scale(xtrainFull),ytrain_oh,epochs=300,batchSize=8,rng=copy(FIXEDRNG),verbosity=NONE);</code></pre><pre class="documenter-example-output">  14.944 s (18322341 allocations: 21.72 GiB)</pre><h3 id="Comparisons-with-Flux"><a class="docs-heading-anchor" href="#Comparisons-with-Flux">Comparisons with Flux</a><a id="Comparisons-with-Flux-1"></a><a class="docs-heading-anchor-permalink" href="#Comparisons-with-Flux" title="Permalink"></a></h3><p>In Flux the input bust be in the form (fields, observations), so we transpose our original matrices</p><pre><code class="language-julia">xtrainT, ytrain_ohT = transpose.([scale(xtrainFull,xScaleFactors), ytrain_oh])
xtestT, ytest_ohT = transpose.([scale(xtestFull,xScaleFactors), ytest_oh])</code></pre><pre class="documenter-example-output">2-element Vector{LinearAlgebra.Transpose{Float64, Matrix{Float64}}}:
 [-0.9354691121867126 0.8211030586728023 … 1.6357452248685191 -0.5790631644760863; 0.3321589709665676 -0.8476406526851753 … -0.2577408408593039 0.3321589709665676; … ; 0.08499289148756756 2.234618551678942 … 1.5539037592850058 -0.022488391522001404; 0.582575265003537 1.1235380110782482 … 1.1235380110782482 -1.3107943462579525]
 [0.0 0.0 … 0.0 1.0; 1.0 1.0 … 1.0 0.0; 0.0 0.0 … 0.0 0.0]</pre><p>We define the Flux neural network model in a similar way than BetaML and load it with data, we train it, predict and measure the accuracies on the training and the test sets:</p><pre><code class="language-julia">Random.seed!(123)

l1         = Flux.Dense(D,ls,Flux.relu)
l2         = Flux.Dense(ls,ls,Flux.relu)
l3         = Flux.Dense(ls,nCl,Flux.relu)
Flux_nn    = Flux.Chain(l1,l2,l3)
loss(x, y) = Flux.logitcrossentropy(Flux_nn(x), y)
ps         = Flux.params(Flux_nn)
nndata     = Flux.Data.DataLoader((xtrainT, ytrain_ohT), batchsize=16,shuffle=true)
begin for i in 1:300  Flux.train!(loss, ps, nndata, Flux.ADAM()) end end
ŷtrain     = Flux.onecold(Flux_nn(xtrainT),1:3)
ŷtest      = Flux.onecold(Flux_nn(xtestT),1:3)
(trainAccuracy,testAccuracy) = accuracy.([ŷtrain,ŷtest],[ytrain,ytest])</code></pre><pre class="documenter-example-output">2-element Vector{Float64}:
 0.9692307692307692
 0.7530864197530864</pre><p>While the train accuracy is the same as in BetaML, the test accuracy is somehow lower</p><p>However the time is again lower than BetaML, even if here for &quot;just&quot; a factor 2</p><pre><code class="language-julia">@btime begin for i in 1:300 Flux.train!(loss, ps, nndata, Flux.ADAM()) end end;</code></pre><pre class="documenter-example-output">  3.694 s (3367500 allocations: 1.53 GiB)</pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This is the summary of the results we had trying to predict the country of origin of the cars, based on their technical characteristics:</p><table><tr><th style="text-align: left">Model</th><th style="text-align: center">Train acc</th><th style="text-align: right">Test Acc</th><th style="text-align: right">Training time (ms)*</th><th style="text-align: right">Training mem (MB)</th></tr><tr><td style="text-align: left">RF</td><td style="text-align: center">0.9969</td><td style="text-align: right">0.8025</td><td style="text-align: right">133</td><td style="text-align: right">196</td></tr><tr><td style="text-align: left">RF (DecisionTree.jl)</td><td style="text-align: center">0.9969</td><td style="text-align: right">0.7531</td><td style="text-align: right">1.4</td><td style="text-align: right">1.5</td></tr><tr><td style="text-align: left">NN</td><td style="text-align: center">0.9754</td><td style="text-align: right">0.8765</td><td style="text-align: right">10684</td><td style="text-align: right">22241</td></tr><tr><td style="text-align: left">NN (Flux.jl)</td><td style="text-align: center">0.9692</td><td style="text-align: right">0.7284</td><td style="text-align: right">9164</td><td style="text-align: right">1577</td></tr></table><ul><li>on a Intel Core i5-8350U laptop</li></ul><p>We find a similar situation as in the bike&#39;s demand <a href="tutorials/Classification - cars/@ref">regression tutorial</a>: neural networks can be more precise than random forests models, but are more computationally expensive (and tricky to set up). When we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less verstatile).</p><p><a href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Classification - cars/betaml_tutorial_classification_cars.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">« A regression task: the prediction of  bike  sharing demand</a><a class="docs-footer-nextpage" href="../Clusterisation - Iris/betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris tdataset) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 2 April 2021 22:48">Friday 2 April 2021</span>. Using Julia version 1.6.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
