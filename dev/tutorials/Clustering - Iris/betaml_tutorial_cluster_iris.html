<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A classification task: the prediction of  plant species from floreal measures (the iris dataset) · BetaML.jl Documentation</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">BetaML.jl Documentation</span></div><form class="docs-search" action="../../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox" checked/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris dataset)</a><ul class="internal"><li><a class="tocitem" href="#Library-and-data-loading"><span>Library and data loading</span></a></li><li><a class="tocitem" href="#Data-preparation"><span>Data preparation</span></a></li><li><a class="tocitem" href="#Main-analysis"><span>Main analysis</span></a></li><li><a class="tocitem" href="#Working-without-the-labels"><span>Working without the labels</span></a></li><li><a class="tocitem" href="#Benchmarking-computational-efficiency"><span>Benchmarking computational efficiency</span></a></li><li><a class="tocitem" href="#Conclusions"><span>Conclusions</span></a></li></ul></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li><a class="is-disabled">Clustering - Iris</a></li><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="clustering_tutorial"><a class="docs-heading-anchor" href="#clustering_tutorial">A classification task: the prediction of  plant species from floreal measures (the iris dataset)</a><a id="clustering_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#clustering_tutorial" title="Permalink"></a></h1><p>The task is to estimate the species of a plant given some floreal measurements. It use the classical &quot;Iris&quot; dataset. Note that in this example we are using clustering approaches, so we try to understand the &quot;structure&quot; of our data, without relying to actually knowing the true labels (&quot;classes&quot; or &quot;factors&quot;). However we have chosen a dataset for which the true labels are actually known, so to compare the accuracy of the algorithms we use, but these labels will not be used during the algorithms training.</p><p>Data origin:</p><ul><li>dataset description: <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris<em>flower</em>data_set</a></li><li>data source we use here: <a href="https://github.com/JuliaStats/RDatasets.jl">https://github.com/JuliaStats/RDatasets.jl</a></li></ul><h2 id="Library-and-data-loading"><a class="docs-heading-anchor" href="#Library-and-data-loading">Library and data loading</a><a id="Library-and-data-loading-1"></a><a class="docs-heading-anchor-permalink" href="#Library-and-data-loading" title="Permalink"></a></h2><p>We load the Beta Machine Learning Toolkit as well as some other packages that we use in this tutorial</p><pre><code class="language-julia">using BetaML
using Random, Statistics, Logging, BenchmarkTools, RDatasets, Plots, DataFrames</code></pre><p>We are also going to compare our results with two other leading packages in Julia for clustering analysis, <a href="https://github.com/JuliaStats/Clustering.jl"><code>Clustering.jl</code></a> that provides (inter alia) kmeans and kmedoids algorithms and <a href="https://github.com/davidavdav/GaussianMixtures.jl"><code>GaussianMixtures.jl</code></a> that provides, as the name says, Gaussian Mixture Models. So we import them (we &quot;import&quot; them, rather than &quot;use&quot;, not to bound their full names into namespace as some would collide with BetaML).</p><pre><code class="language-julia">import Clustering, GaussianMixtures</code></pre><p>We do a few tweeks for the Clustering and GaussianMixtures packages. Note that in BetaML we can also control both the random seed and the verbosity in the algorithm call, not only globally</p><pre><code class="language-julia">Random.seed!(123)
#logger  = Logging.SimpleLogger(stdout, Logging.Error); global_logger(logger); ## For suppressing GaussianMixtures output</code></pre><p>Differently from the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>, we load the data here from [<code>RDatasets</code>](https://github.com/JuliaStats/RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl), a package providing standard datasets.</p><pre><code class="language-julia">iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
describe(iris)</code></pre><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Int64</th><th>DataType</th></tr></thead><tbody><p>5 rows × 7 columns</p><tr><th>1</th><td>SepalLength</td><td>5.84333</td><td>4.3</td><td>5.8</td><td>7.9</td><td>0</td><td>Float64</td></tr><tr><th>2</th><td>SepalWidth</td><td>3.05733</td><td>2.0</td><td>3.0</td><td>4.4</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>PetalLength</td><td>3.758</td><td>1.0</td><td>4.35</td><td>6.9</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>PetalWidth</td><td>1.19933</td><td>0.1</td><td>1.3</td><td>2.5</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>Species</td><td></td><td>setosa</td><td></td><td>virginica</td><td>0</td><td>CategoricalValue{String, UInt8}</td></tr></tbody></table><p>The iris dataset  provides floreal measures in columns 1 to 4 and the assigned species name in column 5. There are no missing values</p><h2 id="Data-preparation"><a class="docs-heading-anchor" href="#Data-preparation">Data preparation</a><a id="Data-preparation-1"></a><a class="docs-heading-anchor-permalink" href="#Data-preparation" title="Permalink"></a></h2><p>The first step is to prepare the data for the analysis. We collect the first 4 columns as our <em>feature</em> <code>x</code> matrix and the last one as our <code>y</code> label vector. As we are using clustering algorithms, we are not actually using the labels to train the algorithms, we&#39;ll behave like we do not know them, we&#39;ll just let the algorithm &quot;learn&quot; fro mthe structure of the data itself. We&#39;ll however use it to judge the accuracy that they did reach.</p><pre><code class="language-julia">x       = Matrix{Float64}(iris[:,1:4]);
yLabels = unique(iris[:,5]);</code></pre><p>As the labels are expressed as strings, the first thing we do is encode them as integers for our analysis using the function <a href="../../Utils.html#BetaML.Utils.integerEncoder-Tuple{AbstractVector{T} where T}"><code>integerEncoder</code></a>.</p><pre><code class="language-julia">y       = integerEncoder(iris[:,5],factors=yLabels);</code></pre><p>The dataset from RDatasets is ordered by species, so we need to shuffle it to avoid biases. Shuffling happens by default in crossValidation, but we are keeping here a copy of the shuffled version for later. Note that the version of <a href="../../Utils.html#Random.shuffle-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:AbstractArray"><code>shuffle</code></a> that is included in BetaML accepts several n-dimensional arrays and shuffle them (by default on rows, by we can specify the dimension) keeping the association  between the various arrays in the shuffled output.</p><pre><code class="language-julia">(xs,ys) = shuffle([x,y]);</code></pre><h2 id="Main-analysis"><a class="docs-heading-anchor" href="#Main-analysis">Main analysis</a><a id="Main-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Main-analysis" title="Permalink"></a></h2><p>We will try 3 BetaML models (<a href="../../Clustering.html#BetaML.Clustering.kmeans-Tuple{Any, Any}"><code>kmeans</code></a>, <a href="../../Clustering.html#BetaML.Clustering.kmedoids-Tuple{Any, Any}"><code>kmedoids</code></a> and <a href="../../Clustering.html#BetaML.Clustering.gmm-Tuple{Any, Any}"><code>gmm</code></a>) and we compare them with <code>kmeans</code> from Clusterings.jl and <code>GMM</code> from GaussianMixtures.jl <code>Kmeans</code> and <code>kmedoids</code> works by first initialising the centers of the k-clusters (the &quot;representative&quot; (step a ) . For <code>kmeans</code> they must be selected within one of the data, for kmeans they are the geometrical center) n a nutshell. Then ( b ) iterate for each point to assign the point to the cluster of the closest representative (according with a user defined distance metric, default to Euclidean), and ( c ) move each representative at the center of its newly acquired cluster (where &quot;center&quot; depends again from the metric). Steps ( b ) and ( c ) are reiterated until the algorithm converge, i.e. the tentative k representative points (and their relative clusters) don&#39;t move any more. The result (output of the algorithm) is that each point is assigned to one of the clusters (classes). The <code>gmm</code> algorithm is similar in that it employs an iterative approach (the Expectation<em>Minimisation algorithm, &quot;em&quot;) but here we make the hipothesis that the data points are the observed outcomes of some _mixture</em> probabilistic models where we have first a k-categorical variables whose outcomes are the (unobservble) parameters of a probabilistic distribution from which the data is finally drawn. Because the parameters of each of the k-possible distributions is unobservable this is also called a model with latent variables. Most <code>gmm</code> models use the Gaussain distribution as the family of the mixture components, so we can tought the <code>gmm</code> acronym to indicate <em>Gaussian Mixture Model</em>. In BetaML we do implemented only Gaussain components, but any distribution could be used by just subclassing <code>AbstractMixture</code> and implementing a couple of methids (you are invited to contribute or just ask for a distribution family you are interested), so I prefer to think &quot;gmm&quot; as an acronym for <em>Generative Mixture Model</em>. The algorithm try to find the mixture that maximises the likelihood that the data has been generated indeed from such mixture, where the &quot;E&quot; step refers to computing the probability that each point belongs to each of the k-composants (somehow similar to the step <em>b</em> in the kmeans/kmedoids algorithm), and the &quot;M&quot; step estimates, giving the association probabilities in step &quot;M&quot;, the parameters of the mixture and of the individual components (similar to step <em>c</em>). The result here is that each point has a categorical distribution (PMF) representing the probabilities that it belongs to any of the k-components (our classes or clusters). This is interesting, as <code>gmm</code> can be used for many other things that clustering. It forms the backbone of the <a href="../../Clustering.html#BetaML.Clustering.predictMissing"><code>predictMissing</code></a> function to impute missing values (on some or all dimensions) based to how close the record seems to its pears. For the same reasons, <code>predictMissing</code> can also be used to predict user&#39;s behaviours (or users&#39; appreciation) according to the behaviour/ranking made by pears (&quot;collaborative filtering&quot;). While the result of <code>gmm</code> is a vector of PMFs (one for each record), error measures and reports with the true values (if known) can be directly applied, as in BetaML they internally call <code>mode()</code> to retrieve the class with the highest probability for each record.</p><p>As we are here, we also try different versions of the BetaML models, even if the default &quot;versions&quot; should be fine. For <code>kmeans</code> and <code>kmedoids</code> we will try different initialisation strategies (&quot;gird&quot;, the default one, &quot;random&quot; and &quot;shuffle&quot;), while for the <code>gmm</code> model we&#39;ll choose different distributions of the Gaussain family (<code>SphericalGaussian</code> - where the variance is a scalar, <code>DiagonalGaussian</code> - with a vector variance, and <code>FullGaussian</code>, where the covariance is a matrix).</p><p>As the result would depend on stochasticity both in the data selected and in the random initialisation, we use a cross-validation approach to run our models several times (with different data) and then we average their results. Cross-Validation in BetaML is very flexible and it is done using the <a href="../../Utils.html#BetaML.Utils.crossValidation"><code>crossValidation</code></a> function. crossValidation works by calling the function <code>f</code>, defined by the user, passing to it the tuple <code>trainData</code>, <code>valData</code> and <code>rng</code> and collecting the result of the function f. The specific method for which <code>trainData</code>, and <code>valData</code> are selected at each iteration depends on the specific <code>sampler</code>. We start by selectign a k-fold sampler that split our data in 5 different parts, it uses 4 for training and 1 part (not used here) for validation. We run the simulations twice and, to be sure to have replicable results, we fix the random seed (at the whole crossValidaiton level, not on each iteration).</p><pre><code class="language-julia">sampler = KFold(nSplits=5,nRepeats=3,shuffle=true, rng=copy(FIXEDRNG))</code></pre><pre class="documenter-example-output">KFold(5, 3, true, StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7))</pre><p>We can now run the cross-validation with our models. Note that instead of defining the function <code>f</code> and then calling <code>crossValidation[f(trainData,testData,rng),[x,y],...)</code> we use the Julia <code>do</code> block syntax and we write directly the content of the <code>f</code> function in the <code>do</code> block. Also, by default crossValidation already returns the mean and the standard deviation of the output of the user-provided <code>f</code> function (or the <code>do</code> block). However this requires that the <code>f</code> function return a single scalar. Here we are returning a vector of the accuracies of the different models (so we can run the cross-validation only once), and hence we indicate with <code>returnStatistics=false</code> to crossValidation not to attempt to generate statistics but rather report the whole output. We&#39;ll compute the statistics ex-post.</p><p>Inside the <code>do</code> block we do 4 things:</p><ul><li>we recover from <code>trainData</code> (a tuple, as we passed a tuple to <code>crossValidation</code> too) the <code>xtrain</code> features and <code>ytrain</code> labels;</li><li>we run the various clustering algorithms</li><li>we use the real labels to compute the model accuracy. Note that the clustering algorithm know nothing about the specific label name or even their order. This is why <a href="../../Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T"><code>accuracy</code></a> has the parameter <code>ignoreLabels</code> to compute the accuracy oven any possible permutation of the classes found.</li><li>we return the various models&#39; accuracies</li></ul><pre><code class="language-julia">cOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng
          # For unsupervised learning we use only the train data.
          # Also, we use the associated labels only to measure the performances
         (xtrain,ytrain)  = trainData;
         # We run the clustering algorithm...
         clusteringOut     = kmeans(xtrain,3,rng=rng) ## init is grid by default
         # ... and we compute the accuracy using the real labels
         kMeansAccuracy    = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=&quot;random&quot;)
         kMeansRAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=&quot;shuffle&quot;)
         kMeansSAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmedoids(xtrain,3,rng=rng)   ## init is grid by default
         kMedoidsAccuracy  = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=&quot;random&quot;)
         kMedoidsRAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=&quot;shuffle&quot;)
         kMedoidsSAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = gmm(xtrain,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE, rng=rng)
         gmmSpherAccuracy  = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)
         clusteringOut     = gmm(xtrain,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE, rng=rng)
         gmmDiagAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)
         clusteringOut     = gmm(xtrain,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE, rng=rng)
         gmmFullAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)
         # For comparision with Clustering.jl
         clusteringOut     = Clustering.kmeans(xtrain&#39;, 3)
         kMeans2Accuracy   = accuracy(clusteringOut.assignments,ytrain,ignoreLabels=true)
         # For comparision with GaussianMistures.jl - sometimes GaussianMistures.jl em! fails with a PosDefException
         dGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:diag)
         GaussianMixtures.em!(dGMM, xtrain)
         gmmDiag2Accuracy  = accuracy(GaussianMixtures.gmmposterior(dGMM, xtrain)[1],ytrain,ignoreLabels=true)
         fGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:full)
         GaussianMixtures.em!(fGMM, xtrain)
         gmmFull2Accuracy  = accuracy(GaussianMixtures.gmmposterior(fGMM, xtrain)[1],ytrain,ignoreLabels=true)
         # Returning the accuracies
         return kMeansAccuracy,kMeansRAccuracy,kMeansSAccuracy,kMedoidsAccuracy,kMedoidsRAccuracy,kMedoidsSAccuracy,gmmSpherAccuracy,gmmDiagAccuracy,gmmFullAccuracy,kMeans2Accuracy,gmmDiag2Accuracy,gmmFull2Accuracy
 end

# We transform the output in matrix for easier analysis
accuracies = fill(0.0,(length(cOut),length(cOut[1])))
[accuracies[r,c] = cOut[r][c] for r in 1:length(cOut),c in 1:length(cOut[1])]
μs = mean(accuracies,dims=1)
σs = std(accuracies,dims=1)


modelLabels=[&quot;kMeansG&quot;,&quot;kMeansR&quot;,&quot;kMeansS&quot;,&quot;kMedoidsG&quot;,&quot;kMedoidsR&quot;,&quot;kMedoidsS&quot;,&quot;gmmSpher&quot;,&quot;gmmDiag&quot;,&quot;gmmFull&quot;,&quot;kMeans (Clustering.jl)&quot;,&quot;gmmDiag (GaussianMixtures.jl)&quot;,&quot;gmmFull (GaussianMixtures.jl)&quot;]
report = DataFrame(mName = modelLabels, avgAccuracy = dropdims(round.(μs&#39;,digits=3),dims=2), stdAccuracy = dropdims(round.(σs&#39;,digits=3),dims=2))</code></pre><table class="data-frame"><thead><tr><th></th><th>mName</th><th>avgAccuracy</th><th>stdAccuracy</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>12 rows × 3 columns</p><tr><th>1</th><td>kMeansG</td><td>0.891</td><td>0.017</td></tr><tr><th>2</th><td>kMeansR</td><td>0.866</td><td>0.083</td></tr><tr><th>3</th><td>kMeansS</td><td>0.764</td><td>0.174</td></tr><tr><th>4</th><td>kMedoidsG</td><td>0.894</td><td>0.015</td></tr><tr><th>5</th><td>kMedoidsR</td><td>0.804</td><td>0.144</td></tr><tr><th>6</th><td>kMedoidsS</td><td>0.893</td><td>0.018</td></tr><tr><th>7</th><td>gmmSpher</td><td>0.893</td><td>0.016</td></tr><tr><th>8</th><td>gmmDiag</td><td>0.917</td><td>0.022</td></tr><tr><th>9</th><td>gmmFull</td><td>0.97</td><td>0.035</td></tr><tr><th>10</th><td>kMeans (Clustering.jl)</td><td>0.858</td><td>0.11</td></tr><tr><th>11</th><td>gmmDiag (GaussianMixtures.jl)</td><td>0.882</td><td>0.096</td></tr><tr><th>12</th><td>gmmFull (GaussianMixtures.jl)</td><td>0.942</td><td>0.079</td></tr></tbody></table><h3 id="BetaML-model-accuracies"><a class="docs-heading-anchor" href="#BetaML-model-accuracies">BetaML model accuracies</a><a id="BetaML-model-accuracies-1"></a><a class="docs-heading-anchor-permalink" href="#BetaML-model-accuracies" title="Permalink"></a></h3><p>From the output We see that the gmm models perform for this dataset generally better than kmeans or kmedoids algorithms, also with very low variances. In detail, it is the (default) <code>grid</code> initialisation that leads to the better results for <code>kmeans</code> and <code>kmedoids</code>, while for the <code>gmm</code> models it is the <code>FullGaussian</code> to perform better.</p><h3 id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl"><a class="docs-heading-anchor" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl">Comparisions with <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code></a><a id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl" title="Permalink"></a></h3><p>For this specific case, both <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code> report substantially worst accuracies, and with very high variances. But we maintain the ranking that Full Gaussian gmm &gt; Diagonal Gaussian &gt; Kmeans accuracy. I suspect the reason that BetaML gmm works so weel is in relation to the usage of kmeans algorithm with itself the grid initialisation. The grid initialisation &quot;guarantee&quot; indeed that the initial means of the mixture components are well spread across the multidimensional space defined by the data, and it helps avoiding the EM algoritm to converge to a bad local optimus.</p><h2 id="Working-without-the-labels"><a class="docs-heading-anchor" href="#Working-without-the-labels">Working without the labels</a><a id="Working-without-the-labels-1"></a><a class="docs-heading-anchor-permalink" href="#Working-without-the-labels" title="Permalink"></a></h2><p>Up to now we used the real labels to compare the model accuracies. But in real clustering examples we don&#39;t have the true classes, or we wouln&#39;t need to do clustering in the first instance, so we don&#39;t know the number of classes to use. There are several methods to judge clusters algorithms goodness, perhaps the simplest one, at least for the expectation-maximisation algorithm employed in <code>gmm</code> to fit the data to the unknown mixture, is to use a information criteria that trade the goodness of the lickelyhood with the parameters used to do the fit. BetaML provide by default in the gmm clustering outputs both the <em>Bayesian information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.bic-Tuple{Any, Any, Any}"><code>BIC</code></a>) and the <em>Akaike information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.aic-Tuple{Any, Any}"><code>AIC</code></a>), where for both a lower value is better.</p><p>We can then run the model with different number of classes and see which one leads to the lower BIC or AIC. We run hence <code>crossValidation</code> again with the <code>FullGaussian</code> gmm model Note that we use the BIC/AIC criteria here for establishing the &quot;best&quot; number of classes but we could have used it also to select the kind of Gaussain distribution to use. This is one example of hyper-parameter tuning that we developed more in detail (but without using cross-validation) in the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>.</p><p>Let&#39;s try up to 8 possible classes:</p><pre><code class="language-julia">K = 8
sampler = KFold(nSplits=5,nRepeats=2,shuffle=true, rng=copy(FIXEDRNG))
cOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng
    (xtrain,ytrain)  = trainData;
    clusteringOut  = [gmm(xtrain,k,mixtures=[FullGaussian() for i in 1:k], verbosity=NONE, rng=rng) for k in 1:K]
    BICS           = [clusteringOut[i].BIC for i in 1:K]
    AICS           = [clusteringOut[i].AIC for i in 1:K]
    return (BICS,AICS)
end

# Transforming the output in matrices for easier analysis
Nit = length(cOut)

BICS = fill(0.0,(Nit,K))
AICS = fill(0.0,(Nit,K))
[BICS[r,c] = cOut[r][1][c] for r in 1:Nit,c in 1:K]
[AICS[r,c] = cOut[r][2][c] for r in 1:Nit,c in 1:K]

μsBICS = mean(BICS,dims=1)</code></pre><pre class="documenter-example-output">1×8 Matrix{Float64}:
 762.112  516.031  539.392  593.272  640.82  702.342  766.786  819.531</pre><pre><code class="language-julia">σsBICS = std(BICS,dims=1)</code></pre><pre class="documenter-example-output">1×8 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026  11.3803  21.0664  13.3093  25.2232</pre><pre><code class="language-julia">μsAICS = mean(AICS,dims=1)</code></pre><pre class="documenter-example-output">1×8 Matrix{Float64}:
 723.087  435.194  416.743  428.81  434.546  454.255  476.887  487.82</pre><pre><code class="language-julia">σsAICS = std(AICS,dims=1)</code></pre><pre class="documenter-example-output">1×8 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026  11.3803  21.0664  13.3093  25.2232</pre><pre><code class="language-julia">plot(1:K,[μsBICS&#39; μsAICS&#39;], labels=[&quot;BIC&quot; &quot;AIC&quot;], title=&quot;Information criteria by number of classes&quot;, xlabel=&quot;number of classes&quot;, ylabel=&quot;lower is better&quot;)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="150" height="100" viewBox="0 0 600 400">
<defs>
  <clipPath id="clip650">
    <rect x="0" y="0" width="600" height="400"/>
  </clipPath>
</defs>
<path clip-path="url(#clip650)" d="
M0 400 L600 400 L600 0 L0 0  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip651">
    <rect x="120" y="0" width="421" height="400"/>
  </clipPath>
</defs>
<path clip-path="url(#clip650)" d="
M25.685 376.315 L588.189 376.315 L588.189 15.748 L25.685 15.748  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip652">
    <rect x="25" y="15" width="564" height="362"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip652)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  117.414,376.315 117.414,15.748 
  "/>
<polyline clip-path="url(#clip652)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  269.032,376.315 269.032,15.748 
  "/>
<polyline clip-path="url(#clip652)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  420.651,376.315 420.651,15.748 
  "/>
<polyline clip-path="url(#clip652)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  572.269,376.315 572.269,15.748 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  25.685,376.315 588.189,376.315 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  117.414,376.315 117.414,371.988 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  269.032,376.315 269.032,371.988 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  420.651,376.315 420.651,371.988 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  572.269,376.315 572.269,371.988 
  "/>
<path clip-path="url(#clip650)" d="M 0 0 M116.077 390.499 L120.157 390.499 L120.157 391.483 L114.671 391.483 L114.671 390.499 Q115.337 389.811 116.482 388.653 Q117.634 387.49 117.929 387.154 Q118.491 386.523 118.71 386.089 Q118.936 385.65 118.936 385.227 Q118.936 384.539 118.45 384.105 Q117.97 383.671 117.194 383.671 Q116.644 383.671 116.031 383.861 Q115.423 384.052 114.729 384.44 L114.729 383.26 Q115.435 382.976 116.048 382.831 Q116.662 382.687 117.171 382.687 Q118.514 382.687 119.312 383.358 Q120.111 384.029 120.111 385.152 Q120.111 385.684 119.908 386.165 Q119.712 386.639 119.185 387.287 Q119.04 387.455 118.265 388.26 Q117.489 389.058 116.077 390.499 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M269.785 383.861 L266.833 388.474 L269.785 388.474 L269.785 383.861 M269.478 382.843 L270.948 382.843 L270.948 388.474 L272.181 388.474 L272.181 389.446 L270.948 389.446 L270.948 391.483 L269.785 391.483 L269.785 389.446 L265.884 389.446 L265.884 388.317 L269.478 382.843 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M420.752 386.697 Q419.965 386.697 419.502 387.235 Q419.045 387.773 419.045 388.711 Q419.045 389.643 419.502 390.187 Q419.965 390.725 420.752 390.725 Q421.539 390.725 421.996 390.187 Q422.459 389.643 422.459 388.711 Q422.459 387.773 421.996 387.235 Q421.539 386.697 420.752 386.697 M423.073 383.034 L423.073 384.099 Q422.633 383.89 422.181 383.78 Q421.736 383.671 421.296 383.671 Q420.139 383.671 419.525 384.452 Q418.918 385.233 418.831 386.813 Q419.172 386.309 419.687 386.043 Q420.202 385.771 420.821 385.771 Q422.124 385.771 422.876 386.564 Q423.634 387.351 423.634 388.711 Q423.634 390.042 422.847 390.846 Q422.06 391.651 420.752 391.651 Q419.253 391.651 418.46 390.505 Q417.668 389.353 417.668 387.172 Q417.668 385.123 418.64 383.908 Q419.612 382.687 421.25 382.687 Q421.69 382.687 422.135 382.774 Q422.586 382.86 423.073 383.034 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M572.269 387.38 Q571.436 387.38 570.955 387.826 Q570.481 388.271 570.481 389.052 Q570.481 389.834 570.955 390.279 Q571.436 390.725 572.269 390.725 Q573.102 390.725 573.583 390.279 Q574.063 389.828 574.063 389.052 Q574.063 388.271 573.583 387.826 Q573.108 387.38 572.269 387.38 M571.1 386.882 Q570.348 386.697 569.925 386.182 Q569.509 385.667 569.509 384.926 Q569.509 383.89 570.244 383.289 Q570.984 382.687 572.269 382.687 Q573.56 382.687 574.295 383.289 Q575.029 383.89 575.029 384.926 Q575.029 385.667 574.607 386.182 Q574.19 386.697 573.444 386.882 Q574.289 387.079 574.757 387.652 Q575.232 388.225 575.232 389.052 Q575.232 390.308 574.462 390.979 Q573.698 391.651 572.269 391.651 Q570.84 391.651 570.07 390.979 Q569.306 390.308 569.306 389.052 Q569.306 388.225 569.781 387.652 Q570.255 387.079 571.1 386.882 M570.672 385.036 Q570.672 385.708 571.089 386.084 Q571.511 386.46 572.269 386.46 Q573.021 386.46 573.444 386.084 Q573.872 385.708 573.872 385.036 Q573.872 384.365 573.444 383.989 Q573.021 383.613 572.269 383.613 Q571.511 383.613 571.089 383.989 Q570.672 384.365 570.672 385.036 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M247.752 407.152 L247.752 412.531 L246.288 412.531 L246.288 407.2 Q246.288 405.934 245.794 405.306 Q245.301 404.677 244.314 404.677 Q243.129 404.677 242.444 405.433 Q241.76 406.189 241.76 407.494 L241.76 412.531 L240.288 412.531 L240.288 403.619 L241.76 403.619 L241.76 405.004 Q242.285 404.2 242.993 403.802 Q243.71 403.404 244.641 403.404 Q246.176 403.404 246.964 404.359 Q247.752 405.306 247.752 407.152 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M249.136 409.014 L249.136 403.619 L250.601 403.619 L250.601 408.958 Q250.601 410.223 251.094 410.86 Q251.587 411.489 252.574 411.489 Q253.759 411.489 254.444 410.733 Q255.136 409.977 255.136 408.672 L255.136 403.619 L256.6 403.619 L256.6 412.531 L255.136 412.531 L255.136 411.162 Q254.603 411.974 253.895 412.372 Q253.195 412.762 252.264 412.762 Q250.728 412.762 249.932 411.807 Q249.136 410.852 249.136 409.014 M252.821 403.404 L252.821 403.404 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M265.075 405.33 Q265.624 404.343 266.387 403.874 Q267.151 403.404 268.186 403.404 Q269.578 403.404 270.334 404.383 Q271.09 405.354 271.09 407.152 L271.09 412.531 L269.618 412.531 L269.618 407.2 Q269.618 405.919 269.164 405.298 Q268.711 404.677 267.78 404.677 Q266.642 404.677 265.982 405.433 Q265.321 406.189 265.321 407.494 L265.321 412.531 L263.849 412.531 L263.849 407.2 Q263.849 405.911 263.396 405.298 Q262.942 404.677 261.995 404.677 Q260.873 404.677 260.213 405.441 Q259.552 406.197 259.552 407.494 L259.552 412.531 L258.08 412.531 L258.08 403.619 L259.552 403.619 L259.552 405.004 Q260.054 404.184 260.754 403.794 Q261.454 403.404 262.417 403.404 Q263.388 403.404 264.064 403.897 Q264.748 404.391 265.075 405.33 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M279.023 408.083 Q279.023 406.468 278.355 405.553 Q277.695 404.63 276.533 404.63 Q275.371 404.63 274.703 405.553 Q274.042 406.468 274.042 408.083 Q274.042 409.698 274.703 410.621 Q275.371 411.536 276.533 411.536 Q277.695 411.536 278.355 410.621 Q279.023 409.698 279.023 408.083 M274.042 404.972 Q274.504 404.176 275.204 403.794 Q275.912 403.404 276.891 403.404 Q278.514 403.404 279.525 404.693 Q280.543 405.982 280.543 408.083 Q280.543 410.184 279.525 411.473 Q278.514 412.762 276.891 412.762 Q275.912 412.762 275.204 412.38 Q274.504 411.99 274.042 411.194 L274.042 412.531 L272.57 412.531 L272.57 400.15 L274.042 400.15 L274.042 404.972 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M289.702 407.709 L289.702 408.425 L282.97 408.425 Q283.066 409.937 283.877 410.733 Q284.697 411.52 286.153 411.52 Q286.996 411.52 287.784 411.314 Q288.58 411.107 289.36 410.693 L289.36 412.077 Q288.572 412.412 287.744 412.587 Q286.917 412.762 286.065 412.762 Q283.933 412.762 282.684 411.52 Q281.442 410.279 281.442 408.162 Q281.442 405.974 282.62 404.693 Q283.806 403.404 285.811 403.404 Q287.609 403.404 288.652 404.566 Q289.702 405.72 289.702 407.709 M288.238 407.279 Q288.222 406.078 287.561 405.362 Q286.909 404.645 285.827 404.645 Q284.601 404.645 283.861 405.338 Q283.129 406.03 283.018 407.287 L288.238 407.279 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M296.402 404.988 Q296.155 404.844 295.861 404.781 Q295.574 404.709 295.224 404.709 Q293.983 404.709 293.314 405.521 Q292.654 406.324 292.654 407.836 L292.654 412.531 L291.182 412.531 L291.182 403.619 L292.654 403.619 L292.654 405.004 Q293.115 404.192 293.855 403.802 Q294.595 403.404 295.654 403.404 Q295.805 403.404 295.988 403.428 Q296.171 403.444 296.394 403.484 L296.402 404.988 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M306.571 404.645 Q305.393 404.645 304.709 405.568 Q304.025 406.484 304.025 408.083 Q304.025 409.682 304.701 410.605 Q305.385 411.52 306.571 411.52 Q307.741 411.52 308.425 410.597 Q309.109 409.674 309.109 408.083 Q309.109 406.499 308.425 405.576 Q307.741 404.645 306.571 404.645 M306.571 403.404 Q308.481 403.404 309.571 404.645 Q310.661 405.887 310.661 408.083 Q310.661 410.271 309.571 411.52 Q308.481 412.762 306.571 412.762 Q304.653 412.762 303.563 411.52 Q302.481 410.271 302.481 408.083 Q302.481 405.887 303.563 404.645 Q304.653 403.404 306.571 403.404 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M316.708 400.15 L316.708 401.367 L315.308 401.367 Q314.52 401.367 314.21 401.685 Q313.907 402.004 313.907 402.831 L313.907 403.619 L316.318 403.619 L316.318 404.757 L313.907 404.757 L313.907 412.531 L312.435 412.531 L312.435 404.757 L311.035 404.757 L311.035 403.619 L312.435 403.619 L312.435 402.998 Q312.435 401.51 313.128 400.834 Q313.82 400.15 315.324 400.15 L316.708 400.15 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M329.838 403.961 L329.838 405.33 Q329.217 404.988 328.588 404.82 Q327.968 404.645 327.331 404.645 Q325.907 404.645 325.119 405.553 Q324.331 406.452 324.331 408.083 Q324.331 409.714 325.119 410.621 Q325.907 411.52 327.331 411.52 Q327.968 411.52 328.588 411.353 Q329.217 411.178 329.838 410.836 L329.838 412.189 Q329.225 412.475 328.564 412.618 Q327.912 412.762 327.172 412.762 Q325.159 412.762 323.973 411.497 Q322.788 410.231 322.788 408.083 Q322.788 405.903 323.981 404.653 Q325.183 403.404 327.267 403.404 Q327.944 403.404 328.588 403.547 Q329.233 403.683 329.838 403.961 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M331.373 400.15 L332.837 400.15 L332.837 412.531 L331.373 412.531 L331.373 400.15 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M338.423 408.051 Q336.649 408.051 335.965 408.457 Q335.28 408.863 335.28 409.841 Q335.28 410.621 335.79 411.083 Q336.307 411.536 337.19 411.536 Q338.407 411.536 339.14 410.677 Q339.88 409.81 339.88 408.377 L339.88 408.051 L338.423 408.051 M341.344 407.446 L341.344 412.531 L339.88 412.531 L339.88 411.178 Q339.378 411.99 338.63 412.38 Q337.882 412.762 336.8 412.762 Q335.432 412.762 334.62 411.998 Q333.816 411.226 333.816 409.937 Q333.816 408.433 334.819 407.669 Q335.829 406.905 337.827 406.905 L339.88 406.905 L339.88 406.762 Q339.88 405.751 339.211 405.202 Q338.551 404.645 337.349 404.645 Q336.585 404.645 335.861 404.828 Q335.137 405.011 334.469 405.377 L334.469 404.025 Q335.272 403.714 336.028 403.563 Q336.784 403.404 337.5 403.404 Q339.434 403.404 340.389 404.407 Q341.344 405.409 341.344 407.446 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M348.561 403.882 L348.561 405.266 Q347.94 404.948 347.272 404.789 Q346.603 404.63 345.887 404.63 Q344.797 404.63 344.248 404.964 Q343.707 405.298 343.707 405.966 Q343.707 406.476 344.097 406.77 Q344.487 407.056 345.664 407.319 L346.166 407.43 Q347.725 407.765 348.378 408.377 Q349.038 408.982 349.038 410.072 Q349.038 411.314 348.052 412.038 Q347.073 412.762 345.354 412.762 Q344.638 412.762 343.858 412.618 Q343.086 412.483 342.227 412.205 L342.227 410.693 Q343.039 411.115 343.826 411.329 Q344.614 411.536 345.386 411.536 Q346.42 411.536 346.977 411.186 Q347.534 410.828 347.534 410.184 Q347.534 409.587 347.128 409.269 Q346.731 408.95 345.37 408.656 L344.861 408.536 Q343.5 408.25 342.895 407.661 Q342.291 407.064 342.291 406.03 Q342.291 404.773 343.182 404.088 Q344.073 403.404 345.712 403.404 Q346.524 403.404 347.24 403.523 Q347.956 403.643 348.561 403.882 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M356.255 403.882 L356.255 405.266 Q355.635 404.948 354.966 404.789 Q354.298 404.63 353.582 404.63 Q352.492 404.63 351.943 404.964 Q351.401 405.298 351.401 405.966 Q351.401 406.476 351.791 406.77 Q352.181 407.056 353.359 407.319 L353.86 407.43 Q355.42 407.765 356.072 408.377 Q356.733 408.982 356.733 410.072 Q356.733 411.314 355.746 412.038 Q354.767 412.762 353.049 412.762 Q352.332 412.762 351.553 412.618 Q350.781 412.483 349.921 412.205 L349.921 410.693 Q350.733 411.115 351.521 411.329 Q352.309 411.536 353.08 411.536 Q354.115 411.536 354.672 411.186 Q355.229 410.828 355.229 410.184 Q355.229 409.587 354.823 409.269 Q354.425 408.95 353.065 408.656 L352.555 408.536 Q351.195 408.25 350.59 407.661 Q349.985 407.064 349.985 406.03 Q349.985 404.773 350.876 404.088 Q351.767 403.404 353.407 403.404 Q354.218 403.404 354.934 403.523 Q355.651 403.643 356.255 403.882 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M365.891 407.709 L365.891 408.425 L359.16 408.425 Q359.255 409.937 360.067 410.733 Q360.886 411.52 362.343 411.52 Q363.186 411.52 363.974 411.314 Q364.769 411.107 365.549 410.693 L365.549 412.077 Q364.761 412.412 363.934 412.587 Q363.106 412.762 362.255 412.762 Q360.122 412.762 358.873 411.52 Q357.632 410.279 357.632 408.162 Q357.632 405.974 358.81 404.693 Q359.995 403.404 362 403.404 Q363.799 403.404 364.841 404.566 Q365.891 405.72 365.891 407.709 M364.427 407.279 Q364.411 406.078 363.751 405.362 Q363.098 404.645 362.016 404.645 Q360.791 404.645 360.051 405.338 Q359.319 406.03 359.207 407.287 L364.427 407.279 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M373.109 403.882 L373.109 405.266 Q372.488 404.948 371.819 404.789 Q371.151 404.63 370.435 404.63 Q369.345 404.63 368.796 404.964 Q368.255 405.298 368.255 405.966 Q368.255 406.476 368.645 406.77 Q369.034 407.056 370.212 407.319 L370.713 407.43 Q372.273 407.765 372.926 408.377 Q373.586 408.982 373.586 410.072 Q373.586 411.314 372.599 412.038 Q371.621 412.762 369.902 412.762 Q369.186 412.762 368.406 412.618 Q367.634 412.483 366.775 412.205 L366.775 410.693 Q367.586 411.115 368.374 411.329 Q369.162 411.536 369.934 411.536 Q370.968 411.536 371.525 411.186 Q372.082 410.828 372.082 410.184 Q372.082 409.587 371.676 409.269 Q371.278 408.95 369.918 408.656 L369.408 408.536 Q368.048 408.25 367.443 407.661 Q366.838 407.064 366.838 406.03 Q366.838 404.773 367.73 404.088 Q368.621 403.404 370.26 403.404 Q371.072 403.404 371.788 403.523 Q372.504 403.643 373.109 403.882 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip652)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  25.685,295.799 588.189,295.799 
  "/>
<polyline clip-path="url(#clip652)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  25.685,211.348 588.189,211.348 
  "/>
<polyline clip-path="url(#clip652)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  25.685,126.898 588.189,126.898 
  "/>
<polyline clip-path="url(#clip652)" style="stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none" points="
  25.685,42.4472 588.189,42.4472 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  25.685,376.315 25.685,15.748 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  25.685,295.799 32.4351,295.799 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  25.685,211.348 32.4351,211.348 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  25.685,126.898 32.4351,126.898 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  25.685,42.4472 32.4351,42.4472 
  "/>
<path clip-path="url(#clip650)" d="M 0 0 M-2.0475 291.479 L2.5416 291.479 L2.5416 292.463 L-0.976903 292.463 L-0.976903 294.581 Q-0.722275 294.494 -0.467646 294.453 Q-0.213018 294.407 0.04161 294.407 Q1.48836 294.407 2.33326 295.2 Q3.17817 295.993 3.17817 297.347 Q3.17817 298.742 2.31012 299.517 Q1.44207 300.287 -0.137787 300.287 Q-0.681766 300.287 -1.24889 300.194 Q-1.81023 300.102 -2.41208 299.916 L-2.41208 298.742 Q-1.89125 299.025 -1.3357 299.164 Q-0.780145 299.303 -0.160935 299.303 Q0.840217 299.303 1.4247 298.776 Q2.00919 298.25 2.00919 297.347 Q2.00919 296.444 1.4247 295.918 Q0.840217 295.391 -0.160935 295.391 Q-0.629683 295.391 -1.09843 295.495 Q-1.56139 295.599 -2.0475 295.819 L-2.0475 291.479 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M6.94551 292.249 Q6.04274 292.249 5.58556 293.14 Q5.13418 294.025 5.13418 295.808 Q5.13418 297.584 5.58556 298.475 Q6.04274 299.361 6.94551 299.361 Q7.85407 299.361 8.30545 298.475 Q8.76263 297.584 8.76263 295.808 Q8.76263 294.025 8.30545 293.14 Q7.85407 292.249 6.94551 292.249 M6.94551 291.323 Q8.39805 291.323 9.16193 292.474 Q9.9316 293.62 9.9316 295.808 Q9.9316 297.989 9.16193 299.141 Q8.39805 300.287 6.94551 300.287 Q5.49297 300.287 4.7233 299.141 Q3.95941 297.989 3.95941 295.808 Q3.95941 293.62 4.7233 292.474 Q5.49297 291.323 6.94551 291.323 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M13.6989 292.249 Q12.7962 292.249 12.339 293.14 Q11.8876 294.025 11.8876 295.808 Q11.8876 297.584 12.339 298.475 Q12.7962 299.361 13.6989 299.361 Q14.6075 299.361 15.0589 298.475 Q15.5161 297.584 15.5161 295.808 Q15.5161 294.025 15.0589 293.14 Q14.6075 292.249 13.6989 292.249 M13.6989 291.323 Q15.1515 291.323 15.9154 292.474 Q16.685 293.62 16.685 295.808 Q16.685 297.989 15.9154 299.141 Q15.1515 300.287 13.6989 300.287 Q12.2464 300.287 11.4767 299.141 Q10.7128 297.989 10.7128 295.808 Q10.7128 293.62 11.4767 292.474 Q12.2464 291.323 13.6989 291.323 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M0.296238 210.883 Q-0.490795 210.883 -0.953755 211.421 Q-1.41093 211.959 -1.41093 212.896 Q-1.41093 213.828 -0.953755 214.372 Q-0.490795 214.91 0.296238 214.91 Q1.08327 214.91 1.54044 214.372 Q2.00341 213.828 2.00341 212.896 Q2.00341 211.959 1.54044 211.421 Q1.08327 210.883 0.296238 210.883 M2.61683 207.219 L2.61683 208.284 Q2.17702 208.076 1.72563 207.966 Q1.28003 207.856 0.840217 207.856 Q-0.317184 207.856 -0.930607 208.637 Q-1.53824 209.418 -1.62505 210.998 Q-1.28361 210.495 -0.768571 210.229 Q-0.253527 209.957 0.365682 209.957 Q1.66776 209.957 2.42007 210.749 Q3.17817 211.536 3.17817 212.896 Q3.17817 214.227 2.39113 215.032 Q1.6041 215.836 0.296238 215.836 Q-1.2026 215.836 -1.99542 214.69 Q-2.78824 213.539 -2.78824 211.357 Q-2.78824 209.308 -1.81602 208.093 Q-0.843802 206.872 0.793921 206.872 Q1.23373 206.872 1.67933 206.959 Q2.13072 207.046 2.61683 207.219 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M6.94551 207.798 Q6.04274 207.798 5.58556 208.689 Q5.13418 209.575 5.13418 211.357 Q5.13418 213.134 5.58556 214.025 Q6.04274 214.91 6.94551 214.91 Q7.85407 214.91 8.30545 214.025 Q8.76263 213.134 8.76263 211.357 Q8.76263 209.575 8.30545 208.689 Q7.85407 207.798 6.94551 207.798 M6.94551 206.872 Q8.39805 206.872 9.16193 208.024 Q9.9316 209.17 9.9316 211.357 Q9.9316 213.539 9.16193 214.69 Q8.39805 215.836 6.94551 215.836 Q5.49297 215.836 4.7233 214.69 Q3.95941 213.539 3.95941 211.357 Q3.95941 209.17 4.7233 208.024 Q5.49297 206.872 6.94551 206.872 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M13.6989 207.798 Q12.7962 207.798 12.339 208.689 Q11.8876 209.575 11.8876 211.357 Q11.8876 213.134 12.339 214.025 Q12.7962 214.91 13.6989 214.91 Q14.6075 214.91 15.0589 214.025 Q15.5161 213.134 15.5161 211.357 Q15.5161 209.575 15.0589 208.689 Q14.6075 207.798 13.6989 207.798 M13.6989 206.872 Q15.1515 206.872 15.9154 208.024 Q16.685 209.17 16.685 211.357 Q16.685 213.539 15.9154 214.69 Q15.1515 215.836 13.6989 215.836 Q12.2464 215.836 11.4767 214.69 Q10.7128 213.539 10.7128 211.357 Q10.7128 209.17 11.4767 208.024 Q12.2464 206.872 13.6989 206.872 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-2.37736 122.578 L3.17817 122.578 L3.17817 123.075 L0.04161 131.218 L-1.17945 131.218 L1.77192 123.562 L-2.37736 123.562 L-2.37736 122.578 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M6.94551 123.347 Q6.04274 123.347 5.58556 124.239 Q5.13418 125.124 5.13418 126.906 Q5.13418 128.683 5.58556 129.574 Q6.04274 130.46 6.94551 130.46 Q7.85407 130.46 8.30545 129.574 Q8.76263 128.683 8.76263 126.906 Q8.76263 125.124 8.30545 124.239 Q7.85407 123.347 6.94551 123.347 M6.94551 122.422 Q8.39805 122.422 9.16193 123.573 Q9.9316 124.719 9.9316 126.906 Q9.9316 129.088 9.16193 130.24 Q8.39805 131.386 6.94551 131.386 Q5.49297 131.386 4.7233 130.24 Q3.95941 129.088 3.95941 126.906 Q3.95941 124.719 4.7233 123.573 Q5.49297 122.422 6.94551 122.422 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M13.6989 123.347 Q12.7962 123.347 12.339 124.239 Q11.8876 125.124 11.8876 126.906 Q11.8876 128.683 12.339 129.574 Q12.7962 130.46 13.6989 130.46 Q14.6075 130.46 15.0589 129.574 Q15.5161 128.683 15.5161 126.906 Q15.5161 125.124 15.0589 124.239 Q14.6075 123.347 13.6989 123.347 M13.6989 122.422 Q15.1515 122.422 15.9154 123.573 Q16.685 124.719 16.685 126.906 Q16.685 129.088 15.9154 130.24 Q15.1515 131.386 13.6989 131.386 Q12.2464 131.386 11.4767 130.24 Q10.7128 129.088 10.7128 126.906 Q10.7128 124.719 11.4767 123.573 Q12.2464 122.422 13.6989 122.422 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M0.21522 42.6642 Q-0.618109 42.6642 -1.09843 43.1098 Q-1.57296 43.5554 -1.57296 44.3367 Q-1.57296 45.1179 -1.09843 45.5635 Q-0.618109 46.0091 0.21522 46.0091 Q1.04855 46.0091 1.52887 45.5635 Q2.00919 45.1121 2.00919 44.3367 Q2.00919 43.5554 1.52887 43.1098 Q1.05434 42.6642 0.21522 42.6642 M-0.953755 42.1665 Q-1.70607 41.9813 -2.12852 41.4663 Q-2.54518 40.9513 -2.54518 40.2105 Q-2.54518 39.1746 -1.81023 38.5728 Q-1.0695 37.9709 0.21522 37.9709 Q1.50572 37.9709 2.24067 38.5728 Q2.97562 39.1746 2.97562 40.2105 Q2.97562 40.9513 2.55317 41.4663 Q2.13651 41.9813 1.38998 42.1665 Q2.23489 42.3633 2.70363 42.9362 Q3.17817 43.5091 3.17817 44.3367 Q3.17817 45.5924 2.4085 46.2637 Q1.64461 46.935 0.21522 46.935 Q-1.21417 46.935 -1.98384 46.2637 Q-2.74773 45.5924 -2.74773 44.3367 Q-2.74773 43.5091 -2.27319 42.9362 Q-1.79866 42.3633 -0.953755 42.1665 M-1.38199 40.3205 Q-1.38199 40.9918 -0.965329 41.3679 Q-0.542878 41.7441 0.21522 41.7441 Q0.967531 41.7441 1.38998 41.3679 Q1.81822 40.9918 1.81822 40.3205 Q1.81822 39.6492 1.38998 39.273 Q0.967531 38.8969 0.21522 38.8969 Q-0.542878 38.8969 -0.965329 39.273 Q-1.38199 39.6492 -1.38199 40.3205 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M6.94551 38.8969 Q6.04274 38.8969 5.58556 39.7881 Q5.13418 40.6735 5.13418 42.4559 Q5.13418 44.2325 5.58556 45.1237 Q6.04274 46.0091 6.94551 46.0091 Q7.85407 46.0091 8.30545 45.1237 Q8.76263 44.2325 8.76263 42.4559 Q8.76263 40.6735 8.30545 39.7881 Q7.85407 38.8969 6.94551 38.8969 M6.94551 37.9709 Q8.39805 37.9709 9.16193 39.1226 Q9.9316 40.2684 9.9316 42.4559 Q9.9316 44.6376 9.16193 45.7892 Q8.39805 46.935 6.94551 46.935 Q5.49297 46.935 4.7233 45.7892 Q3.95941 44.6376 3.95941 42.4559 Q3.95941 40.2684 4.7233 39.1226 Q5.49297 37.9709 6.94551 37.9709 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M13.6989 38.8969 Q12.7962 38.8969 12.339 39.7881 Q11.8876 40.6735 11.8876 42.4559 Q11.8876 44.2325 12.339 45.1237 Q12.7962 46.0091 13.6989 46.0091 Q14.6075 46.0091 15.0589 45.1237 Q15.5161 44.2325 15.5161 42.4559 Q15.5161 40.6735 15.0589 39.7881 Q14.6075 38.8969 13.6989 38.8969 M13.6989 37.9709 Q15.1515 37.9709 15.9154 39.1226 Q16.685 40.2684 16.685 42.4559 Q16.685 44.6376 15.9154 45.7892 Q15.1515 46.935 13.6989 46.935 Q12.2464 46.935 11.4767 45.7892 Q10.7128 44.6376 10.7128 42.4559 Q10.7128 40.2684 11.4767 39.1226 Q12.2464 37.9709 13.6989 37.9709 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-27.7335 249.316 L-27.7335 247.852 L-15.3522 247.852 L-15.3522 249.316 L-27.7335 249.316 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-23.2378 242.863 Q-23.2378 244.041 -22.3147 244.725 Q-21.3997 245.409 -19.8003 245.409 Q-18.2009 245.409 -17.2779 244.733 Q-16.3628 244.049 -16.3628 242.863 Q-16.3628 241.694 -17.2858 241.009 Q-18.2088 240.325 -19.8003 240.325 Q-21.3837 240.325 -22.3068 241.009 Q-23.2378 241.694 -23.2378 242.863 M-24.4791 242.863 Q-24.4791 240.953 -23.2378 239.863 Q-21.9964 238.773 -19.8003 238.773 Q-17.6121 238.773 -16.3628 239.863 Q-15.1215 240.953 -15.1215 242.863 Q-15.1215 244.781 -16.3628 245.871 Q-17.6121 246.953 -19.8003 246.953 Q-21.9964 246.953 -23.2378 245.871 Q-24.4791 244.781 -24.4791 242.863 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-24.2642 238.089 L-24.2642 236.625 L-17.3097 234.795 L-24.2642 232.972 L-24.2642 231.246 L-17.3097 229.416 L-24.2642 227.593 L-24.2642 226.129 L-15.3522 228.461 L-15.3522 230.187 L-22.6569 232.105 L-15.3522 234.031 L-15.3522 235.757 L-24.2642 238.089 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-20.1743 216.971 L-19.4581 216.971 L-19.4581 223.702 Q-17.9463 223.607 -17.1505 222.795 Q-16.3628 221.976 -16.3628 220.52 Q-16.3628 219.676 -16.5697 218.888 Q-16.7766 218.093 -17.1903 217.313 L-15.8058 217.313 Q-15.4716 218.101 -15.2965 218.928 Q-15.1215 219.756 -15.1215 220.607 Q-15.1215 222.74 -16.3628 223.989 Q-17.6041 225.23 -19.7207 225.23 Q-21.9089 225.23 -23.19 224.053 Q-24.4791 222.867 -24.4791 220.862 Q-24.4791 219.063 -23.3173 218.021 Q-22.1635 216.971 -20.1743 216.971 M-20.6039 218.435 Q-21.8055 218.451 -22.5216 219.111 Q-23.2378 219.764 -23.2378 220.846 Q-23.2378 222.071 -22.5455 222.811 Q-21.8532 223.543 -20.596 223.655 L-20.6039 218.435 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-22.8956 210.271 Q-23.0388 210.517 -23.1025 210.812 Q-23.1741 211.098 -23.1741 211.448 Q-23.1741 212.69 -22.3625 213.358 Q-21.5588 214.019 -20.0469 214.019 L-15.3522 214.019 L-15.3522 215.491 L-24.2642 215.491 L-24.2642 214.019 L-22.8797 214.019 Q-23.6913 213.557 -24.0812 212.817 Q-24.4791 212.077 -24.4791 211.019 Q-24.4791 210.868 -24.4552 210.685 Q-24.4393 210.502 -24.3995 210.279 L-22.8956 210.271 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-24.2642 203.555 L-24.2642 202.091 L-15.3522 202.091 L-15.3522 203.555 L-24.2642 203.555 M-27.7335 203.555 L-27.7335 202.091 L-25.8795 202.091 L-25.8795 203.555 L-27.7335 203.555 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-24.0016 194.874 L-22.6171 194.874 Q-22.9354 195.494 -23.0945 196.163 Q-23.2537 196.831 -23.2537 197.547 Q-23.2537 198.637 -22.9195 199.186 Q-22.5853 199.728 -21.9169 199.728 Q-21.4076 199.728 -21.1132 199.338 Q-20.8267 198.948 -20.5642 197.77 L-20.4528 197.269 Q-20.1186 195.709 -19.5059 195.057 Q-18.9011 194.396 -17.811 194.396 Q-16.5697 194.396 -15.8456 195.383 Q-15.1215 196.362 -15.1215 198.08 Q-15.1215 198.797 -15.2647 199.576 Q-15.4 200.348 -15.6785 201.208 L-17.1903 201.208 Q-16.7686 200.396 -16.5538 199.608 Q-16.3469 198.82 -16.3469 198.049 Q-16.3469 197.014 -16.697 196.457 Q-17.0551 195.9 -17.6996 195.9 Q-18.2964 195.9 -18.6147 196.306 Q-18.9329 196.704 -19.2274 198.065 L-19.3467 198.574 Q-19.6332 199.934 -20.222 200.539 Q-20.8188 201.144 -21.8532 201.144 Q-23.1104 201.144 -23.7948 200.253 Q-24.4791 199.362 -24.4791 197.722 Q-24.4791 196.911 -24.3597 196.195 Q-24.2404 195.478 -24.0016 194.874 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-19.8003 181.283 Q-21.4156 181.283 -22.3306 181.951 Q-23.2537 182.612 -23.2537 183.774 Q-23.2537 184.935 -22.3306 185.604 Q-21.4156 186.264 -19.8003 186.264 Q-18.185 186.264 -17.2619 185.604 Q-16.3469 184.935 -16.3469 183.774 Q-16.3469 182.612 -17.2619 181.951 Q-18.185 181.283 -19.8003 181.283 M-22.9115 186.264 Q-23.7072 185.803 -24.0892 185.102 Q-24.4791 184.394 -24.4791 183.415 Q-24.4791 181.792 -23.19 180.782 Q-21.901 179.763 -19.8003 179.763 Q-17.6996 179.763 -16.4105 180.782 Q-15.1215 181.792 -15.1215 183.415 Q-15.1215 184.394 -15.5034 185.102 Q-15.8933 185.803 -16.689 186.264 L-15.3522 186.264 L-15.3522 187.736 L-27.7335 187.736 L-27.7335 186.264 L-22.9115 186.264 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-20.1743 170.604 L-19.4581 170.604 L-19.4581 177.336 Q-17.9463 177.241 -17.1505 176.429 Q-16.3628 175.61 -16.3628 174.153 Q-16.3628 173.31 -16.5697 172.522 Q-16.7766 171.726 -17.1903 170.947 L-15.8058 170.947 Q-15.4716 171.734 -15.2965 172.562 Q-15.1215 173.389 -15.1215 174.241 Q-15.1215 176.373 -16.3628 177.623 Q-17.6041 178.864 -19.7207 178.864 Q-21.9089 178.864 -23.19 177.686 Q-24.4791 176.501 -24.4791 174.496 Q-24.4791 172.697 -23.3173 171.655 Q-22.1635 170.604 -20.1743 170.604 M-20.6039 172.069 Q-21.8055 172.085 -22.5216 172.745 Q-23.2378 173.397 -23.2378 174.48 Q-23.2378 175.705 -22.5455 176.445 Q-21.8532 177.177 -20.596 177.288 L-20.6039 172.069 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-26.7946 167.621 L-24.2642 167.621 L-24.2642 164.605 L-23.1264 164.605 L-23.1264 167.621 L-18.2884 167.621 Q-17.1983 167.621 -16.888 167.326 Q-16.5776 167.024 -16.5776 166.109 L-16.5776 164.605 L-15.3522 164.605 L-15.3522 166.109 Q-15.3522 167.804 -15.9808 168.448 Q-16.6174 169.093 -18.2884 169.093 L-23.1264 169.093 L-23.1264 170.167 L-24.2642 170.167 L-24.2642 169.093 L-26.7946 169.093 L-26.7946 167.621 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-26.7946 161.621 L-24.2642 161.621 L-24.2642 158.605 L-23.1264 158.605 L-23.1264 161.621 L-18.2884 161.621 Q-17.1983 161.621 -16.888 161.326 Q-16.5776 161.024 -16.5776 160.109 L-16.5776 158.605 L-15.3522 158.605 L-15.3522 160.109 Q-15.3522 161.804 -15.9808 162.448 Q-16.6174 163.093 -18.2884 163.093 L-23.1264 163.093 L-23.1264 164.167 L-24.2642 164.167 L-24.2642 163.093 L-26.7946 163.093 L-26.7946 161.621 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-20.1743 149.446 L-19.4581 149.446 L-19.4581 156.178 Q-17.9463 156.083 -17.1505 155.271 Q-16.3628 154.451 -16.3628 152.995 Q-16.3628 152.152 -16.5697 151.364 Q-16.7766 150.568 -17.1903 149.789 L-15.8058 149.789 Q-15.4716 150.576 -15.2965 151.404 Q-15.1215 152.231 -15.1215 153.083 Q-15.1215 155.215 -16.3628 156.465 Q-17.6041 157.706 -19.7207 157.706 Q-21.9089 157.706 -23.19 156.528 Q-24.4791 155.343 -24.4791 153.337 Q-24.4791 151.539 -23.3173 150.497 Q-22.1635 149.446 -20.1743 149.446 M-20.6039 150.911 Q-21.8055 150.926 -22.5216 151.587 Q-23.2378 152.239 -23.2378 153.322 Q-23.2378 154.547 -22.5455 155.287 Q-21.8532 156.019 -20.596 156.13 L-20.6039 150.911 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M-22.8956 142.747 Q-23.0388 142.993 -23.1025 143.288 Q-23.1741 143.574 -23.1741 143.924 Q-23.1741 145.166 -22.3625 145.834 Q-21.5588 146.494 -20.0469 146.494 L-15.3522 146.494 L-15.3522 147.966 L-24.2642 147.966 L-24.2642 146.494 L-22.8797 146.494 Q-23.6913 146.033 -24.0812 145.293 Q-24.4791 144.553 -24.4791 143.495 Q-24.4791 143.343 -24.4552 143.16 Q-24.4393 142.977 -24.3995 142.755 L-22.8956 142.747 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M114.722 3.024 L116.767 3.024 L116.767 18.144 L114.722 18.144 L114.722 3.024 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M128.15 11.298 L128.15 18.144 L126.287 18.144 L126.287 11.3587 Q126.287 9.7485 125.659 8.94845 Q125.031 8.14839 123.775 8.14839 Q122.266 8.14839 121.395 9.11048 Q120.525 10.0726 120.525 11.7334 L120.525 18.144 L118.651 18.144 L118.651 6.80147 L120.525 6.80147 L120.525 8.56361 Q121.193 7.54076 122.094 7.0344 Q123.006 6.52803 124.191 6.52803 Q126.145 6.52803 127.148 7.7433 Q128.15 8.94845 128.15 11.298 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M135.847 2.38598 L135.847 3.93545 L134.065 3.93545 Q133.062 3.93545 132.667 4.34054 Q132.282 4.74563 132.282 5.79887 L132.282 6.80147 L135.351 6.80147 L135.351 8.24967 L132.282 8.24967 L132.282 18.144 L130.409 18.144 L130.409 8.24967 L128.626 8.24967 L128.626 6.80147 L130.409 6.80147 L130.409 6.01154 Q130.409 4.11774 131.29 3.25693 Q132.171 2.38598 134.085 2.38598 L135.847 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M142.197 8.10788 Q140.698 8.10788 139.827 9.28265 Q138.956 10.4473 138.956 12.4829 Q138.956 14.5184 139.817 15.6932 Q140.688 16.8578 142.197 16.8578 Q143.686 16.8578 144.557 15.6831 Q145.427 14.5083 145.427 12.4829 Q145.427 10.4675 144.557 9.29277 Q143.686 8.10788 142.197 8.10788 M142.197 6.52803 Q144.627 6.52803 146.015 8.10788 Q147.402 9.68774 147.402 12.4829 Q147.402 15.2679 146.015 16.8578 Q144.627 18.4377 142.197 18.4377 Q139.756 18.4377 138.369 16.8578 Q136.991 15.2679 136.991 12.4829 Q136.991 9.68774 138.369 8.10788 Q139.756 6.52803 142.197 6.52803 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M155.929 8.54336 Q155.615 8.36107 155.241 8.28005 Q154.876 8.1889 154.431 8.1889 Q152.851 8.1889 152 9.22188 Q151.159 10.2447 151.159 12.1689 L151.159 18.144 L149.286 18.144 L149.286 6.80147 L151.159 6.80147 L151.159 8.56361 Q151.747 7.53063 152.689 7.0344 Q153.631 6.52803 154.977 6.52803 Q155.17 6.52803 155.403 6.55841 Q155.636 6.57867 155.919 6.6293 L155.929 8.54336 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M166.35 8.97883 Q167.049 7.72305 168.021 7.12554 Q168.994 6.52803 170.31 6.52803 Q172.082 6.52803 173.044 7.77369 Q174.007 9.00921 174.007 11.298 L174.007 18.144 L172.133 18.144 L172.133 11.3587 Q172.133 9.72825 171.556 8.93832 Q170.979 8.14839 169.794 8.14839 Q168.345 8.14839 167.505 9.11048 Q166.664 10.0726 166.664 11.7334 L166.664 18.144 L164.791 18.144 L164.791 11.3587 Q164.791 9.71812 164.214 8.93832 Q163.636 8.14839 162.431 8.14839 Q161.003 8.14839 160.163 9.12061 Q159.322 10.0827 159.322 11.7334 L159.322 18.144 L157.449 18.144 L157.449 6.80147 L159.322 6.80147 L159.322 8.56361 Q159.96 7.5205 160.851 7.02427 Q161.742 6.52803 162.968 6.52803 Q164.203 6.52803 165.064 7.15592 Q165.935 7.78381 166.35 8.97883 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M181.116 12.4424 Q178.858 12.4424 177.987 12.9588 Q177.116 13.4753 177.116 14.721 Q177.116 15.7135 177.764 16.3008 Q178.422 16.8781 179.546 16.8781 Q181.096 16.8781 182.027 15.7843 Q182.969 14.6805 182.969 12.8576 L182.969 12.4424 L181.116 12.4424 M184.833 11.6727 L184.833 18.144 L182.969 18.144 L182.969 16.4224 Q182.331 17.4553 181.379 17.9516 Q180.427 18.4377 179.05 18.4377 Q177.308 18.4377 176.275 17.4655 Q175.252 16.4831 175.252 14.8425 Q175.252 12.9285 176.528 11.9562 Q177.814 10.984 180.356 10.984 L182.969 10.984 L182.969 10.8017 Q182.969 9.51557 182.119 8.81679 Q181.278 8.10788 179.749 8.10788 Q178.777 8.10788 177.855 8.34081 Q176.933 8.57374 176.083 9.03959 L176.083 7.31796 Q177.106 6.923 178.068 6.73058 Q179.03 6.52803 179.941 6.52803 Q182.402 6.52803 183.617 7.80407 Q184.833 9.0801 184.833 11.6727 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M188.63 3.581 L188.63 6.80147 L192.469 6.80147 L192.469 8.24967 L188.63 8.24967 L188.63 14.407 Q188.63 15.7945 189.005 16.1894 Q189.39 16.5844 190.555 16.5844 L192.469 16.5844 L192.469 18.144 L190.555 18.144 Q188.397 18.144 187.577 17.3439 Q186.757 16.5338 186.757 14.407 L186.757 8.24967 L185.39 8.24967 L185.39 6.80147 L186.757 6.80147 L186.757 3.581 L188.63 3.581 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M194.423 6.80147 L196.287 6.80147 L196.287 18.144 L194.423 18.144 L194.423 6.80147 M194.423 2.38598 L196.287 2.38598 L196.287 4.74563 L194.423 4.74563 L194.423 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M202.636 8.10788 Q201.138 8.10788 200.267 9.28265 Q199.396 10.4473 199.396 12.4829 Q199.396 14.5184 200.256 15.6932 Q201.127 16.8578 202.636 16.8578 Q204.125 16.8578 204.996 15.6831 Q205.867 14.5083 205.867 12.4829 Q205.867 10.4675 204.996 9.29277 Q204.125 8.10788 202.636 8.10788 M202.636 6.52803 Q205.067 6.52803 206.454 8.10788 Q207.842 9.68774 207.842 12.4829 Q207.842 15.2679 206.454 16.8578 Q205.067 18.4377 202.636 18.4377 Q200.196 18.4377 198.808 16.8578 Q197.431 15.2679 197.431 12.4829 Q197.431 9.68774 198.808 8.10788 Q200.196 6.52803 202.636 6.52803 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M219.225 11.298 L219.225 18.144 L217.361 18.144 L217.361 11.3587 Q217.361 9.7485 216.733 8.94845 Q216.106 8.14839 214.85 8.14839 Q213.341 8.14839 212.47 9.11048 Q211.599 10.0726 211.599 11.7334 L211.599 18.144 L209.725 18.144 L209.725 6.80147 L211.599 6.80147 L211.599 8.56361 Q212.267 7.54076 213.169 7.0344 Q214.08 6.52803 215.265 6.52803 Q217.22 6.52803 218.222 7.7433 Q219.225 8.94845 219.225 11.298 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M235.935 7.23694 L235.935 8.97883 Q235.145 8.54336 234.345 8.33068 Q233.555 8.10788 232.745 8.10788 Q230.932 8.10788 229.929 9.26239 Q228.927 10.4068 228.927 12.4829 Q228.927 14.5589 229.929 15.7135 Q230.932 16.8578 232.745 16.8578 Q233.555 16.8578 234.345 16.6452 Q235.145 16.4224 235.935 15.9869 L235.935 17.7085 Q235.155 18.0731 234.314 18.2554 Q233.484 18.4377 232.542 18.4377 Q229.98 18.4377 228.471 16.8275 Q226.962 15.2172 226.962 12.4829 Q226.962 9.70799 228.481 8.11801 Q230.01 6.52803 232.664 6.52803 Q233.524 6.52803 234.345 6.71032 Q235.165 6.88249 235.935 7.23694 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M244.462 8.54336 Q244.148 8.36107 243.773 8.28005 Q243.409 8.1889 242.963 8.1889 Q241.383 8.1889 240.533 9.22188 Q239.692 10.2447 239.692 12.1689 L239.692 18.144 L237.818 18.144 L237.818 6.80147 L239.692 6.80147 L239.692 8.56361 Q240.279 7.53063 241.221 7.0344 Q242.163 6.52803 243.51 6.52803 Q243.702 6.52803 243.935 6.55841 Q244.168 6.57867 244.452 6.6293 L244.462 8.54336 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M246.416 6.80147 L248.28 6.80147 L248.28 18.144 L246.416 18.144 L246.416 6.80147 M246.416 2.38598 L248.28 2.38598 L248.28 4.74563 L246.416 4.74563 L246.416 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M252.078 3.581 L252.078 6.80147 L255.916 6.80147 L255.916 8.24967 L252.078 8.24967 L252.078 14.407 Q252.078 15.7945 252.452 16.1894 Q252.837 16.5844 254.002 16.5844 L255.916 16.5844 L255.916 18.144 L254.002 18.144 Q251.845 18.144 251.024 17.3439 Q250.204 16.5338 250.204 14.407 L250.204 8.24967 L248.837 8.24967 L248.837 6.80147 L250.204 6.80147 L250.204 3.581 L252.078 3.581 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M267.572 12.0069 L267.572 12.9183 L259.005 12.9183 Q259.126 14.8425 260.159 15.8552 Q261.202 16.8578 263.056 16.8578 Q264.129 16.8578 265.132 16.5945 Q266.144 16.3312 267.137 15.8046 L267.137 17.5667 Q266.134 17.9921 265.081 18.2149 Q264.028 18.4377 262.944 18.4377 Q260.23 18.4377 258.64 16.8578 Q257.06 15.278 257.06 12.5841 Q257.06 9.79914 258.559 8.16865 Q260.068 6.52803 262.62 6.52803 Q264.909 6.52803 266.236 8.00661 Q267.572 9.47506 267.572 12.0069 M265.709 11.46 Q265.689 9.93079 264.848 9.01934 Q264.018 8.10788 262.64 8.10788 Q261.081 8.10788 260.139 8.98896 Q259.207 9.87003 259.065 11.4701 L265.709 11.46 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M276.099 8.54336 Q275.786 8.36107 275.411 8.28005 Q275.046 8.1889 274.601 8.1889 Q273.021 8.1889 272.17 9.22188 Q271.33 10.2447 271.33 12.1689 L271.33 18.144 L269.456 18.144 L269.456 6.80147 L271.33 6.80147 L271.33 8.56361 Q271.917 7.53063 272.859 7.0344 Q273.801 6.52803 275.148 6.52803 Q275.34 6.52803 275.573 6.55841 Q275.806 6.57867 276.089 6.6293 L276.099 8.54336 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M278.054 6.80147 L279.917 6.80147 L279.917 18.144 L278.054 18.144 L278.054 6.80147 M278.054 2.38598 L279.917 2.38598 L279.917 4.74563 L278.054 4.74563 L278.054 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M287.027 12.4424 Q284.768 12.4424 283.897 12.9588 Q283.027 13.4753 283.027 14.721 Q283.027 15.7135 283.675 16.3008 Q284.333 16.8781 285.457 16.8781 Q287.007 16.8781 287.938 15.7843 Q288.88 14.6805 288.88 12.8576 L288.88 12.4424 L287.027 12.4424 M290.744 11.6727 L290.744 18.144 L288.88 18.144 L288.88 16.4224 Q288.242 17.4553 287.29 17.9516 Q286.338 18.4377 284.961 18.4377 Q283.219 18.4377 282.186 17.4655 Q281.163 16.4831 281.163 14.8425 Q281.163 12.9285 282.439 11.9562 Q283.725 10.984 286.267 10.984 L288.88 10.984 L288.88 10.8017 Q288.88 9.51557 288.029 8.81679 Q287.189 8.10788 285.66 8.10788 Q284.687 8.10788 283.766 8.34081 Q282.844 8.57374 281.994 9.03959 L281.994 7.31796 Q283.016 6.923 283.979 6.73058 Q284.941 6.52803 285.852 6.52803 Q288.313 6.52803 289.528 7.80407 Q290.744 9.0801 290.744 11.6727 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M307.433 12.4829 Q307.433 10.427 306.583 9.26239 Q305.742 8.08763 304.263 8.08763 Q302.785 8.08763 301.934 9.26239 Q301.094 10.427 301.094 12.4829 Q301.094 14.5387 301.934 15.7135 Q302.785 16.8781 304.263 16.8781 Q305.742 16.8781 306.583 15.7135 Q307.433 14.5387 307.433 12.4829 M301.094 8.5231 Q301.681 7.51038 302.572 7.02427 Q303.473 6.52803 304.719 6.52803 Q306.785 6.52803 308.071 8.16865 Q309.368 9.80926 309.368 12.4829 Q309.368 15.1565 308.071 16.7971 Q306.785 18.4377 304.719 18.4377 Q303.473 18.4377 302.572 17.9516 Q301.681 17.4553 301.094 16.4426 L301.094 18.144 L299.22 18.144 L299.22 2.38598 L301.094 2.38598 L301.094 8.5231 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M316.041 19.1972 Q315.251 21.2227 314.502 21.8405 Q313.753 22.4582 312.497 22.4582 L311.008 22.4582 L311.008 20.8986 L312.102 20.8986 Q312.872 20.8986 313.297 20.534 Q313.722 20.1695 314.239 18.8124 L314.573 17.9617 L309.985 6.80147 L311.96 6.80147 L315.505 15.6729 L319.049 6.80147 L321.024 6.80147 L316.041 19.1972 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M339 11.298 L339 18.144 L337.136 18.144 L337.136 11.3587 Q337.136 9.7485 336.509 8.94845 Q335.881 8.14839 334.625 8.14839 Q333.116 8.14839 332.245 9.11048 Q331.374 10.0726 331.374 11.7334 L331.374 18.144 L329.501 18.144 L329.501 6.80147 L331.374 6.80147 L331.374 8.56361 Q332.042 7.54076 332.944 7.0344 Q333.855 6.52803 335.04 6.52803 Q336.995 6.52803 337.997 7.7433 Q339 8.94845 339 11.298 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M340.762 13.6678 L340.762 6.80147 L342.625 6.80147 L342.625 13.5969 Q342.625 15.2071 343.253 16.0173 Q343.881 16.8173 345.137 16.8173 Q346.646 16.8173 347.517 15.8552 Q348.398 14.8931 348.398 13.2323 L348.398 6.80147 L350.261 6.80147 L350.261 18.144 L348.398 18.144 L348.398 16.4021 Q347.719 17.4351 346.818 17.9415 Q345.927 18.4377 344.742 18.4377 Q342.788 18.4377 341.775 17.2224 Q340.762 16.0071 340.762 13.6678 M345.451 6.52803 L345.451 6.52803 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M361.047 8.97883 Q361.746 7.72305 362.718 7.12554 Q363.69 6.52803 365.007 6.52803 Q366.779 6.52803 367.741 7.77369 Q368.703 9.00921 368.703 11.298 L368.703 18.144 L366.83 18.144 L366.83 11.3587 Q366.83 9.72825 366.252 8.93832 Q365.675 8.14839 364.49 8.14839 Q363.042 8.14839 362.201 9.11048 Q361.361 10.0726 361.361 11.7334 L361.361 18.144 L359.487 18.144 L359.487 11.3587 Q359.487 9.71812 358.91 8.93832 Q358.333 8.14839 357.128 8.14839 Q355.7 8.14839 354.859 9.12061 Q354.019 10.0827 354.019 11.7334 L354.019 18.144 L352.145 18.144 L352.145 6.80147 L354.019 6.80147 L354.019 8.56361 Q354.657 7.5205 355.548 7.02427 Q356.439 6.52803 357.664 6.52803 Q358.9 6.52803 359.761 7.15592 Q360.632 7.78381 361.047 8.97883 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M378.8 12.4829 Q378.8 10.427 377.949 9.26239 Q377.109 8.08763 375.63 8.08763 Q374.152 8.08763 373.301 9.26239 Q372.46 10.427 372.46 12.4829 Q372.46 14.5387 373.301 15.7135 Q374.152 16.8781 375.63 16.8781 Q377.109 16.8781 377.949 15.7135 Q378.8 14.5387 378.8 12.4829 M372.46 8.5231 Q373.048 7.51038 373.939 7.02427 Q374.84 6.52803 376.086 6.52803 Q378.152 6.52803 379.438 8.16865 Q380.734 9.80926 380.734 12.4829 Q380.734 15.1565 379.438 16.7971 Q378.152 18.4377 376.086 18.4377 Q374.84 18.4377 373.939 17.9516 Q373.048 17.4553 372.46 16.4426 L372.46 18.144 L370.587 18.144 L370.587 2.38598 L372.46 2.38598 L372.46 8.5231 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M392.391 12.0069 L392.391 12.9183 L383.823 12.9183 Q383.945 14.8425 384.978 15.8552 Q386.021 16.8578 387.874 16.8578 Q388.948 16.8578 389.95 16.5945 Q390.963 16.3312 391.955 15.8046 L391.955 17.5667 Q390.953 17.9921 389.9 18.2149 Q388.846 18.4377 387.763 18.4377 Q385.049 18.4377 383.459 16.8578 Q381.879 15.278 381.879 12.5841 Q381.879 9.79914 383.378 8.16865 Q384.887 6.52803 387.439 6.52803 Q389.727 6.52803 391.054 8.00661 Q392.391 9.47506 392.391 12.0069 M390.527 11.46 Q390.507 9.93079 389.667 9.01934 Q388.836 8.10788 387.459 8.10788 Q385.899 8.10788 384.957 8.98896 Q384.026 9.87003 383.884 11.4701 L390.527 11.46 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M400.918 8.54336 Q400.604 8.36107 400.229 8.28005 Q399.865 8.1889 399.419 8.1889 Q397.839 8.1889 396.989 9.22188 Q396.148 10.2447 396.148 12.1689 L396.148 18.144 L394.275 18.144 L394.275 6.80147 L396.148 6.80147 L396.148 8.56361 Q396.735 7.53063 397.677 7.0344 Q398.619 6.52803 399.966 6.52803 Q400.158 6.52803 400.391 6.55841 Q400.624 6.57867 400.908 6.6293 L400.918 8.54336 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M413.861 8.10788 Q412.362 8.10788 411.491 9.28265 Q410.62 10.4473 410.62 12.4829 Q410.62 14.5184 411.481 15.6932 Q412.352 16.8578 413.861 16.8578 Q415.349 16.8578 416.22 15.6831 Q417.091 14.5083 417.091 12.4829 Q417.091 10.4675 416.22 9.29277 Q415.349 8.10788 413.861 8.10788 M413.861 6.52803 Q416.291 6.52803 417.679 8.10788 Q419.066 9.68774 419.066 12.4829 Q419.066 15.2679 417.679 16.8578 Q416.291 18.4377 413.861 18.4377 Q411.42 18.4377 410.033 16.8578 Q408.655 15.2679 408.655 12.4829 Q408.655 9.68774 410.033 8.10788 Q411.42 6.52803 413.861 6.52803 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M426.763 2.38598 L426.763 3.93545 L424.98 3.93545 Q423.978 3.93545 423.583 4.34054 Q423.198 4.74563 423.198 5.79887 L423.198 6.80147 L426.267 6.80147 L426.267 8.24967 L423.198 8.24967 L423.198 18.144 L421.324 18.144 L421.324 8.24967 L419.542 8.24967 L419.542 6.80147 L421.324 6.80147 L421.324 6.01154 Q421.324 4.11774 422.205 3.25693 Q423.087 2.38598 425.001 2.38598 L426.763 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M443.473 7.23694 L443.473 8.97883 Q442.683 8.54336 441.883 8.33068 Q441.093 8.10788 440.283 8.10788 Q438.47 8.10788 437.467 9.26239 Q436.465 10.4068 436.465 12.4829 Q436.465 14.5589 437.467 15.7135 Q438.47 16.8578 440.283 16.8578 Q441.093 16.8578 441.883 16.6452 Q442.683 16.4224 443.473 15.9869 L443.473 17.7085 Q442.693 18.0731 441.852 18.2554 Q441.022 18.4377 440.08 18.4377 Q437.518 18.4377 436.009 16.8275 Q434.5 15.2172 434.5 12.4829 Q434.5 9.70799 436.019 8.11801 Q437.548 6.52803 440.202 6.52803 Q441.062 6.52803 441.883 6.71032 Q442.703 6.88249 443.473 7.23694 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M445.427 2.38598 L447.291 2.38598 L447.291 18.144 L445.427 18.144 L445.427 2.38598 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M454.4 12.4424 Q452.142 12.4424 451.271 12.9588 Q450.4 13.4753 450.4 14.721 Q450.4 15.7135 451.048 16.3008 Q451.706 16.8781 452.83 16.8781 Q454.38 16.8781 455.312 15.7843 Q456.253 14.6805 456.253 12.8576 L456.253 12.4424 L454.4 12.4424 M458.117 11.6727 L458.117 18.144 L456.253 18.144 L456.253 16.4224 Q455.615 17.4553 454.663 17.9516 Q453.711 18.4377 452.334 18.4377 Q450.592 18.4377 449.559 17.4655 Q448.536 16.4831 448.536 14.8425 Q448.536 12.9285 449.812 11.9562 Q451.099 10.984 453.641 10.984 L456.253 10.984 L456.253 10.8017 Q456.253 9.51557 455.403 8.81679 Q454.562 8.10788 453.033 8.10788 Q452.061 8.10788 451.139 8.34081 Q450.217 8.57374 449.367 9.03959 L449.367 7.31796 Q450.39 6.923 451.352 6.73058 Q452.314 6.52803 453.225 6.52803 Q455.686 6.52803 456.901 7.80407 Q458.117 9.0801 458.117 11.6727 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M467.302 7.13567 L467.302 8.89781 Q466.512 8.49272 465.662 8.29018 Q464.811 8.08763 463.899 8.08763 Q462.512 8.08763 461.813 8.51298 Q461.125 8.93832 461.125 9.78901 Q461.125 10.4372 461.621 10.8119 Q462.117 11.1764 463.616 11.5106 L464.254 11.6524 Q466.239 12.0778 467.069 12.8576 Q467.91 13.6272 467.91 15.0147 Q467.91 16.5945 466.654 17.5161 Q465.408 18.4377 463.221 18.4377 Q462.309 18.4377 461.317 18.2554 Q460.335 18.0832 459.241 17.7288 L459.241 15.8046 Q460.274 16.3413 461.276 16.6148 Q462.279 16.8781 463.261 16.8781 Q464.578 16.8781 465.287 16.4325 Q465.996 15.9768 465.996 15.1565 Q465.996 14.3969 465.479 13.9918 Q464.973 13.5867 463.241 13.212 L462.593 13.0601 Q460.861 12.6955 460.092 11.9461 Q459.322 11.1866 459.322 9.87003 Q459.322 8.26992 460.456 7.39898 Q461.59 6.52803 463.677 6.52803 Q464.71 6.52803 465.621 6.67994 Q466.533 6.83185 467.302 7.13567 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M477.095 7.13567 L477.095 8.89781 Q476.305 8.49272 475.455 8.29018 Q474.604 8.08763 473.692 8.08763 Q472.305 8.08763 471.606 8.51298 Q470.918 8.93832 470.918 9.78901 Q470.918 10.4372 471.414 10.8119 Q471.91 11.1764 473.409 11.5106 L474.047 11.6524 Q476.032 12.0778 476.862 12.8576 Q477.703 13.6272 477.703 15.0147 Q477.703 16.5945 476.447 17.5161 Q475.201 18.4377 473.014 18.4377 Q472.103 18.4377 471.11 18.2554 Q470.128 18.0832 469.034 17.7288 L469.034 15.8046 Q470.067 16.3413 471.07 16.6148 Q472.072 16.8781 473.054 16.8781 Q474.371 16.8781 475.08 16.4325 Q475.789 15.9768 475.789 15.1565 Q475.789 14.3969 475.272 13.9918 Q474.766 13.5867 473.034 13.212 L472.386 13.0601 Q470.654 12.6955 469.885 11.9461 Q469.115 11.1866 469.115 9.87003 Q469.115 8.26992 470.249 7.39898 Q471.383 6.52803 473.47 6.52803 Q474.503 6.52803 475.414 6.67994 Q476.326 6.83185 477.095 7.13567 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M489.359 12.0069 L489.359 12.9183 L480.792 12.9183 Q480.913 14.8425 481.946 15.8552 Q482.989 16.8578 484.843 16.8578 Q485.916 16.8578 486.919 16.5945 Q487.931 16.3312 488.924 15.8046 L488.924 17.5667 Q487.921 17.9921 486.868 18.2149 Q485.815 18.4377 484.731 18.4377 Q482.017 18.4377 480.427 16.8578 Q478.847 15.278 478.847 12.5841 Q478.847 9.79914 480.346 8.16865 Q481.855 6.52803 484.407 6.52803 Q486.696 6.52803 488.023 8.00661 Q489.359 9.47506 489.359 12.0069 M487.496 11.46 Q487.476 9.93079 486.635 9.01934 Q485.805 8.10788 484.427 8.10788 Q482.868 8.10788 481.926 8.98896 Q480.994 9.87003 480.852 11.4701 L487.496 11.46 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M498.545 7.13567 L498.545 8.89781 Q497.755 8.49272 496.904 8.29018 Q496.053 8.08763 495.142 8.08763 Q493.755 8.08763 493.056 8.51298 Q492.367 8.93832 492.367 9.78901 Q492.367 10.4372 492.863 10.8119 Q493.36 11.1764 494.858 11.5106 L495.496 11.6524 Q497.481 12.0778 498.312 12.8576 Q499.152 13.6272 499.152 15.0147 Q499.152 16.5945 497.897 17.5161 Q496.651 18.4377 494.463 18.4377 Q493.552 18.4377 492.56 18.2554 Q491.577 18.0832 490.483 17.7288 L490.483 15.8046 Q491.516 16.3413 492.519 16.6148 Q493.522 16.8781 494.504 16.8781 Q495.821 16.8781 496.529 16.4325 Q497.238 15.9768 497.238 15.1565 Q497.238 14.3969 496.722 13.9918 Q496.216 13.5867 494.484 13.212 L493.836 13.0601 Q492.104 12.6955 491.334 11.9461 Q490.564 11.1866 490.564 9.87003 Q490.564 8.26992 491.699 7.39898 Q492.833 6.52803 494.919 6.52803 Q495.952 6.52803 496.864 6.67994 Q497.775 6.83185 498.545 7.13567 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip652)" style="stroke:#009af9; stroke-width:1; stroke-opacity:1; fill:none" points="
  41.605,74.4437 117.414,282.26 193.223,262.532 269.032,217.03 344.842,176.875 420.651,124.92 496.46,70.4967 572.269,25.9528 
  "/>
<polyline clip-path="url(#clip652)" style="stroke:#e26f46; stroke-width:1; stroke-opacity:1; fill:none" points="
  41.605,107.4 117.414,350.528 193.223,366.11 269.032,355.919 344.842,351.075 420.651,334.431 496.46,315.318 572.269,306.085 
  "/>
<path clip-path="url(#clip650)" d="
M489.057 73.1269 L569.439 73.1269 L569.439 27.7669 L489.057 27.7669  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip650)" style="stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none" points="
  489.057,73.1269 569.439,73.1269 569.439,27.7669 489.057,27.7669 489.057,73.1269 
  "/>
<polyline clip-path="url(#clip650)" style="stroke:#009af9; stroke-width:1; stroke-opacity:1; fill:none" points="
  495.307,42.8869 532.807,42.8869 
  "/>
<path clip-path="url(#clip650)" d="M 0 0 M540.226 43.0808 L540.226 46.2463 L542.101 46.2463 Q543.044 46.2463 543.496 45.8586 Q543.953 45.465 543.953 44.6606 Q543.953 43.8505 543.496 43.4685 Q543.044 43.0808 542.101 43.0808 L540.226 43.0808 M540.226 39.5276 L540.226 42.1317 L541.956 42.1317 Q542.813 42.1317 543.229 41.8134 Q543.652 41.4894 543.652 40.8296 Q543.652 40.1757 543.229 39.8516 Q542.813 39.5276 541.956 39.5276 L540.226 39.5276 M539.057 38.5669 L542.043 38.5669 Q543.38 38.5669 544.103 39.1225 Q544.827 39.678 544.827 40.7023 Q544.827 41.4952 544.456 41.9639 Q544.086 42.4326 543.368 42.5484 Q544.23 42.7336 544.705 43.3238 Q545.185 43.9083 545.185 44.788 Q545.185 45.9454 544.398 46.5761 Q543.611 47.2069 542.159 47.2069 L539.057 47.2069 L539.057 38.5669 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M546.349 38.5669 L547.518 38.5669 L547.518 47.2069 L546.349 47.2069 L546.349 38.5669 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M555.151 39.2324 L555.151 40.4651 Q554.56 39.9153 553.889 39.6433 Q553.223 39.3713 552.471 39.3713 Q550.99 39.3713 550.203 40.2799 Q549.416 41.1827 549.416 42.8956 Q549.416 44.6028 550.203 45.5113 Q550.99 46.4141 552.471 46.4141 Q553.223 46.4141 553.889 46.1421 Q554.56 45.8701 555.151 45.3204 L555.151 46.5414 Q554.537 46.9581 553.848 47.1664 Q553.166 47.3748 552.402 47.3748 Q550.44 47.3748 549.311 46.1768 Q548.183 44.9731 548.183 42.8956 Q548.183 40.8123 549.311 39.6144 Q550.44 38.4107 552.402 38.4107 Q553.177 38.4107 553.86 38.619 Q554.549 38.8216 555.151 39.2324 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip650)" style="stroke:#e26f46; stroke-width:1; stroke-opacity:1; fill:none" points="
  495.307,58.0069 532.807,58.0069 
  "/>
<path clip-path="url(#clip650)" d="M 0 0 M543.015 54.8385 L541.43 59.1383 L544.607 59.1383 L543.015 54.8385 M542.356 53.6869 L543.681 53.6869 L546.974 62.3269 L545.758 62.3269 L544.971 60.1105 L541.077 60.1105 L540.29 62.3269 L539.057 62.3269 L542.356 53.6869 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M548.137 53.6869 L549.306 53.6869 L549.306 62.3269 L548.137 62.3269 L548.137 53.6869 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip650)" d="M 0 0 M556.939 54.3524 L556.939 55.5851 Q556.348 55.0353 555.677 54.7633 Q555.012 54.4913 554.259 54.4913 Q552.778 54.4913 551.991 55.3999 Q551.204 56.3027 551.204 58.0156 Q551.204 59.7228 551.991 60.6313 Q552.778 61.5341 554.259 61.5341 Q555.012 61.5341 555.677 61.2621 Q556.348 60.9901 556.939 60.4404 L556.939 61.6614 Q556.325 62.0781 555.637 62.2864 Q554.954 62.4948 554.19 62.4948 Q552.228 62.4948 551.1 61.2968 Q549.971 60.0931 549.971 58.0156 Q549.971 55.9323 551.1 54.7344 Q552.228 53.5307 554.19 53.5307 Q554.965 53.5307 555.648 53.739 Q556.337 53.9416 556.939 54.3524 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /></svg>
<p>We see that following the &quot;lowest AIC&quot; rule we would indeed choose three classes, while following the &quot;best AIC&quot; criteria we would have choosen only two classes. This means that there is two classes that, concerning the floreal measures used in the database, are very similar, and opur models are unsure about them. Perhaps the biologists will end up one day with the conclusion that it is indeed only one specie :-).</p><p>We could study this issue more in detail by analysing the <a href="../../Utils.html#BetaML.Utils.ConfusionMatrix"><code>ConfusionMatrix</code></a>, but the one used in BetaML does not account for the ignoreLabels option (yet).</p><h2 id="Benchmarking-computational-efficiency"><a class="docs-heading-anchor" href="#Benchmarking-computational-efficiency">Benchmarking computational efficiency</a><a id="Benchmarking-computational-efficiency-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmarking-computational-efficiency" title="Permalink"></a></h2><p>We now benchmark the time and memory required by the various models by using the <code>@btime</code> macro of the <code>BenchmarkTools</code> package:</p><pre><code class="language-none">@btime kmeans($xs,3);
# 261.540 μs (3777 allocations: 442.53 KiB)
@btime kmedoids($xs,3);
4.576 ms (97356 allocations: 10.42 MiB)
@btime gmm($xs,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE);
# 5.498 ms (133365 allocations: 8.42 MiB)
@btime gmm($xs,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE);
# 18.901 ms (404333 allocations: 25.65 MiB)
@btime gmm($xs,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE);
# 49.257 ms (351500 allocations: 61.95 MiB)
@btime Clustering.kmeans($xs&#39;, 3);
# 17.071 μs (23 allocations: 14.31 KiB)
@btime begin dGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:diag); GaussianMixtures.em!(dGMM, $xs) end;
# 530.528 μs (2088 allocations: 488.05 KiB)
@btime begin fGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:full); GaussianMixtures.em!(fGMM, $xs) end;
# 4.166 ms (58910 allocations: 3.59 MiB)</code></pre><p>(<em>note: the values reported here are of a local pc, not of the GitHub CI server, as sometimes - depending on data and random initialisation - <code>GaussainMixtures.em!</code><code>fails with a</code>PosDefException`. This in turln would lead the whole documentation to fail to compile</em>)</p><p>Like for supervised models, dedicated models are much better optimized than BetaML models, and are order of magnitude more efficient. However even the slowest BetaML clusering model (gmm using full gaussians) is realtively fast and can handle mid-size datasets (tens to hundreds of thousand records) without significant slow downs.</p><h2 id="Conclusions"><a class="docs-heading-anchor" href="#Conclusions">Conclusions</a><a id="Conclusions-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions" title="Permalink"></a></h2><p>We have shown in this tutorial how we can easily run clustering almgorithms in BetaML with just one line of code <code>choosenModel(x,k)</code>, but also how can we use cross-validation in order to help the model or parameter selection, with or whithout knowing the real classes. We retrieve here what we observed with supervised models. Globally the accuracy of BetaML models are comparable to those of leading specialised packages (in this case they are even better), but there is a significant gap in computational efficiency that restricts the pratical usage of BetaML to mid-size datasets. However we trade this relative inefficiency with very flexible model definition and utility functions (for example the BetaML gmm works with missing data, allowing it to be used as the backbone of the <a href="../../Clustering.html#BetaML.Clustering.predictMissing"><code>predictMissing</code></a> missing imputation function, or for collaborative reccomendation systems).</p><p><a href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Classification - cars/betaml_tutorial_classification_cars.html">« A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a><a class="docs-footer-nextpage" href="../../Perceptron.html">Perceptron »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 2 June 2021 10:04">Wednesday 2 June 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
