<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A regression task: the prediction of  bike  sharing demand · BetaML.jl Documentation</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">BetaML.jl Documentation</span></div><form class="docs-search" action="../../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li><li><span class="tocitem">Tutorials</span><ul><li><input class="collapse-toggle" id="menuitem-7-1" type="checkbox"/><label class="tocitem" for="menuitem-7-1"><span class="docs-label">Getting started</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Getting started/betaml_tutorial_getting_started.html">Getting started</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-2" type="checkbox" checked/><label class="tocitem" for="menuitem-7-2"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a><ul class="internal"><li><a class="tocitem" href="#Library-and-data-loading"><span>Library and data loading</span></a></li><li><a class="tocitem" href="#Decision-Trees"><span>Decision Trees</span></a></li><li><a class="tocitem" href="#Random-Forests"><span>Random Forests</span></a></li><li><a class="tocitem" href="#Neural-Networks"><span>Neural Networks</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-3" type="checkbox"/><label class="tocitem" for="menuitem-7-3"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-4" type="checkbox"/><label class="tocitem" for="menuitem-7-4"><span class="docs-label">Clusterisation - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Clusterisation - Iris/betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris tdataset)</a></li></ul></li></ul></li><li><a class="tocitem" href="../../Examples.html">Examples</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">Regression - bike sharing</a></li><li class="is-active"><a href="betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="regression_tutorial"><a class="docs-heading-anchor" href="#regression_tutorial">A regression task: the prediction of  bike  sharing demand</a><a id="regression_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#regression_tutorial" title="Permalink"></a></h1><p>The task is to estimate the influence of several variables (like the weather, the season, the day of the week..) on the demand of shared bicycles, so that the authority in charge of the service can organise the service in the best way.</p><p>Data origin:</p><ul><li>original full dataset (by hour, not used here): <a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset">https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset</a></li><li>simplified dataset (by day, with some simple scaling): <a href="tutorials/Regression - bike sharing/ https:/www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec">https://www.hds.utc.fr/~tdenoeux/dokuwiki/en/aec</a></li><li>description: <a href="https://www.hds.utc.fr/~tdenoeux/dokuwiki/_media/en/exam_2019_ace_.pdf">https://www.hds.utc.fr/~tdenoeux/dokuwiki/<em>media/en/exam</em>2019<em>ace</em>.pdf</a></li><li>data: <a href="https://www.hds.utc.fr/~tdenoeux/dokuwiki/_media/en/bike_sharing_day.csv.zip">https://www.hds.utc.fr/~tdenoeux/dokuwiki/<em>media/en/bike</em>sharing_day.csv.zip</a></li></ul><p>Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. <span>$Y_t = f(X_t)$</span> alone).</p><h2 id="Library-and-data-loading"><a class="docs-heading-anchor" href="#Library-and-data-loading">Library and data loading</a><a id="Library-and-data-loading-1"></a><a class="docs-heading-anchor-permalink" href="#Library-and-data-loading" title="Permalink"></a></h2><p>We first load all the packages we are going to use</p><pre><code class="language-julia">using  LinearAlgebra, Random, Statistics, DataFrames, CSV, Plots, Pipe, BenchmarkTools, BetaML
import Distributions: Uniform
import DecisionTree, Flux ## For comparisions</code></pre><p>Here we load the data from a csv provided by the BataML package</p><pre><code class="language-">baseDir = joinpath(dirname(pathof(BetaML)),&quot;..&quot;,&quot;docs&quot;,&quot;src&quot;,&quot;tutorials&quot;,&quot;Regression - bike sharing&quot;)
data    = CSV.File(joinpath(baseDir,&quot;data&quot;,&quot;bike_sharing_day.csv&quot;),delim=&#39;,&#39;) |&gt; DataFrame
describe(data)</code></pre><p>The variable we want to learn to predict is <code>cnt</code>, the total demand of bikes for a given day. Even if it is indeed an integer, we treat it as a continuous variable, so each single prediction will be a scalar <span>$Y \in \mathbb{R}$</span>.</p><pre><code class="language-">plot(data.cnt, title=&quot;Daily bike sharing rents (2Y)&quot;, label=nothing)</code></pre><h2 id="Decision-Trees"><a class="docs-heading-anchor" href="#Decision-Trees">Decision Trees</a><a id="Decision-Trees-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-Trees" title="Permalink"></a></h2><p>We start our regression task with Decision Trees</p><h3 id="Data-preparation"><a class="docs-heading-anchor" href="#Data-preparation">Data preparation</a><a id="Data-preparation-1"></a><a class="docs-heading-anchor-permalink" href="#Data-preparation" title="Permalink"></a></h3><p>The first step is to prepare the data for the analysis. This indeed depends already on the model we want to employ, as some models &quot;accept&quot; everything as input, no matter if the data is numerical or categorical, if it has missing values or not... while other models are instead much more exigents, and require more work to &quot;clean up&quot; our dataset. Here we start using  Decision Tree and Random Forest models that belong to the first group, so the only things we have to do is to select the variables in input (the &quot;feature matrix&quot;, we wil lindicate it with &quot;X&quot;) and those representing our output (the values we want to learn to predict, we call them &quot;y&quot;):</p><pre><code class="language-">x    = Matrix{Float64}(data[:,[:instant,:season,:yr,:mnth,:holiday,:weekday,:workingday,:weathersit,:temp,:atemp,:hum,:windspeed]])
y    = data[:,16];
nothing #hide</code></pre><p>We can now split the dataset between the data we will use for training the algorithm (<code>xtrain</code>/<code>ytrain</code>), those for selecting the hyperparameters (<code>xval</code>/<code>yval</code>) and finally those for testing the quality of the algoritm with the optimal hyperparameters (<code>xtest</code>/<code>ytest</code>). We use the <code>partition</code> function specifying the share we want to use for these three different subsets, here 75%, 12.5% and 12.5 respectively. As our data represents indeed a time serie, we want our model to be able to predict <em>future</em> demand of bike sharing from <em>past</em>, observed rented bikes, so we do not shuffle the datasets as it would be the default.</p><pre><code class="language-">((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)
(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)</code></pre><p>We can now &quot;tune&quot; our model so-called hyperparameters, i.e. choose the best exogenous parameters of our algorithm, where &quot;best&quot; refers to some minimisation of a &quot;loss&quot; function between the true and the predicted value. To make the comparision we use a specific &quot;validation&quot; subset of data (<code>xval</code> and <code>yval</code>).</p><p>BetaML doesn&#39;t have a dedicated function for hyperparameters optimisation, but it is easy to write some custom julia code, at least for a simple grid-based &quot;search&quot;. Indeed one of the main reasons that a dedicated function exists in other Machine Learning libraries is that loops in other languages are slow, but this is not a problem in julia, so we can retain the flexibility to write the kind of hyperparameter tuning that best fits our needs.</p><p>Below is an example of a possible such function. Note there are more &quot;elegant&quot; ways to code it, but this one does the job. We will see the various functions inside <code>tuneHyperParameters()</code> in a moment. For now let&#39;s going just to observe that <code>tuneHyperParameters</code> just loops over all the possible hyperparameters and selects the one where the error between <code>xval</code> and <code>yval</code> is minimised. For the meaning of the various hyperparameter, consult the documentation of the <code>buildTree</code> and <code>buildForest</code> functions. The function uses multiple threads, so we calls <code>generateParallelRngs()</code> (in the <code>BetaML.Utils</code> submodule) to generate thread-safe random number generators and locks the comparision step.</p><pre><code class="language-julia">function tuneHyperParameters(model,xtrain,ytrain,xval,yval;maxDepthRange=15:15,maxFeaturesRange=size(xtrain,2):size(xtrain,2),nTreesRange=20:20,βRange=0:0,minRecordsRange=2:2,repetitions=5,rng=Random.GLOBAL_RNG)
    # We start with an infinitely high error
    bestRme         = +Inf
    bestMaxDepth    = 1
    bestMaxFeatures = 1
    bestMinRecords  = 2
    bestNTrees      = 1
    bestβ           = 0
    compLock        = ReentrantLock()

    # Generate one random number generator per thread
    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generateParallelRngs
    rngs = generateParallelRngs(rng,Threads.nthreads())

    # We loop over all possible hyperparameter combinations...
    parLengths = (length(maxDepthRange),length(maxFeaturesRange),length(minRecordsRange),length(nTreesRange),length(βRange))
    Threads.@threads for ij in CartesianIndices(parLengths) ## This to avoid many nested for loops
           (maxDepth,maxFeatures,minRecords,nTrees,β)   = (maxDepthRange[Tuple(ij)[1]], maxFeaturesRange[Tuple(ij)[2]], minRecordsRange[Tuple(ij)[3]], nTreesRange[Tuple(ij)[4]], βRange[Tuple(ij)[5]]) ## The specific hyperparameters of this nested loop
           tsrng = rngs[Threads.threadid()] ## The random number generator is specific for each thread..
           joinedIndx = LinearIndices(parLengths)[ij]
           # And here we make the seeding depending on the id of the loop, not the thread: hence we get the same results indipendently of the number of threads
           Random.seed!(tsrng,masterSeed+joinedIndx*10)
           totAttemptError = 0.0
           # We run several repetitions with the same hyperparameter combination to account for stochasticity...
           for r in 1:repetitions
              if model == &quot;DecisionTree&quot;
                 # Here we train the Decition Tree model
                 myTrainedModel = buildTree(xtrain,ytrain, maxDepth=maxDepth,maxFeatures=maxFeatures,minRecords=minRecords,rng=tsrng)
              else
                 # Here we train the Random Forest model
                 myTrainedModel = buildForest(xtrain,ytrain,nTrees,maxDepth=maxDepth,maxFeatures=maxFeatures,minRecords=minRecords,β=β,rng=tsrng)
              end
              # Here we make prediciton with this trained model and we compute its error
              ŷval   = predict(myTrainedModel, xval,rng=tsrng)
              rmeVal = meanRelError(ŷval,yval,normRec=false)
              totAttemptError += rmeVal
           end
           avgAttemptedDepthError = totAttemptError / repetitions
           begin
               lock(compLock) ## This step can&#39;t be run in parallel...
               try
                   # Select this specific combination of hyperparameters if the error is the lowest
                   if avgAttemptedDepthError &lt; bestRme
                     bestRme         = avgAttemptedDepthError
                     bestMaxDepth    = maxDepth
                     bestMaxFeatures = maxFeatures
                     bestNTrees      = nTrees
                     bestβ           = β
                     bestMinRecords  = minRecords
                   end
               finally
                   unlock(compLock)
               end
           end
    end
    return (bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords,bestNTrees,bestβ)
end</code></pre><pre class="documenter-example-output">tuneHyperParameters (generic function with 1 method)</pre><p>We can now run the hyperparameter optimisation function with some &quot;reasonable&quot; ranges. To obtain replicable results we call <code>tuneHyperParameters</code> with <code>rng=copy(FIXEDRNG)</code>, where <code>FIXEDRNG</code> is a fixed-seeded random number generator guaranteed to maintain the same stream of random numbers even between different julia versions. That&#39;s also what we use for our unit tests.</p><pre><code class="language-">(bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords) = tuneHyperParameters(&quot;DecisionTree&quot;,xtrain,ytrain,xval,yval,
           maxDepthRange=3:7,maxFeaturesRange=10:12,minRecordsRange=2:6,repetitions=10,rng=copy(FIXEDRNG))</code></pre><p>Now that we have found the &quot;optimal&quot; hyperparameters we can build (&quot;train&quot;) our model using them:</p><pre><code class="language-">myTree = buildTree(xtrain,ytrain, maxDepth=bestMaxDepth, maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,rng=copy(FIXEDRNG));
nothing #hide</code></pre><p>Let&#39;s benchmark the time and memory usage of the training step of a decision tree:</p><pre><code class="language-">@btime  buildTree(xtrain,ytrain, maxDepth=bestMaxDepth, maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,rng=copy(FIXEDRNG));
nothing #hide</code></pre><p>Individual decision trees are blazing fast, among the fastest algorithms we could use.</p><p>The above <code>buildTree</code>function produces a DecisionTree object that can be used to make predictions given some new features, i.e. given some X matrix of (number of observations x dimensions), predict the corresponding Y vector of scalers in R.</p><pre><code class="language-">(ŷtrain,ŷval,ŷtest) = predict.([myTree], [xtrain,xval,xtest])</code></pre><p>Note that the above code uses the &quot;dot syntax&quot; to &quot;broadcast&quot; <code>predict()</code> over an array of label matrices. It is exactly equivalent to:</p><pre><code class="language-">ŷtrain = predict(myTree, xtrain);
ŷval   = predict(myTree, xval);
ŷtest  = predict(myTree, xtest);
nothing #hide</code></pre><p>We now compute the relative mean error for the training, the validation and the test set. The <code>meanRelError</code> is a very flexible error function. Without additional parameter, it computes, as the name says, the <em>mean relative error</em>, also known as the &quot;mean absolute percentage error&quot; (MAPE)](https://en.wikipedia.org/wiki/Mean<em>absolute</em>percentage<em>error)&quot;) between an estimated and a true vector. However it can also compute the _relative mean error</em> (as we do here), or use a p-norm higher than 1. The <em>mean relative error</em> enfatises the relativeness of the error, i.e. all observations and dimensions weigth the same, wether large or small. Conversly, in the <em>relative mean error</em> the same relative error on larger observations (or dimensions) weights more. In this exercise we use the later, as our data has clearly some outlier days with very small rents, and we care more of avoiding our customers finding empty bike racks than having unrented bikes on the rack. Targeting a low mean average error would push all our predicitons down to try accomodate the low-level predicitons (to avoid a large relative error), and that&#39;s not what we want.</p><p>For example let&#39;c consider the following example:</p><pre><code class="language-julia">y     = [30,28,27,3,32,38];
ŷpref = [32,30,28,10,31,40];
ŷbad  = [29,25,24,5,28,35];</code></pre><p>Here ŷpref is an ipotetical output of a model that minimise the relative mean error, while ŷbad minimise the mean realative error</p><pre><code class="language-">meanRelError.([ŷbad, ŷpref],[y,y],normRec=true) ## Mean relative error</code></pre><pre><code class="language-">meanRelError.([ŷbad, ŷpref],[y,y],normRec=false) ## Relative mean error</code></pre><pre><code class="language-">plot([y ŷbad ŷpref], colour=[:black :red :green], label=[&quot;obs&quot; &quot;bad est&quot; &quot;good est&quot;])</code></pre><p>We can then compute the relative mean error for the decision tree</p><pre><code class="language-">(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)</code></pre><p>We can plot the true labels vs the estimated one for the three subsets...</p><pre><code class="language-">scatter(ytrain,ŷtrain,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in training period (DT)&quot;)</code></pre><pre><code class="language-">scatter(yval,ŷval,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in validation period (DT)&quot;)</code></pre><pre><code class="language-">scatter(ytest,ŷtest,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in testing period (DT)&quot;)</code></pre><p>Or we can visualise the true vs estimated bike shared on a temporal base. First on the full period (2 years) ...</p><pre><code class="language-">ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))
ŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))
ŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[&quot;obs&quot; &quot;train&quot; &quot;val&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period (DT)&quot;)</code></pre><p>..and then focusing on the testing period</p><pre><code class="language-">stc = 620
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[&quot;obs&quot; &quot;val&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (DT)&quot;)</code></pre><p>The predictions aren&#39;t so bad in this case, however decision trees are highly instable, and the output could have depended just from the specific initial random seed.</p><h2 id="Random-Forests"><a class="docs-heading-anchor" href="#Random-Forests">Random Forests</a><a id="Random-Forests-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Forests" title="Permalink"></a></h2><p>Rather than trying to solve this problem using a single Decision Tree model, let&#39;s not try to use a <em>Random Forest</em> model. Random forests average the results of many different decision trees and provide a more &quot;stable&quot; result. Being made of many decision trees, random forests are hovever more computationally expensive to train, but luckily they tend to self-tune (or self-regularise). In particular the default <code>maxDepth and</code>maxFeatures` shouldn&#39;t need tuning. We still tune however the model for other parameters, and in particular the β parameter, a prerogative of BetaML Random Forests that allows to assign more weigth to the best performing trees in the forest. It may be particularly important if there are many outliers in the data we don&#39;t want to &quot;learn&quot; from.</p><pre><code class="language-">minRecordsRange=[2,4,5]; nTreesRange=60:10:80; βRange=100:100:300
(bestRme,bestMaxDepth,bestMaxFeatures,bestMinRecords,bestNTrees,bestβ) = tuneHyperParameters(&quot;RandomForest&quot;,xtrain,ytrain,xval,yval,
        maxDepthRange=size(xtrain,1):size(xtrain,1),maxFeaturesRange=Int(round(sqrt(size(xtrain,2)))):Int(round(sqrt(size(xtrain,2)))),
        minRecordsRange=minRecordsRange,nTreesRange=nTreesRange,βRange=βRange,repetitions=5,rng=copy(FIXEDRNG))</code></pre><p>As for decision trees, once the hyper-parameters of the model are tuned we wan refit the model using the optimal parameters.</p><pre><code class="language-">myForest = buildForest(xtrain,ytrain, bestNTrees, maxDepth=bestMaxDepth,maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,β=bestβ,oob=true,rng=copy(FIXEDRNG));
nothing #hide</code></pre><p>Let&#39;s now benchmark of the training of BetaML Random Forest model</p><pre><code class="language-">@btime buildForest(xtrain,ytrain, bestNTrees, maxDepth=bestMaxDepth,maxFeatures=bestMaxFeatures,minRecords=bestMinRecords,β=bestβ,oob=true,rng=copy(FIXEDRNG));
nothing #hide</code></pre><p>Random forests are evidently slower than individual decision trees but are still relativly fast. We should also consider that they are by default efficiently parallelised, so their speed increases with the number of available cores (in building this documentation page, GitHub allows for a single core).</p><p>Random forests support the so-called &quot;out-of-bag&quot; error, an estimation of the error that we would have when the model is applied on a testing sample. However in this case the oob reported is much smaller than the testing error we will find. This is due to the fact that the division between training/validation and testing in this exercise is not random, but has a temporal basis. It seems that in this example the data in validation/testing follows a different pattern/variance than those in training (in probabilistic terms, they are not i.i.d.).</p><pre><code class="language-julia">oobError, trueTestMeanRelativeError  = myForest.oobError,meanRelError(ŷtest,ytest,normRec=true)</code></pre><pre><code class="language-">(ŷtrain,ŷval,ŷtest)         = predict.([myForest], [xtrain,xval,xtest])
(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)</code></pre><p>In this case we found an error very similar to the one employing a single decision tree. Let&#39;s print the observed data vs the estimated one using the random forest and then along the temporal axis:</p><pre><code class="language-">scatter(ytrain,ŷtrain,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in training period (RF)&quot;)</code></pre><pre><code class="language-">scatter(yval,ŷval,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in validation period (RF)&quot;)</code></pre><pre><code class="language-">scatter(ytest,ŷtest,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in testing period (RF)&quot;)</code></pre><p>Full period plot (2 years):</p><pre><code class="language-">ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))
ŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))
ŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[&quot;obs&quot; &quot;train&quot; &quot;val&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period (RF)&quot;)</code></pre><p>Focus on the testing period:</p><pre><code class="language-">stc = 620
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[&quot;obs&quot; &quot;val&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (RF)&quot;)</code></pre><h3 id="Comparison-with-DecisionTree.jl-random-forest"><a class="docs-heading-anchor" href="#Comparison-with-DecisionTree.jl-random-forest">Comparison with DecisionTree.jl random forest</a><a id="Comparison-with-DecisionTree.jl-random-forest-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison-with-DecisionTree.jl-random-forest" title="Permalink"></a></h3><p>We now compare our results with those obtained employing the same model in the <a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree package</a>, using the default suggested hyperparameters:</p><p>Hyperparameters of the DecisionTree.jl random forest model</p><pre><code class="language-">n_subfeatures=-1; n_trees=bestNTrees; partial_sampling=1; max_depth=26
min_samples_leaf=bestMinRecords; min_samples_split=bestMinRecords; min_purity_increase=0.0; seed=3</code></pre><p>We train the model..</p><pre><code class="language-">model = DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),
                     n_subfeatures,
                     n_trees,
                     partial_sampling,
                     max_depth,
                     min_samples_leaf,
                     min_samples_split,
                     min_purity_increase;
                     rng = seed)</code></pre><p>And we generate predictions and measure their error</p><pre><code class="language-">(ŷtrain,ŷval,ŷtest) = DecisionTree.apply_forest.([model],[xtrain,xval,xtest]);
nothing #hide</code></pre><p>Let&#39;s benchmark the DecisionTrees.jl Random Forest training</p><pre><code class="language-">@btime  DecisionTree.build_forest(ytrain, convert(Matrix,xtrain),
                     n_subfeatures,
                     n_trees,
                     partial_sampling,
                     max_depth,
                     min_samples_leaf,
                     min_samples_split,
                     min_purity_increase;
                     rng = seed);
nothing #hide</code></pre><p>DecisionTrees.jl makes a good job in optimising the Random Forest algorithm, as it is over 3 times faster that BetaML.</p><pre><code class="language-">(rmeTrain, rmeVal, rmeTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)</code></pre><p>However the error on the test set remains relativly high. The very low error level on the training set is a sign that it overspecialised on the training set, and we should have better ran a dedicated hyperparameter optimisation for the model.</p><p>Finally we plot the DecisionTree.jl predictions alongside the observed value:</p><pre><code class="language-">ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))
ŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))
ŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[&quot;obs&quot; &quot;train&quot; &quot;val&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period (DT.jl RF)&quot;)</code></pre><p>Again, focusing on the testing data:</p><pre><code class="language-">stc  = 620
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[&quot;obs&quot; &quot;val&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (DT.jl RF)&quot;)</code></pre><h3 id="Conclusions-of-Decision-Trees-/-Random-Forests-methods"><a class="docs-heading-anchor" href="#Conclusions-of-Decision-Trees-/-Random-Forests-methods">Conclusions of Decision Trees / Random Forests methods</a><a id="Conclusions-of-Decision-Trees-/-Random-Forests-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions-of-Decision-Trees-/-Random-Forests-methods" title="Permalink"></a></h3><p>The error obtained employing DecisionTree.jl is significantly larger than those obtained with the BetaML random forest model, altought to be fair with DecisionTrees.jl we didn&#39;t tuned its hyper-parameters. Also, DecisionTree.jl random forest model is much faster. This is partially due by the fact that internally DecisionTree.jl models optimise the algorithm by sorting the observations. BetaML trees/forests don&#39;t employ this optimisation and hence it can work with true categorical data for which ordering is not defined. An other explanation of this difference in speed is that BetaML Random Forest models accept <code>missing</code> values within the feature matrix. To sum up, BetaML random forests are ideal algorithms when we want to obtain good predictions in the most simpler way, even without tuning the hyperparameters, and without spending time in cleaning (&quot;munging&quot;) the feature matrix, as they accept almost &quot;any kind&quot; of data as it is.</p><h2 id="Neural-Networks"><a class="docs-heading-anchor" href="#Neural-Networks">Neural Networks</a><a id="Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Networks" title="Permalink"></a></h2><p>BetaML provides only <em>deep forward neural networks</em>, artificial neural network units where the individual &quot;nodes&quot; are arranged in <em>layers</em>, from the <em>input layer</em>, where each unit holds the input coordinate, through various <em>hidden layer</em> transformations, until the actual <em>output</em> of the model:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/MITx_6.86x/master/Unit 03%20-%20Neural%20networks/assets/nn_scheme.png &quot;Neural networks conceptual scheme&quot;" alt="Neural Networks"/></p><p>In this layerwise computation, each unit in a particular layer takes input from <em>all</em> the preceding layer units and it has its own parameters that are adjusted to perform the overall computation. The <em>training</em> of the network consists in retrieving the coefficients that minimise a <em>loss</em> function betwenn the output of the model and the known data. In particular, a <em>deep</em> (feedforward) neural network refers to a neural network that contains not only the input and output layers, but also hidden layers in between.</p><p>Neural networks accept only numerical inputs. We hence need to convert all categorical data in numerical units. A common approach is to use the so-called &quot;one-hot-encoding&quot; where the catagorical values are converted into indicator variables (0/1), one for each possible value. This can be done in BetaML using the <code>oneHotEncoder</code> function:</p><pre><code class="language-">seasonDummies  = convert(Array{Float64,2},oneHotEncoder(data[:,:season]))
weatherDummies = convert(Array{Float64,2},oneHotEncoder(data[:,:weathersit]))
wdayDummies    = convert(Array{Float64,2},oneHotEncoder(data[:,:weekday] .+ 1 ))

# We compose the feature matrix with the new dimensions obtained from the oneHotEncoder functions
x = hcat(Matrix{Float64}(data[:,[:instant,:yr,:mnth,:holiday,:workingday,:temp,:atemp,:hum,:windspeed]]),
         seasonDummies,
         weatherDummies,
         wdayDummies)
y = data[:,16];
nothing #hide</code></pre><p>As usual, we split the data in training, validation and testing sets</p><pre><code class="language-">((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],shuffle=false)
(ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)</code></pre><p>An other common operation with neural networks is to scale the feature vectors (X) and the labels (Y). The BetaML <code>scale()</code> function, by default, scale the data such that each dimension has mean 0 and variance 1. Note that we can provide the function with different scale factors or specify the columns not to scale (e.g. those resulting from the one-hot encoding). Finally we can reverse the scaling (this is useful to retrieve the unscaled features from a model trained with scaled ones).</p><pre><code class="language-">colsNotToScale = [2;4;5;10:23]
xScaleFactors   = getScaleFactors(xtrain,skip=colsNotToScale)
yScaleFactors   = ([0],[0.001]) # getScaleFactors(ytrain) # This just divide by 1000. Using full scaling of Y we may get negative demand.
xtrainScaled    = scale(xtrain,xScaleFactors)
xvalScaled      = scale(xval,xScaleFactors)
xtestScaled     = scale(xtest,xScaleFactors)
ytrainScaled    = scale(ytrain,yScaleFactors)
yvalScaled      = scale(yval,yScaleFactors)
ytestScaled     = scale(ytest,yScaleFactors)
D               = size(xtrain,2)</code></pre><p>As before, we select the best hyperparameters by using the validation set (it may take a while)...</p><pre><code class="language-">function tuneHyperParameters(xtrain,ytrain,xval,yval;epochRange=50:50,hiddenLayerSizeRange=12:12,repetitions=5,rng=Random.GLOBAL_RNG)
    # We start with an infinititly high error
    bestRme         = +Inf
    bestEpoch       = 0
    bestSize        = 0
    compLock        = ReentrantLock()

    # Generate one random number generator per thread
    masterSeed = rand(rng,100:9999999999999) ## Some RNG have problems with very small seed. Also, the master seed has to be computed _before_ generateParallelRngs
    rngs       = generateParallelRngs(rng,Threads.nthreads())

    # We loop over all possible hyperparameter combinations...
    parLengths = (length(epochRange),length(hiddenLayerSizeRange))
    Threads.@threads for ij in CartesianIndices(parLengths)
       (epoch,hiddenLayerSize)   = (epochRange[Tuple(ij)[1]], hiddenLayerSizeRange[Tuple(ij)[2]])
       tsrng = rngs[Threads.threadid()]
       joinedIndx = LinearIndices(parLengths)[ij]
       # And here we make the seeding depending on the i of the loop, not the thread: hence we get the same results indipendently of the number of threads
       Random.seed!(tsrng,masterSeed+joinedIndx*10)
       totAttemptError = 0.0
       println(&quot;Testing epochs $epoch, layer size $hiddenLayerSize ...&quot;)
       # We run several repetitions with the same hyperparameter combination to account for stochasticity...
       for r in 1:repetitions
           l1   = DenseLayer(D,hiddenLayerSize,f=relu,rng=tsrng) # Activation function is ReLU
           l2   = DenseLayer(hiddenLayerSize,hiddenLayerSize,f=identity,rng=tsrng)
           l3   = DenseLayer(hiddenLayerSize,1,f=relu,rng=tsrng)
           mynn = buildNetwork([l1,l2,l3],squaredCost,name=&quot;Bike sharing regression model&quot;) # Build the NN and use the squared cost (aka MSE) as error function
           # Training it (default to ADAM)
           res  = train!(mynn,xtrain,ytrain,epochs=epoch,batchSize=8,optAlg=ADAM(),verbosity=NONE, rng=tsrng) # Use optAlg=SGD() to use Stochastic Gradient Descent
           ŷval = predict(mynn,xval)
           rmeVal  = meanRelError(ŷval,yval,normRec=false)
           totAttemptError += rmeVal
       end
       avgRme = totAttemptError / repetitions
       begin
           lock(compLock) ## This step can&#39;t be run in parallel...
           try
               # Select this specific combination of hyperparameters if the error is the lowest
               if avgRme &lt; bestRme
                 bestRme    = avgRme
                 bestEpoch  = epoch
                 bestSize   = hiddenLayerSize
               end
           finally
               unlock(compLock)
           end
       end
    end
    return (bestRme=bestRme,bestEpoch=bestEpoch,bestSize=bestSize)
end

epochsToTest     = [100,400]
hiddenLayerSizes = [5,15,30]
(bestRme,bestEpoch,bestSize) = tuneHyperParameters(xtrainScaled,ytrainScaled,xvalScaled,yvalScaled;epochRange=epochsToTest,hiddenLayerSizeRange=hiddenLayerSizes,repetitions=3,rng=copy(FIXEDRNG))</code></pre><p>We now build our feed-forward neaural network. We create three layers, the first layers will always have a input size equal to the dimensions of our data (the number of columns), and the output layer, for a simple regression where the predictions are scalars, it will always be one. There are already several kind of layers available (and you can build your own kind by defining a new <code>struct</code> and implementing a few functions. See the <code>Nn</code> module documentation for details). Here we use only <em>dense</em> layers, those found in typycal feed-fordward neural networks. For each layer, on top of its size (in &quot;neurons&quot;) we can specify an <em>activation</em> function. Here we use the <code>relu</code> for the two terminal layers (this will guarantee that our predictions are always positive) and <code>identity</code> for the hidden layer. Again, consult the <code>Nn</code> module documentation for other activation layers already defined, or use any function of your choice. Initial weight parameters can also be specified if needed. By default <code>DenseLayer</code> use the so-called <em>Xavier initialisation</em>.</p><pre><code class="language-">l1   = DenseLayer(D,bestSize,f=relu,rng=copy(FIXEDRNG)) # Activation function is ReLU
l2   = DenseLayer(bestSize,bestSize,f=identity,rng=copy(FIXEDRNG))
l3   = DenseLayer(bestSize,1,f=relu,rng=copy(FIXEDRNG))</code></pre><p>Finally we &quot;chain&quot; the layer together and we assign a final loss function (agian, you can provide your own, if those available in BetaML don&#39;t suit your needs) in order to compose the &quot;neural network&quot; object.</p><pre><code class="language-">mynn = buildNetwork([l1,l2,l3],squaredCost,name=&quot;Bike sharing regression model&quot;) ## Build the NN and use the squared cost (aka MSE) as error function</code></pre><p>The above neural network will use automatic differentiation (using the <a href="tutorials/Regression - bike sharing/">Zygote</a> package) to compute the gradient to minimise in the training step. Using manual differentiaiton, for the layers that support it, is however really simple. The network below is exactly equivalent to the one above, except it avoids automatic differentiation:</p><pre><code class="language-">mynnManual = buildNetwork([
        DenseLayer(D,bestSize,f=relu,df=drelu,rng=copy(FIXEDRNG)),
        DenseLayer(bestSize,bestSize,f=identity,df=didentity,rng=copy(FIXEDRNG)),
        DenseLayer(bestSize,1,f=relu,df=drelu,rng=copy(FIXEDRNG))
    ], squaredCost, name=&quot;Bike sharing regression model&quot;, dcf=dSquaredCost)</code></pre><p>We can now re-do the training with the best hyperparameters. Several optimisation algorithms are available, and each accepts different parameters, like the <em>learning rate</em> for the Stochastic Gradient Descent algorithm (used by default) or the exponential decay rates for the  moments estimates for the ADAM algorithm (that we use here, with the default parameters).</p><pre><code class="language-">println(&quot;Final training of $bestEpoch epochs, with layer size $bestSize ...&quot;)
res  = train!(mynn,xtrainScaled,ytrainScaled,epochs=bestEpoch,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG)) ## Use optAlg=SGD() to use Stochastic Gradient Descent</code></pre><p>Let&#39;s benchmark the BetaML neural network training</p><pre><code class="language-">@btime train!(mynnManual,xtrainScaled,ytrainScaled,epochs=bestEpoch,batchSize=8,optAlg=ADAM(),rng=copy(FIXEDRNG), verbosity=NONE);
nothing #hide</code></pre><p>As we can see the model training is one order of magnitude slower than random forests, altought the memory requirement is approximatly the same</p><p>To obtain the neural network predictions we apply the function <code>predict</code> to the feature matrix X for which we want to generate previsions, and then, in order to obtain the <em>unscaled</em> unscaled estimates we use the <code>scale</code> function applied to the scaled values with the original scaling factors and the parameter <code>rev</code> set to <code>true</code>. Note the usage of the <em>pipe</em> operator to avoid ugly <code>function1(function2(function3(...)))</code> nested calls:</p><pre><code class="language-">ŷtrain = @pipe predict(mynn,xtrainScaled) |&gt; scale(_, yScaleFactors,rev=true);
ŷval   = @pipe predict(mynn,xvalScaled)   |&gt; scale(_, yScaleFactors,rev=true);
ŷtest  = @pipe predict(mynn,xtestScaled)  |&gt; scale(_ ,yScaleFactors,rev=true);
nothing #hide</code></pre><pre><code class="language-">(mreTrain, mreVal, mreTest) = meanRelError.([ŷtrain,ŷval,ŷtest],[ytrain,yval,ytest],normRec=false)</code></pre><p>The error is much lower. Let&#39;s plot our predictions:</p><p>Again, we can start by plotting the estimated vs the observed value:</p><pre><code class="language-">scatter(ytrain,ŷtrain,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in training period (NN)&quot;)</code></pre><pre><code class="language-">scatter(yval,ŷval,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in validation period (NN)&quot;)</code></pre><pre><code class="language-">scatter(ytest,ŷtest,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in testing period (NN)&quot;)</code></pre><p>We now plot across the time dimension, first plotting the whole period (2 years):</p><pre><code class="language-">ŷtrainfull = vcat(ŷtrain,fill(missing,nval+ntest))
ŷvalfull   = vcat(fill(missing,ntrain), ŷval, fill(missing,ntest))
ŷtestfull  = vcat(fill(missing,ntrain+nval), ŷtest)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfull ŷvalfull ŷtestfull], label=[&quot;obs&quot; &quot;train&quot; &quot;val&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period  (NN)&quot;)</code></pre><p>...and then focusing on the testing data</p><pre><code class="language-">stc  = 620
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfull[stc:endc] ŷtestfull[stc:endc]], label=[&quot;obs&quot; &quot;val&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (NN)&quot;)</code></pre><h3 id="Comparison-with-Flux"><a class="docs-heading-anchor" href="#Comparison-with-Flux">Comparison with Flux</a><a id="Comparison-with-Flux-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison-with-Flux" title="Permalink"></a></h3><p>We now to apply the same Neural Network model using the <a href="https://fluxml.ai/">Flux</a> framework, a dedicated neural network library. reusing the optimal parameters that we did found in <code>tuneHyperParameters</code></p><p>We fix the default random number generator so that the Flux example gives a reproducible output</p><pre><code class="language-">Random.seed!(123)</code></pre><p>We define the Flux neural network model and load it with data...</p><pre><code class="language-">l1         = Flux.Dense(D,bestSize,Flux.relu)
l2         = Flux.Dense(bestSize,bestSize,identity)
l3         = Flux.Dense(bestSize,1,Flux.relu)
Flux_nn    = Flux.Chain(l1,l2,l3)
loss(x, y) = Flux.mse(Flux_nn(x), y)
ps         = Flux.params(Flux_nn)
nndata     = Flux.Data.DataLoader((xtrainScaled&#39;, ytrainScaled&#39;), batchsize=8,shuffle=true)

Flux_nn2   = deepcopy(Flux_nn)      ## A copy for the time benchmarking
ps2        = Flux.params(Flux_nn2)  ## A copy for the time benchmarking</code></pre><p>We do the training of the Flux model...</p><pre><code class="language-">Flux.@epochs bestEpoch Flux.train!(loss, ps, nndata, Flux.ADAM(0.001, (0.9, 0.8)))</code></pre><p>..and we benchmark it..</p><pre><code class="language-">@btime begin for i in 1:bestEpoch Flux.train!(loss, ps2, nndata, Flux.ADAM(0.001, (0.9, 0.8))) end end</code></pre><p>On this small example the speed of Flux is on the same order than BetaML (the actual difference seems to depend on the specific RNG seed and hardware), however I suspect that Flux scales much better with larger networks and/or data.</p><p>We obtain the estimates...</p><pre><code class="language-">ŷtrainf = @pipe Flux_nn(xtrainScaled&#39;)&#39; |&gt; scale(_,yScaleFactors,rev=true);
ŷvalf   = @pipe Flux_nn(xvalScaled&#39;)&#39;   |&gt; scale(_,yScaleFactors,rev=true);
ŷtestf  = @pipe Flux_nn(xtestScaled&#39;)&#39;  |&gt; scale(_,yScaleFactors,rev=true);
nothing #hide</code></pre><p>..and we compute the mean relative errors..</p><pre><code class="language-">(mreTrain, mreVal, mreTest) = meanRelError.([ŷtrainf,ŷvalf,ŷtestf],[ytrain,yval,ytest],normRec=false)</code></pre><p>.. finding an error not significantly different than the one obtained from BetaML.Nn.</p><p>Plots:</p><pre><code class="language-">scatter(ytrain,ŷtrainf,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in training period (Flux.NN)&quot;)</code></pre><pre><code class="language-">scatter(yval,ŷvalf,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in validation period (Flux.NN)&quot;)</code></pre><pre><code class="language-">scatter(ytest,ŷtestf,xlabel=&quot;daily rides&quot;,ylabel=&quot;est. daily rides&quot;,label=nothing,title=&quot;Est vs. obs in testing period (Flux.NN)&quot;)</code></pre><pre><code class="language-">ŷtrainfullf = vcat(ŷtrainf,fill(missing,nval+ntest))
ŷvalfullf   = vcat(fill(missing,ntrain), ŷvalf, fill(missing,ntest))
ŷtestfullf  = vcat(fill(missing,ntrain+nval), ŷtestf)
plot(data[:,:dteday],[data[:,:cnt] ŷtrainfullf ŷvalfullf ŷtestfullf], label=[&quot;obs&quot; &quot;train&quot; &quot;val&quot; &quot;test&quot;], legend=:topleft, ylabel=&quot;daily rides&quot;, title=&quot;Daily bike sharing demand observed/estimated across the\n whole 2-years period (Flux.NN)&quot;)</code></pre><pre><code class="language-">stc = 620
endc = size(x,1)
plot(data[stc:endc,:dteday],[data[stc:endc,:cnt] ŷvalfullf[stc:endc] ŷtestfullf[stc:endc]], label=[&quot;obs&quot; &quot;val&quot; &quot;test&quot;], legend=:bottomleft, ylabel=&quot;Daily rides&quot;, title=&quot;Focus on the testing period (Flux.NN)&quot;)</code></pre><h3 id="Conclusions-of-Neural-Network-models"><a class="docs-heading-anchor" href="#Conclusions-of-Neural-Network-models">Conclusions of Neural Network models</a><a id="Conclusions-of-Neural-Network-models-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions-of-Neural-Network-models" title="Permalink"></a></h3><p>If we strive for the most accurate predictions, deep neural networks usually offer the best solution. However they are computationally expensive, so with limited resourses we may get better results by fine tuning and running many repetitions of &quot;simpler&quot; decision trees or even random forest models than a large naural network with insufficient hyperparameter tuning. Also, we shoudl consider that decision trees/random forests are much simple to work with.</p><p>That said, specialised neural network libraries, like Flux, allow to use GPU and specialised hardware letting neural networks to scale with very large datasets.</p><p>Still, for small and medium datasets, BetaML provides simpler yet customisable solutions that are accurate and fast.</p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This is the summary of the results we had trying to predict the daily bike sharing demand, given weather and calendar information of the day</p><table><tr><th style="text-align: left">Model</th><th style="text-align: center">Train rme</th><th style="text-align: right">Test rme</th><th style="text-align: right">Training time (ms)*</th><th style="text-align: right">Training mem (MB)</th></tr><tr><td style="text-align: left">DT</td><td style="text-align: center">0.1266</td><td style="text-align: right">0.2223</td><td style="text-align: right">26.5</td><td style="text-align: right">58</td></tr><tr><td style="text-align: left">RF</td><td style="text-align: center">0.0651</td><td style="text-align: right">0.2223</td><td style="text-align: right">362</td><td style="text-align: right">971</td></tr><tr><td style="text-align: left">RF (DecisionTree.jl)</td><td style="text-align: center">0.0312</td><td style="text-align: right">0.3142</td><td style="text-align: right">36</td><td style="text-align: right">11</td></tr><tr><td style="text-align: left">NN</td><td style="text-align: center">0.0884</td><td style="text-align: right">0.1761</td><td style="text-align: right">1768</td><td style="text-align: right">758</td></tr><tr><td style="text-align: left">NN (Flux.jl)</td><td style="text-align: center">0.0981</td><td style="text-align: right">0.1618</td><td style="text-align: right">1708</td><td style="text-align: right">282</td></tr></table><ul><li>on a Intel Core i5-8350U laptop</li></ul><p>Neural networks can be more precise than random forests models, but are more computationally expensive (and tricky to set up). When we compare BetaML with the algorithm-specific leading packages, we found similar results in terms of accuracy, but often the leading packages are better optimised and run more efficiently (but sometimes at the cost of being less verstatile).</p><p><a href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Regression - bike sharing/betaml_tutorial_regression_sharingBikes.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Getting started/betaml_tutorial_getting_started.html">« Getting started</a><a class="docs-footer-nextpage" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 5 April 2021 22:07">Monday 5 April 2021</span>. Using Julia version 1.6.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
