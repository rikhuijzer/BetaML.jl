<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A classification task: the prediction of  plant species from floreal measures (the iris tdataset) · BetaML.jl Documentation</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">BetaML.jl Documentation</span></div><form class="docs-search" action="../../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li><li><span class="tocitem">Tutorials</span><ul><li><input class="collapse-toggle" id="menuitem-7-1" type="checkbox"/><label class="tocitem" for="menuitem-7-1"><span class="docs-label">Getting started</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Getting started/betaml_tutorial_getting_started.html">-</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-2" type="checkbox"/><label class="tocitem" for="menuitem-7-2"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-3" type="checkbox"/><label class="tocitem" for="menuitem-7-3"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-4" type="checkbox" checked/><label class="tocitem" for="menuitem-7-4"><span class="docs-label">Clusterisation - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris tdataset)</a><ul class="internal"><li><a class="tocitem" href="#Library-and-data-loading"><span>Library and data loading</span></a></li><li class="toplevel"><a class="tocitem" href="###-Decision-Trees-and-Random-Forests"><span>## Decision Trees and Random Forests</span></a></li><li class="toplevel"><a class="tocitem" href="####-Data-preparation"><span>### Data preparation</span></a></li><li class="toplevel"><a class="tocitem" href="#The-first-step-is-to-prepare-the-data-for-the-analysis.-This-indeed-depends-already-on-the-model-we-want-to-employ,-as-some-models-&quot;accept&quot;-everything-as-input,-no-matter-if-the-data-is-numerical-or-categorical,-if-it-has-missing-values-or-not...-while-other-models-are-instead-much-more-exigents,-and-require-more-work-to-&quot;clean-up&quot;-our-dataset."><span>The first step is to prepare the data for the analysis. This indeed depends already on the model we want to employ, as some models &quot;accept&quot; everything as input, no matter if the data is numerical or categorical, if it has missing values or not... while other models are instead much more exigents, and require more work to &quot;clean up&quot; our dataset.</span></a></li><li class="toplevel"><a class="tocitem" href="#Here-we-start-using-Decision-Tree-and-Random-Forest-models-that-belong-to-the-first-group,-so-the-only-things-we-have-to-do-is-to-select-the-variables-in-input-(the-&quot;feature-matrix&quot;,-we-wil-lindicate-it-with-&quot;X&quot;)-and-those-representing-our-output-(the-values-we-want-to-learn-to-predict,-we-call-them-&quot;y&quot;):"><span>Here we start using  Decision Tree and Random Forest models that belong to the first group, so the only things we have to do is to select the variables in input (the &quot;feature matrix&quot;, we wil lindicate it with &quot;X&quot;) and those representing our output (the values we want to learn to predict, we call them &quot;y&quot;):</span></a></li><li class="toplevel"><a class="tocitem" href="#We-can-now-split-the-dataset-between-the-data-we-will-use-for-training-the-algorithm-(xtrain/ytrain),-those-for-selecting-the-hyperparameters-(xval/yval)-and-finally-those-for-testing-the-quality-of-the-algoritm-with-the-optimal-hyperparameters-(xtest/ytest).-We-use-the-partition-function-specifying-the-share-we-want-to-use-for-these-three-different-subsets,-here-75%,-12.5%-and-12.5-respectively.-As-the-dataset-is-shuffled-by-default,-to-obtain-replicable-results-we-call-partition-with-rngcopy(FIXEDRNG),-where-FIXEDRNG-is-a-fixed-seeded-random-number-generator-guaranteed-to-maintain-the-same-stream-of-random-numbers-even-between-different-julia-versions.-That&#39;s-also-what-we-use-for-our-unit-tests."><span>We can now split the dataset between the data we will use for training the algorithm (<code>xtrain</code>/<code>ytrain</code>), those for selecting the hyperparameters (<code>xval</code>/<code>yval</code>) and finally those for testing the quality of the algoritm with the optimal hyperparameters (<code>xtest</code>/<code>ytest</code>). We use the <code>partition</code> function specifying the share we want to use for these three different subsets, here 75%, 12.5% and 12.5 respectively. As the dataset is shuffled by default, to obtain replicable results we call <code>partition</code> with <code>rng=copy(FIXEDRNG)</code>, where <code>FIXEDRNG</code> is a fixed-seeded random number generator guaranteed to maintain the same stream of random numbers even between different julia versions. That&#39;s also what we use for our unit tests.</span></a></li></ul></li></ul></li></ul></li><li><a class="tocitem" href="../../Examples.html">Examples</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">Clusterisation - Iris</a></li><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris tdataset)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris tdataset)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="A-classification-task:-the-prediction-of-plant-species-from-floreal-measures-(the-iris-tdataset)"><a class="docs-heading-anchor" href="#A-classification-task:-the-prediction-of-plant-species-from-floreal-measures-(the-iris-tdataset)">A classification task: the prediction of  plant species from floreal measures (the iris tdataset)</a><a id="A-classification-task:-the-prediction-of-plant-species-from-floreal-measures-(the-iris-tdataset)-1"></a><a class="docs-heading-anchor-permalink" href="#A-classification-task:-the-prediction-of-plant-species-from-floreal-measures-(the-iris-tdataset)" title="Permalink"></a></h1><p>The task is to estimate the species of a plant given some floreal measurements. It is a very</p><p>Data origin:</p><ul><li>dataset description: <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris<em>flower</em>data_set</a></li><li>data source we use here: <a href="https://github.com/JuliaStats/RDatasets.jl">https://github.com/JuliaStats/RDatasets.jl</a></li></ul><p>Note that even if we are estimating a time serie, we are not using here a recurrent neural network as we assume the temporal dependence to be negligible (i.e. <span>$Y_t = f(X_t)$</span> alone).</p><h2 id="Library-and-data-loading"><a class="docs-heading-anchor" href="#Library-and-data-loading">Library and data loading</a><a id="Library-and-data-loading-1"></a><a class="docs-heading-anchor-permalink" href="#Library-and-data-loading" title="Permalink"></a></h2><pre><code class="language-julia">using LinearAlgebra, Random, Statistics, DataFrames, CSV, Plots, Pipe, BetaML, BenchmarkTools, RDatasets
import Distributions: Uniform
import DecisionTree, Flux ## For comparisions</code></pre><p>Differently from the <a href="tutorials/A%20regression%20task%20-%20sharing%20bike%20demand%20prediction/betaml_tutorial_regression_sharingBikes.html">regression tutorial</a>, we load the data here from <code>RDatasets</code>, a package providing standard datasets.</p><pre><code class="language-julia">iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
describe(iris)</code></pre><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Int64</th><th>DataType</th></tr></thead><tbody><p>5 rows × 7 columns</p><tr><th>1</th><td>SepalLength</td><td>5.84333</td><td>4.3</td><td>5.8</td><td>7.9</td><td>0</td><td>Float64</td></tr><tr><th>2</th><td>SepalWidth</td><td>3.05733</td><td>2.0</td><td>3.0</td><td>4.4</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>PetalLength</td><td>3.758</td><td>1.0</td><td>4.35</td><td>6.9</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>PetalWidth</td><td>1.19933</td><td>0.1</td><td>1.3</td><td>2.5</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>Species</td><td></td><td>setosa</td><td></td><td>virginica</td><td>0</td><td>CategoricalValue{String, UInt8}</td></tr></tbody></table><h1 id="-Decision-Trees-and-Random-Forests"><a class="docs-heading-anchor" href="###-Decision-Trees-and-Random-Forests">## Decision Trees and Random Forests</a><a id="-Decision-Trees-and-Random-Forests-1"></a><a class="docs-heading-anchor-permalink" href="###-Decision-Trees-and-Random-Forests" title="Permalink"></a></h1><h1 id="-Data-preparation"><a class="docs-heading-anchor" href="####-Data-preparation">### Data preparation</a><a id="-Data-preparation-1"></a><a class="docs-heading-anchor-permalink" href="####-Data-preparation" title="Permalink"></a></h1><h1 id="The-first-step-is-to-prepare-the-data-for-the-analysis.-This-indeed-depends-already-on-the-model-we-want-to-employ,-as-some-models-&quot;accept&quot;-everything-as-input,-no-matter-if-the-data-is-numerical-or-categorical,-if-it-has-missing-values-or-not...-while-other-models-are-instead-much-more-exigents,-and-require-more-work-to-&quot;clean-up&quot;-our-dataset."><a class="docs-heading-anchor" href="#The-first-step-is-to-prepare-the-data-for-the-analysis.-This-indeed-depends-already-on-the-model-we-want-to-employ,-as-some-models-&quot;accept&quot;-everything-as-input,-no-matter-if-the-data-is-numerical-or-categorical,-if-it-has-missing-values-or-not...-while-other-models-are-instead-much-more-exigents,-and-require-more-work-to-&quot;clean-up&quot;-our-dataset.">The first step is to prepare the data for the analysis. This indeed depends already on the model we want to employ, as some models &quot;accept&quot; everything as input, no matter if the data is numerical or categorical, if it has missing values or not... while other models are instead much more exigents, and require more work to &quot;clean up&quot; our dataset.</a><a id="The-first-step-is-to-prepare-the-data-for-the-analysis.-This-indeed-depends-already-on-the-model-we-want-to-employ,-as-some-models-&quot;accept&quot;-everything-as-input,-no-matter-if-the-data-is-numerical-or-categorical,-if-it-has-missing-values-or-not...-while-other-models-are-instead-much-more-exigents,-and-require-more-work-to-&quot;clean-up&quot;-our-dataset.-1"></a><a class="docs-heading-anchor-permalink" href="#The-first-step-is-to-prepare-the-data-for-the-analysis.-This-indeed-depends-already-on-the-model-we-want-to-employ,-as-some-models-&quot;accept&quot;-everything-as-input,-no-matter-if-the-data-is-numerical-or-categorical,-if-it-has-missing-values-or-not...-while-other-models-are-instead-much-more-exigents,-and-require-more-work-to-&quot;clean-up&quot;-our-dataset." title="Permalink"></a></h1><h1 id="Here-we-start-using-Decision-Tree-and-Random-Forest-models-that-belong-to-the-first-group,-so-the-only-things-we-have-to-do-is-to-select-the-variables-in-input-(the-&quot;feature-matrix&quot;,-we-wil-lindicate-it-with-&quot;X&quot;)-and-those-representing-our-output-(the-values-we-want-to-learn-to-predict,-we-call-them-&quot;y&quot;):"><a class="docs-heading-anchor" href="#Here-we-start-using-Decision-Tree-and-Random-Forest-models-that-belong-to-the-first-group,-so-the-only-things-we-have-to-do-is-to-select-the-variables-in-input-(the-&quot;feature-matrix&quot;,-we-wil-lindicate-it-with-&quot;X&quot;)-and-those-representing-our-output-(the-values-we-want-to-learn-to-predict,-we-call-them-&quot;y&quot;):">Here we start using  Decision Tree and Random Forest models that belong to the first group, so the only things we have to do is to select the variables in input (the &quot;feature matrix&quot;, we wil lindicate it with &quot;X&quot;) and those representing our output (the values we want to learn to predict, we call them &quot;y&quot;):</a><a id="Here-we-start-using-Decision-Tree-and-Random-Forest-models-that-belong-to-the-first-group,-so-the-only-things-we-have-to-do-is-to-select-the-variables-in-input-(the-&quot;feature-matrix&quot;,-we-wil-lindicate-it-with-&quot;X&quot;)-and-those-representing-our-output-(the-values-we-want-to-learn-to-predict,-we-call-them-&quot;y&quot;):-1"></a><a class="docs-heading-anchor-permalink" href="#Here-we-start-using-Decision-Tree-and-Random-Forest-models-that-belong-to-the-first-group,-so-the-only-things-we-have-to-do-is-to-select-the-variables-in-input-(the-&quot;feature-matrix&quot;,-we-wil-lindicate-it-with-&quot;X&quot;)-and-those-representing-our-output-(the-values-we-want-to-learn-to-predict,-we-call-them-&quot;y&quot;):" title="Permalink"></a></h1><p>x = Matrix{Float64}(iris[:,1:4]) y = Vector{String}(iris[:,5])</p><h1 id="We-can-now-split-the-dataset-between-the-data-we-will-use-for-training-the-algorithm-(xtrain/ytrain),-those-for-selecting-the-hyperparameters-(xval/yval)-and-finally-those-for-testing-the-quality-of-the-algoritm-with-the-optimal-hyperparameters-(xtest/ytest).-We-use-the-partition-function-specifying-the-share-we-want-to-use-for-these-three-different-subsets,-here-75%,-12.5%-and-12.5-respectively.-As-the-dataset-is-shuffled-by-default,-to-obtain-replicable-results-we-call-partition-with-rngcopy(FIXEDRNG),-where-FIXEDRNG-is-a-fixed-seeded-random-number-generator-guaranteed-to-maintain-the-same-stream-of-random-numbers-even-between-different-julia-versions.-That&#39;s-also-what-we-use-for-our-unit-tests."><a class="docs-heading-anchor" href="#We-can-now-split-the-dataset-between-the-data-we-will-use-for-training-the-algorithm-(xtrain/ytrain),-those-for-selecting-the-hyperparameters-(xval/yval)-and-finally-those-for-testing-the-quality-of-the-algoritm-with-the-optimal-hyperparameters-(xtest/ytest).-We-use-the-partition-function-specifying-the-share-we-want-to-use-for-these-three-different-subsets,-here-75%,-12.5%-and-12.5-respectively.-As-the-dataset-is-shuffled-by-default,-to-obtain-replicable-results-we-call-partition-with-rngcopy(FIXEDRNG),-where-FIXEDRNG-is-a-fixed-seeded-random-number-generator-guaranteed-to-maintain-the-same-stream-of-random-numbers-even-between-different-julia-versions.-That&#39;s-also-what-we-use-for-our-unit-tests.">We can now split the dataset between the data we will use for training the algorithm (<code>xtrain</code>/<code>ytrain</code>), those for selecting the hyperparameters (<code>xval</code>/<code>yval</code>) and finally those for testing the quality of the algoritm with the optimal hyperparameters (<code>xtest</code>/<code>ytest</code>). We use the <code>partition</code> function specifying the share we want to use for these three different subsets, here 75%, 12.5% and 12.5 respectively. As the dataset is shuffled by default, to obtain replicable results we call <code>partition</code> with <code>rng=copy(FIXEDRNG)</code>, where <code>FIXEDRNG</code> is a fixed-seeded random number generator guaranteed to maintain the same stream of random numbers even between different julia versions. That&#39;s also what we use for our unit tests.</a><a id="We-can-now-split-the-dataset-between-the-data-we-will-use-for-training-the-algorithm-(xtrain/ytrain),-those-for-selecting-the-hyperparameters-(xval/yval)-and-finally-those-for-testing-the-quality-of-the-algoritm-with-the-optimal-hyperparameters-(xtest/ytest).-We-use-the-partition-function-specifying-the-share-we-want-to-use-for-these-three-different-subsets,-here-75%,-12.5%-and-12.5-respectively.-As-the-dataset-is-shuffled-by-default,-to-obtain-replicable-results-we-call-partition-with-rngcopy(FIXEDRNG),-where-FIXEDRNG-is-a-fixed-seeded-random-number-generator-guaranteed-to-maintain-the-same-stream-of-random-numbers-even-between-different-julia-versions.-That&#39;s-also-what-we-use-for-our-unit-tests.-1"></a><a class="docs-heading-anchor-permalink" href="#We-can-now-split-the-dataset-between-the-data-we-will-use-for-training-the-algorithm-(xtrain/ytrain),-those-for-selecting-the-hyperparameters-(xval/yval)-and-finally-those-for-testing-the-quality-of-the-algoritm-with-the-optimal-hyperparameters-(xtest/ytest).-We-use-the-partition-function-specifying-the-share-we-want-to-use-for-these-three-different-subsets,-here-75%,-12.5%-and-12.5-respectively.-As-the-dataset-is-shuffled-by-default,-to-obtain-replicable-results-we-call-partition-with-rngcopy(FIXEDRNG),-where-FIXEDRNG-is-a-fixed-seeded-random-number-generator-guaranteed-to-maintain-the-same-stream-of-random-numbers-even-between-different-julia-versions.-That&#39;s-also-what-we-use-for-our-unit-tests." title="Permalink"></a></h1><p>((xtrain,xval,xtest),(ytrain,yval,ytest)) = partition([x,y],[0.75,0.125,1-0.75-0.125],rng=copy(FIXEDRNG)) (ntrain, nval, ntest) = size.([ytrain,yval,ytest],1)</p><p>clusterOut  = gmm(x,3,mixtures=[FullGaussian() for i in 1:3],minVariance=0.002,rng=copy(FIXEDRNG)) clusterOut.BIC accuracy(clusterOut.pₙₖ,y,ignoreLabels=true)</p><p>clusterOut  = kmeans(x2,3,rng=copy(FIXEDRNG)) accuracy(clusterOut[1],y,ignoreLabels=true)</p><p>clusterOut  = kmedoids(x2,3,rng=copy(FIXEDRNG)) accuracy(clusterOut[1],y,ignoreLabels=true)</p><p><a href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clusterisation - Iris/betaml_tutorial_cluster_iris.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Classification - cars/betaml_tutorial_classification_cars.html">« A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a><a class="docs-footer-nextpage" href="../../Examples.html">Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 2 April 2021 22:48">Friday 2 April 2021</span>. Using Julia version 1.6.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
