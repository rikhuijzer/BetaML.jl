<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A classification task: the prediction of  plant species from floreal measures (the iris dataset) · BetaML.jl Documentation</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">BetaML.jl Documentation</span></div><form class="docs-search" action="../../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../index.html">Index</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../Betaml_tutorial_getting_started.html">Getting started</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Regression - bike sharing</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html">A regression task: the prediction of  bike  sharing demand</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Classification - cars</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Classification - cars/betaml_tutorial_classification_cars.html">A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox" checked/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Clustering - Iris</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris dataset)</a><ul class="internal"><li><a class="tocitem" href="#Library-and-data-loading"><span>Library and data loading</span></a></li><li><a class="tocitem" href="#Data-preparation"><span>Data preparation</span></a></li><li><a class="tocitem" href="#Main-analysis"><span>Main analysis</span></a></li><li><a class="tocitem" href="#Working-without-the-labels"><span>Working without the labels</span></a></li><li><a class="tocitem" href="#Benchmarking-computational-efficiency"><span>Benchmarking computational efficiency</span></a></li><li><a class="tocitem" href="#Conclusions"><span>Conclusions</span></a></li></ul></li></ul></li></ul></li><li><span class="tocitem">API (Reference manual)</span><ul><li><a class="tocitem" href="../../Perceptron.html">Perceptron</a></li><li><a class="tocitem" href="../../Trees.html">Trees</a></li><li><a class="tocitem" href="../../Nn.html">Nn</a></li><li><a class="tocitem" href="../../Clustering.html">Clustering</a></li><li><a class="tocitem" href="../../Utils.html">Utils</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li><a class="is-disabled">Clustering - Iris</a></li><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="betaml_tutorial_cluster_iris.html">A classification task: the prediction of  plant species from floreal measures (the iris dataset)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="clustering_tutorial"><a class="docs-heading-anchor" href="#clustering_tutorial">A classification task: the prediction of  plant species from floreal measures (the iris dataset)</a><a id="clustering_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#clustering_tutorial" title="Permalink"></a></h1><p>The task is to estimate the species of a plant given some floreal measurements. It use the classical &quot;Iris&quot; dataset. Note that in this example we are using clustering approaches, so we try to understand the &quot;structure&quot; of our data, without relying to actually knowing the true labels (&quot;classes&quot; or &quot;factors&quot;). However we have chosen a dataset for which the true labels are actually known, so to compare the accuracy of the algorithms we use, but these labels will not be used during the algorithms training.</p><p>Data origin:</p><ul><li>dataset description: <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris<em>flower</em>data_set</a></li><li>data source we use here: <a href="https://github.com/JuliaStats/RDatasets.jl">https://github.com/JuliaStats/RDatasets.jl</a></li></ul><h2 id="Library-and-data-loading"><a class="docs-heading-anchor" href="#Library-and-data-loading">Library and data loading</a><a id="Library-and-data-loading-1"></a><a class="docs-heading-anchor-permalink" href="#Library-and-data-loading" title="Permalink"></a></h2><p>We load the Beta Machine Learning Toolkit as well as some other packages that we use in this tutorial</p><pre><code class="language-julia">using BetaML
using Random, Statistics, Logging, BenchmarkTools, RDatasets, Plots, DataFrames</code></pre><p>We are also going to compare our results with two other leading packages in Julia for clustering analysis, <a href="https://github.com/JuliaStats/Clustering.jl"><code>Clustering.jl</code></a> that provides (inter alia) kmeans and kmedoids algorithms and <a href="https://github.com/davidavdav/GaussianMixtures.jl"><code>GaussianMixtures.jl</code></a> that provides, as the name says, Gaussian Mixture Models. So we import them (we &quot;import&quot; them, rather than &quot;use&quot;, not to bound their full names into namespace as some would collide with BetaML).</p><pre><code class="language-julia">import Clustering, GaussianMixtures</code></pre><p>We do a few tweeks for the Clustering and GaussianMixtures packages. Note that in BetaML we can also control both the random seed and the verbosity in the algorithm call, not only globally</p><pre><code class="language-julia">Random.seed!(123)
#logger  = Logging.SimpleLogger(stdout, Logging.Error); global_logger(logger); ## For suppressing GaussianMixtures output</code></pre><p>Differently from the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>, we load the data here from [<code>RDatasets</code>](https://github.com/JuliaStats/RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl), a package providing standard datasets.</p><pre><code class="language-julia">iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
describe(iris)</code></pre><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Int64</th><th>DataType</th></tr></thead><tbody><p>5 rows × 7 columns</p><tr><th>1</th><td>SepalLength</td><td>5.84333</td><td>4.3</td><td>5.8</td><td>7.9</td><td>0</td><td>Float64</td></tr><tr><th>2</th><td>SepalWidth</td><td>3.05733</td><td>2.0</td><td>3.0</td><td>4.4</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>PetalLength</td><td>3.758</td><td>1.0</td><td>4.35</td><td>6.9</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>PetalWidth</td><td>1.19933</td><td>0.1</td><td>1.3</td><td>2.5</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>Species</td><td></td><td>setosa</td><td></td><td>virginica</td><td>0</td><td>CategoricalValue{String, UInt8}</td></tr></tbody></table><p>The iris dataset  provides floreal measures in columns 1 to 4 and the assigned species name in column 5. There are no missing values</p><h2 id="Data-preparation"><a class="docs-heading-anchor" href="#Data-preparation">Data preparation</a><a id="Data-preparation-1"></a><a class="docs-heading-anchor-permalink" href="#Data-preparation" title="Permalink"></a></h2><p>The first step is to prepare the data for the analysis. We collect the first 4 columns as our <em>feature</em> <code>x</code> matrix and the last one as our <code>y</code> label vector. As we are using clustering algorithms, we are not actually using the labels to train the algorithms, we&#39;ll behave like we do not know them, we&#39;ll just let the algorithm &quot;learn&quot; fro mthe structure of the data itself. We&#39;ll however use it to judge the accuracy that they did reach.</p><pre><code class="language-julia">x       = Matrix{Float64}(iris[:,1:4]);
yLabels = unique(iris[:,5]);</code></pre><p>As the labels are expressed as strings, the first thing we do is encode them as integers for our analysis using the function <a href="../../Utils.html#BetaML.Utils.integerEncoder-Tuple{AbstractVector{T} where T}"><code>integerEncoder</code></a>.</p><pre><code class="language-julia">y       = integerEncoder(iris[:,5],factors=yLabels);</code></pre><p>The dataset from RDatasets is ordered by species, so we need to shuffle it to avoid biases. Shuffling happens by default in crossValidation, but we are keeping here a copy of the shuffled version for later. Note that the version of <a href="../../Utils.html#Random.shuffle-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:AbstractArray"><code>shuffle</code></a> that is included in BetaML accepts several n-dimensional arrays and shuffle them (by default on rows, by we can specify the dimension) keeping the association  between the various arrays in the shuffled output.</p><pre><code class="language-julia">(xs,ys) = shuffle([x,y]);</code></pre><h2 id="Main-analysis"><a class="docs-heading-anchor" href="#Main-analysis">Main analysis</a><a id="Main-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Main-analysis" title="Permalink"></a></h2><p>We will try 3 BetaML models (<a href="../../Clustering.html#BetaML.Clustering.kmeans-Tuple{Any, Any}"><code>kmeans</code></a>, <a href="../../Clustering.html#BetaML.Clustering.kmedoids-Tuple{Any, Any}"><code>kmedoids</code></a> and <a href="../../Clustering.html#BetaML.Clustering.gmm-Tuple{Any, Any}"><code>gmm</code></a>) and we compare them with <code>kmeans</code> from Clusterings.jl and <code>GMM</code> from GaussianMixtures.jl <code>Kmeans</code> and <code>kmedoids</code> works by first initialising the centers of the k-clusters (the &quot;representative&quot; (step a ) . For <code>kmeans</code> they must be selected within one of the data, for kmeans they are the geometrical center) n a nutshell. Then ( b ) iterate for each point to assign the point to the cluster of the closest representative (according with a user defined distance metric, default to Euclidean), and ( c ) move each representative at the center of its newly acquired cluster (where &quot;center&quot; depends again from the metric). Steps ( b ) and ( c ) are reiterated until the algorithm converge, i.e. the tentative k representative points (and their relative clusters) don&#39;t move any more. The result (output of the algorithm) is that each point is assigned to one of the clusters (classes). The <code>gmm</code> algorithm is similar in that it employs an iterative approach (the Expectation<em>Minimisation algorithm, &quot;em&quot;) but here we make the hipothesis that the data points are the observed outcomes of some _mixture</em> probabilistic models where we have first a k-categorical variables whose outcomes are the (unobservble) parameters of a probabilistic distribution from which the data is finally drawn. Because the parameters of each of the k-possible distributions is unobservable this is also called a model with latent variables. Most <code>gmm</code> models use the Gaussain distribution as the family of the mixture components, so we can tought the <code>gmm</code> acronym to indicate <em>Gaussian Mixture Model</em>. In BetaML we do implemented only Gaussain components, but any distribution could be used by just subclassing <code>AbstractMixture</code> and implementing a couple of methids (you are invited to contribute or just ask for a distribution family you are interested), so I prefer to think &quot;gmm&quot; as an acronym for <em>Generative Mixture Model</em>. The algorithm try to find the mixture that maximises the likelihood that the data has been generated indeed from such mixture, where the &quot;E&quot; step refers to computing the probability that each point belongs to each of the k-composants (somehow similar to the step <em>b</em> in the kmeans/kmedoids algorithm), and the &quot;M&quot; step estimates, giving the association probabilities in step &quot;M&quot;, the parameters of the mixture and of the individual components (similar to step <em>c</em>). The result here is that each point has a categorical distribution (PMF) representing the probabilities that it belongs to any of the k-components (our classes or clusters). This is interesting, as <code>gmm</code> can be used for many other things that clustering. It forms the backbone of the <a href="../../Clustering.html#BetaML.Clustering.predictMissing"><code>predictMissing</code></a> function to impute missing values (on some or all dimensions) based to how close the record seems to its pears. For the same reasons, <code>predictMissing</code> can also be used to predict user&#39;s behaviours (or users&#39; appreciation) according to the behaviour/ranking made by pears (&quot;collaborative filtering&quot;). While the result of <code>gmm</code> is a vector of PMFs (one for each record), error measures and reports with the true values (if known) can be directly applied, as in BetaML they internally call <code>mode()</code> to retrieve the class with the highest probability for each record.</p><p>As we are here, we also try different versions of the BetaML models, even if the default &quot;versions&quot; should be fine. For <code>kmeans</code> and <code>kmedoids</code> we will try different initialisation strategies (&quot;gird&quot;, the default one, &quot;random&quot; and &quot;shuffle&quot;), while for the <code>gmm</code> model we&#39;ll choose different distributions of the Gaussain family (<code>SphericalGaussian</code> - where the variance is a scalar, <code>DiagonalGaussian</code> - with a vector variance, and <code>FullGaussian</code>, where the covariance is a matrix).</p><p>As the result would depend on stochasticity both in the data selected and in the random initialisation, we use a cross-validation approach to run our models several times (with different data) and then we average their results. Cross-Validation in BetaML is very flexible and it is done using the <a href="../../Utils.html#BetaML.Utils.crossValidation"><code>crossValidation</code></a> function. crossValidation works by calling the function <code>f</code>, defined by the user, passing to it the tuple <code>trainData</code>, <code>valData</code> and <code>rng</code> and collecting the result of the function f. The specific method for which <code>trainData</code>, and <code>valData</code> are selected at each iteration depends on the specific <code>sampler</code>. We start by selectign a k-fold sampler that split our data in 5 different parts, it uses 4 for training and 1 part (not used here) for validation. We run the simulations twice and, to be sure to have replicable results, we fix the random seed (at the whole crossValidaiton level, not on each iteration).</p><pre><code class="language-julia">sampler = KFold(nSplits=5,nRepeats=3,shuffle=true, rng=copy(FIXEDRNG))</code></pre><pre class="documenter-example-output">KFold(5, 3, true, StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7))</pre><p>We can now run the cross-validation with our models. Note that instead of defining the function <code>f</code> and then calling <code>crossValidation[f(trainData,testData,rng),[x,y],...)</code> we use the Julia <code>do</code> block syntax and we write directly the content of the <code>f</code> function in the <code>do</code> block. Also, by default crossValidation already returns the mean and the standard deviation of the output of the user-provided <code>f</code> function (or the <code>do</code> block). However this requires that the <code>f</code> function return a single scalar. Here we are returning a vector of the accuracies of the different models (so we can run the cross-validation only once), and hence we indicate with <code>returnStatistics=false</code> to crossValidation not to attempt to generate statistics but rather report the whole output. We&#39;ll compute the statistics ex-post.</p><p>Inside the <code>do</code> block we do 4 things:</p><ul><li>we recover from <code>trainData</code> (a tuple, as we passed a tuple to <code>crossValidation</code> too) the <code>xtrain</code> features and <code>ytrain</code> labels;</li><li>we run the various clustering algorithms</li><li>we use the real labels to compute the model accuracy. Note that the clustering algorithm know nothing about the specific label name or even their order. This is why <a href="../../Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T"><code>accuracy</code></a> has the parameter <code>ignoreLabels</code> to compute the accuracy oven any possible permutation of the classes found.</li><li>we return the various models&#39; accuracies</li></ul><pre><code class="language-julia">cOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng
          # For unsupervised learning we use only the train data.
          # Also, we use the associated labels only to measure the performances
         (xtrain,ytrain)  = trainData;
         # We run the clustering algorithm...
         clusteringOut     = kmeans(xtrain,3,rng=rng) ## init is grid by default
         # ... and we compute the accuracy using the real labels
         kMeansAccuracy    = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=&quot;random&quot;)
         kMeansRAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmeans(xtrain,3,rng=rng,initStrategy=&quot;shuffle&quot;)
         kMeansSAccuracy   = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmedoids(xtrain,3,rng=rng)   ## init is grid by default
         kMedoidsAccuracy  = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=&quot;random&quot;)
         kMedoidsRAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = kmedoids(xtrain,3,rng=rng,initStrategy=&quot;shuffle&quot;)
         kMedoidsSAccuracy = accuracy(clusteringOut[1],ytrain,ignoreLabels=true)
         clusteringOut     = gmm(xtrain,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE, rng=rng)
         gmmSpherAccuracy  = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)
         clusteringOut     = gmm(xtrain,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE, rng=rng)
         gmmDiagAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)
         clusteringOut     = gmm(xtrain,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE, rng=rng)
         gmmFullAccuracy   = accuracy(clusteringOut.pₙₖ,ytrain,ignoreLabels=true, rng=rng)
         # For comparision with Clustering.jl
         clusteringOut     = Clustering.kmeans(xtrain&#39;, 3)
         kMeans2Accuracy   = accuracy(clusteringOut.assignments,ytrain,ignoreLabels=true)
         # For comparision with GaussianMistures.jl - sometimes GaussianMistures.jl em! fails with a PosDefException
         dGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:diag)
         GaussianMixtures.em!(dGMM, xtrain)
         gmmDiag2Accuracy  = accuracy(GaussianMixtures.gmmposterior(dGMM, xtrain)[1],ytrain,ignoreLabels=true)
         fGMM              = GaussianMixtures.GMM(3, xtrain; method=:kmeans, kind=:full)
         GaussianMixtures.em!(fGMM, xtrain)
         gmmFull2Accuracy  = accuracy(GaussianMixtures.gmmposterior(fGMM, xtrain)[1],ytrain,ignoreLabels=true)
         # Returning the accuracies
         return kMeansAccuracy,kMeansRAccuracy,kMeansSAccuracy,kMedoidsAccuracy,kMedoidsRAccuracy,kMedoidsSAccuracy,gmmSpherAccuracy,gmmDiagAccuracy,gmmFullAccuracy,kMeans2Accuracy,gmmDiag2Accuracy,gmmFull2Accuracy
 end

# We transform the output in matrix for easier analysis
accuracies = fill(0.0,(length(cOut),length(cOut[1])))
[accuracies[r,c] = cOut[r][c] for r in 1:length(cOut),c in 1:length(cOut[1])]
μs = mean(accuracies,dims=1)
σs = std(accuracies,dims=1)


modelLabels=[&quot;kMeansG&quot;,&quot;kMeansR&quot;,&quot;kMeansS&quot;,&quot;kMedoidsG&quot;,&quot;kMedoidsR&quot;,&quot;kMedoidsS&quot;,&quot;gmmSpher&quot;,&quot;gmmDiag&quot;,&quot;gmmFull&quot;,&quot;kMeans (Clustering.jl)&quot;,&quot;gmmDiag (GaussianMixtures.jl)&quot;,&quot;gmmFull (GaussianMixtures.jl)&quot;]
report = DataFrame(mName = modelLabels, avgAccuracy = dropdims(round.(μs&#39;,digits=3),dims=2), stdAccuracy = dropdims(round.(σs&#39;,digits=3),dims=2))</code></pre><table class="data-frame"><thead><tr><th></th><th>mName</th><th>avgAccuracy</th><th>stdAccuracy</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>12 rows × 3 columns</p><tr><th>1</th><td>kMeansG</td><td>0.891</td><td>0.017</td></tr><tr><th>2</th><td>kMeansR</td><td>0.866</td><td>0.083</td></tr><tr><th>3</th><td>kMeansS</td><td>0.764</td><td>0.174</td></tr><tr><th>4</th><td>kMedoidsG</td><td>0.894</td><td>0.015</td></tr><tr><th>5</th><td>kMedoidsR</td><td>0.804</td><td>0.144</td></tr><tr><th>6</th><td>kMedoidsS</td><td>0.893</td><td>0.018</td></tr><tr><th>7</th><td>gmmSpher</td><td>0.893</td><td>0.016</td></tr><tr><th>8</th><td>gmmDiag</td><td>0.917</td><td>0.022</td></tr><tr><th>9</th><td>gmmFull</td><td>0.97</td><td>0.035</td></tr><tr><th>10</th><td>kMeans (Clustering.jl)</td><td>0.858</td><td>0.11</td></tr><tr><th>11</th><td>gmmDiag (GaussianMixtures.jl)</td><td>0.882</td><td>0.096</td></tr><tr><th>12</th><td>gmmFull (GaussianMixtures.jl)</td><td>0.942</td><td>0.079</td></tr></tbody></table><h3 id="BetaML-model-accuracies"><a class="docs-heading-anchor" href="#BetaML-model-accuracies">BetaML model accuracies</a><a id="BetaML-model-accuracies-1"></a><a class="docs-heading-anchor-permalink" href="#BetaML-model-accuracies" title="Permalink"></a></h3><p>From the output We see that the gmm models perform for this dataset generally better than kmeans or kmedoids algorithms, also with very low variances. In detail, it is the (default) <code>grid</code> initialisation that leads to the better results for <code>kmeans</code> and <code>kmedoids</code>, while for the <code>gmm</code> models it is the <code>FullGaussian</code> to perform better.</p><h3 id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl"><a class="docs-heading-anchor" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl">Comparisions with <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code></a><a id="Comparisions-with-Clustering.jl-and-GaussianMixtures.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Comparisions-with-Clustering.jl-and-GaussianMixtures.jl" title="Permalink"></a></h3><p>For this specific case, both <code>Clustering.jl</code> and <code>GaussianMixtures.jl</code> report substantially worst accuracies, and with very high variances. But we maintain the ranking that Full Gaussian gmm &gt; Diagonal Gaussian &gt; Kmeans accuracy. I suspect the reason that BetaML gmm works so weel is in relation to the usage of kmeans algorithm with itself the grid initialisation. The grid initialisation &quot;guarantee&quot; indeed that the initial means of the mixture components are well spread across the multidimensional space defined by the data, and it helps avoiding the EM algoritm to converge to a bad local optimus.</p><h2 id="Working-without-the-labels"><a class="docs-heading-anchor" href="#Working-without-the-labels">Working without the labels</a><a id="Working-without-the-labels-1"></a><a class="docs-heading-anchor-permalink" href="#Working-without-the-labels" title="Permalink"></a></h2><p>Up to now we used the real labels to compare the model accuracies. But in real clustering examples we don&#39;t have the true classes, or we wouln&#39;t need to do clustering in the first instance, so we don&#39;t know the number of classes to use. There are several methods to judge clusters algorithms goodness, perhaps the simplest one, at least for the expectation-maximisation algorithm employed in <code>gmm</code> to fit the data to the unknown mixture, is to use a information criteria that trade the goodness of the lickelyhood with the parameters used to do the fit. BetaML provide by default in the gmm clustering outputs both the <em>Bayesian information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.bic-Tuple{Any, Any, Any}"><code>BIC</code></a>) and the <em>Akaike information criterion</em>  (<a href="../../Utils.html#BetaML.Utils.aic-Tuple{Any, Any}"><code>AIC</code></a>), where for both a lower value is better.</p><p>We can then run the model with different number of classes and see which one leads to the lower BIC or AIC. We run hence <code>crossValidation</code> again with the <code>FullGaussian</code> gmm model Note that we use the BIC/AIC criteria here for establishing the &quot;best&quot; number of classes but we could have used it also to select the kind of Gaussain distribution to use. This is one example of hyper-parameter tuning that we developed more in detail (but without using cross-validation) in the <a href="../Regression - bike sharing/betaml_tutorial_regression_sharingBikes.html#regression_tutorial">regression tutorial</a>.</p><p>Let&#39;s try up to 8 possible classes:</p><pre><code class="language-julia">K = 8
sampler = KFold(nSplits=5,nRepeats=2,shuffle=true, rng=copy(FIXEDRNG))
cOut = crossValidation([x,y],sampler,returnStatistics=false) do trainData,testData,rng
    (xtrain,ytrain)  = trainData;
    clusteringOut  = [gmm(xtrain,k,mixtures=[FullGaussian() for i in 1:k], verbosity=NONE, rng=rng) for k in 1:K]
    BICS           = [clusteringOut[i].BIC for i in 1:K]
    AICS           = [clusteringOut[i].AIC for i in 1:K]
    return (BICS,AICS)
end

# Transforming the output in matrices for easier analysis
Nit = length(cOut)

BICS = fill(0.0,(Nit,K))
AICS = fill(0.0,(Nit,K))
[BICS[r,c] = cOut[r][1][c] for r in 1:Nit,c in 1:K]
[AICS[r,c] = cOut[r][2][c] for r in 1:Nit,c in 1:K]

μsBICS = mean(BICS,dims=1)</code></pre><pre class="documenter-example-output">1×8 Matrix{Float64}:
 762.112  516.031  539.392  593.272  640.82  702.342  766.786  819.531</pre><pre><code class="language-julia">σsBICS = std(BICS,dims=1)</code></pre><pre class="documenter-example-output">1×8 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026  11.3803  21.0664  13.3093  25.2232</pre><pre><code class="language-julia">μsAICS = mean(AICS,dims=1)</code></pre><pre class="documenter-example-output">1×8 Matrix{Float64}:
 723.087  435.194  416.743  428.81  434.546  454.255  476.887  487.82</pre><pre><code class="language-julia">σsAICS = std(AICS,dims=1)</code></pre><pre class="documenter-example-output">1×8 Matrix{Float64}:
 12.2912  15.8085  17.7181  24.6026  11.3803  21.0664  13.3093  25.2232</pre><pre><code class="language-julia">plot(1:K,[μsBICS&#39; μsAICS&#39;], labels=[&quot;BIC&quot; &quot;AIC&quot;], title=&quot;Information criteria by number of classes&quot;, xlabel=&quot;number of classes&quot;, ylabel=&quot;lower is better&quot;)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip920">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip920)" d="
M0 1600 L2400 1600 L2400 0 L0 0  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip921">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip920)" d="
M102.74 1505.26 L2352.76 1505.26 L2352.76 62.9921 L102.74 62.9921  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip922">
    <rect x="102" y="62" width="2251" height="1443"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip922)" style="stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  469.656,1505.26 469.656,62.9921 
  "/>
<polyline clip-path="url(#clip922)" style="stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1076.13,1505.26 1076.13,62.9921 
  "/>
<polyline clip-path="url(#clip922)" style="stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1682.6,1505.26 1682.6,62.9921 
  "/>
<polyline clip-path="url(#clip922)" style="stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  2289.08,1505.26 2289.08,62.9921 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  102.74,1505.26 2352.76,1505.26 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  469.656,1505.26 469.656,1487.95 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  1076.13,1505.26 1076.13,1487.95 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  1682.6,1505.26 1682.6,1487.95 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  2289.08,1505.26 2289.08,1487.95 
  "/>
<path clip-path="url(#clip920)" d="M 0 0 M464.309 1562 L480.629 1562 L480.629 1565.93 L458.684 1565.93 L458.684 1562 Q461.346 1559.24 465.93 1554.61 Q470.536 1549.96 471.717 1548.62 Q473.962 1546.09 474.842 1544.36 Q475.744 1542.6 475.744 1540.91 Q475.744 1538.15 473.8 1536.42 Q471.879 1534.68 468.777 1534.68 Q466.578 1534.68 464.124 1535.45 Q461.694 1536.21 458.916 1537.76 L458.916 1533.04 Q461.74 1531.9 464.194 1531.33 Q466.647 1530.75 468.684 1530.75 Q474.055 1530.75 477.249 1533.43 Q480.443 1536.12 480.443 1540.61 Q480.443 1542.74 479.633 1544.66 Q478.846 1546.56 476.74 1549.15 Q476.161 1549.82 473.059 1553.04 Q469.957 1556.23 464.309 1562 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1079.14 1535.45 L1067.33 1553.89 L1079.14 1553.89 L1079.14 1535.45 M1077.91 1531.37 L1083.79 1531.37 L1083.79 1553.89 L1088.72 1553.89 L1088.72 1557.78 L1083.79 1557.78 L1083.79 1565.93 L1079.14 1565.93 L1079.14 1557.78 L1063.54 1557.78 L1063.54 1553.27 L1077.91 1531.37 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1683.01 1546.79 Q1679.86 1546.79 1678.01 1548.94 Q1676.18 1551.09 1676.18 1554.84 Q1676.18 1558.57 1678.01 1560.75 Q1679.86 1562.9 1683.01 1562.9 Q1686.16 1562.9 1687.98 1560.75 Q1689.84 1558.57 1689.84 1554.84 Q1689.84 1551.09 1687.98 1548.94 Q1686.16 1546.79 1683.01 1546.79 M1692.29 1532.14 L1692.29 1536.39 Q1690.53 1535.56 1688.73 1535.12 Q1686.94 1534.68 1685.18 1534.68 Q1680.55 1534.68 1678.1 1537.81 Q1675.67 1540.93 1675.32 1547.25 Q1676.69 1545.24 1678.75 1544.17 Q1680.81 1543.08 1683.29 1543.08 Q1688.49 1543.08 1691.5 1546.26 Q1694.54 1549.4 1694.54 1554.84 Q1694.54 1560.17 1691.39 1563.39 Q1688.24 1566.6 1683.01 1566.6 Q1677.01 1566.6 1673.84 1562.02 Q1670.67 1557.41 1670.67 1548.69 Q1670.67 1540.49 1674.56 1535.63 Q1678.45 1530.75 1685 1530.75 Q1686.76 1530.75 1688.54 1531.09 Q1690.35 1531.44 1692.29 1532.14 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M2289.08 1549.52 Q2285.74 1549.52 2283.82 1551.3 Q2281.92 1553.08 2281.92 1556.21 Q2281.92 1559.33 2283.82 1561.12 Q2285.74 1562.9 2289.08 1562.9 Q2292.41 1562.9 2294.33 1561.12 Q2296.25 1559.31 2296.25 1556.21 Q2296.25 1553.08 2294.33 1551.3 Q2292.43 1549.52 2289.08 1549.52 M2284.4 1547.53 Q2281.39 1546.79 2279.7 1544.73 Q2278.03 1542.67 2278.03 1539.71 Q2278.03 1535.56 2280.97 1533.15 Q2283.94 1530.75 2289.08 1530.75 Q2294.24 1530.75 2297.18 1533.15 Q2300.12 1535.56 2300.12 1539.71 Q2300.12 1542.67 2298.43 1544.73 Q2296.76 1546.79 2293.78 1547.53 Q2297.15 1548.32 2299.03 1550.61 Q2300.93 1552.9 2300.93 1556.21 Q2300.93 1561.23 2297.85 1563.92 Q2294.79 1566.6 2289.08 1566.6 Q2283.36 1566.6 2280.28 1563.92 Q2277.22 1561.23 2277.22 1556.21 Q2277.22 1552.9 2279.12 1550.61 Q2281.02 1548.32 2284.4 1547.53 M2282.69 1540.14 Q2282.69 1542.83 2284.35 1544.33 Q2286.04 1545.84 2289.08 1545.84 Q2292.09 1545.84 2293.78 1544.33 Q2295.49 1542.83 2295.49 1540.14 Q2295.49 1537.46 2293.78 1535.96 Q2292.09 1534.45 2289.08 1534.45 Q2286.04 1534.45 2284.35 1535.96 Q2282.69 1537.46 2282.69 1540.14 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M964.876 1628.61 L964.876 1650.12 L959.02 1650.12 L959.02 1628.8 Q959.02 1623.74 957.046 1621.22 Q955.073 1618.71 951.126 1618.71 Q946.384 1618.71 943.647 1621.73 Q940.909 1624.76 940.909 1629.98 L940.909 1650.12 L935.021 1650.12 L935.021 1614.48 L940.909 1614.48 L940.909 1620.01 Q943.01 1616.8 945.843 1615.21 Q948.707 1613.62 952.431 1613.62 Q958.574 1613.62 961.725 1617.44 Q964.876 1621.22 964.876 1628.61 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M975.953 1636.06 L975.953 1614.48 L981.809 1614.48 L981.809 1635.83 Q981.809 1640.89 983.782 1643.44 Q985.756 1645.95 989.702 1645.95 Q994.445 1645.95 997.182 1642.93 Q999.951 1639.91 999.951 1634.69 L999.951 1614.48 L1005.81 1614.48 L1005.81 1650.12 L999.951 1650.12 L999.951 1644.65 Q997.819 1647.9 994.986 1649.49 Q992.185 1651.05 988.461 1651.05 Q982.318 1651.05 979.135 1647.23 Q975.953 1643.41 975.953 1636.06 M990.689 1613.62 L990.689 1613.62 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1045.63 1621.32 Q1047.82 1617.37 1050.88 1615.49 Q1053.93 1613.62 1058.07 1613.62 Q1063.64 1613.62 1066.66 1617.53 Q1069.69 1621.41 1069.69 1628.61 L1069.69 1650.12 L1063.8 1650.12 L1063.8 1628.8 Q1063.8 1623.67 1061.99 1621.19 Q1060.17 1618.71 1056.45 1618.71 Q1051.9 1618.71 1049.25 1621.73 Q1046.61 1624.76 1046.61 1629.98 L1046.61 1650.12 L1040.72 1650.12 L1040.72 1628.8 Q1040.72 1623.64 1038.91 1621.19 Q1037.1 1618.71 1033.31 1618.71 Q1028.82 1618.71 1026.18 1621.76 Q1023.54 1624.79 1023.54 1629.98 L1023.54 1650.12 L1017.65 1650.12 L1017.65 1614.48 L1023.54 1614.48 L1023.54 1620.01 Q1025.54 1616.74 1028.34 1615.18 Q1031.14 1613.62 1034.99 1613.62 Q1038.88 1613.62 1041.58 1615.59 Q1044.32 1617.56 1045.63 1621.32 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1106.96 1632.33 Q1106.96 1625.87 1104.29 1622.21 Q1101.64 1618.52 1097 1618.52 Q1092.35 1618.52 1089.68 1622.21 Q1087.03 1625.87 1087.03 1632.33 Q1087.03 1638.79 1089.68 1642.48 Q1092.35 1646.15 1097 1646.15 Q1101.64 1646.15 1104.29 1642.48 Q1106.96 1638.79 1106.96 1632.33 M1087.03 1619.89 Q1088.88 1616.7 1091.68 1615.18 Q1094.51 1613.62 1098.43 1613.62 Q1104.92 1613.62 1108.96 1618.77 Q1113.04 1623.93 1113.04 1632.33 Q1113.04 1640.73 1108.96 1645.89 Q1104.92 1651.05 1098.43 1651.05 Q1094.51 1651.05 1091.68 1649.52 Q1088.88 1647.96 1087.03 1644.78 L1087.03 1650.12 L1081.15 1650.12 L1081.15 1600.6 L1087.03 1600.6 L1087.03 1619.89 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1153.24 1630.84 L1153.24 1633.7 L1126.31 1633.7 Q1126.69 1639.75 1129.94 1642.93 Q1133.22 1646.08 1139.04 1646.08 Q1142.42 1646.08 1145.57 1645.25 Q1148.75 1644.43 1151.87 1642.77 L1151.87 1648.31 Q1148.72 1649.65 1145.41 1650.35 Q1142.1 1651.05 1138.69 1651.05 Q1130.16 1651.05 1125.16 1646.08 Q1120.2 1641.12 1120.2 1632.65 Q1120.2 1623.9 1124.91 1618.77 Q1129.65 1613.62 1137.67 1613.62 Q1144.87 1613.62 1149.04 1618.26 Q1153.24 1622.88 1153.24 1630.84 M1147.38 1629.12 Q1147.32 1624.31 1144.68 1621.45 Q1142.07 1618.58 1137.74 1618.58 Q1132.84 1618.58 1129.88 1621.35 Q1126.95 1624.12 1126.5 1629.15 L1147.38 1629.12 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1183.51 1619.95 Q1182.52 1619.38 1181.34 1619.12 Q1180.2 1618.84 1178.8 1618.84 Q1173.83 1618.84 1171.16 1622.08 Q1168.52 1625.3 1168.52 1631.35 L1168.52 1650.12 L1162.63 1650.12 L1162.63 1614.48 L1168.52 1614.48 L1168.52 1620.01 Q1170.36 1616.77 1173.32 1615.21 Q1176.28 1613.62 1180.51 1613.62 Q1181.12 1613.62 1181.85 1613.71 Q1182.58 1613.78 1183.47 1613.93 L1183.51 1619.95 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1224.18 1618.58 Q1219.47 1618.58 1216.74 1622.27 Q1214 1625.93 1214 1632.33 Q1214 1638.73 1216.7 1642.42 Q1219.44 1646.08 1224.18 1646.08 Q1228.86 1646.08 1231.6 1642.39 Q1234.34 1638.7 1234.34 1632.33 Q1234.34 1626 1231.6 1622.31 Q1228.86 1618.58 1224.18 1618.58 M1224.18 1613.62 Q1231.82 1613.62 1236.18 1618.58 Q1240.54 1623.55 1240.54 1632.33 Q1240.54 1641.08 1236.18 1646.08 Q1231.82 1651.05 1224.18 1651.05 Q1216.51 1651.05 1212.15 1646.08 Q1207.82 1641.08 1207.82 1632.33 Q1207.82 1623.55 1212.15 1618.58 Q1216.51 1613.62 1224.18 1613.62 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1268.3 1600.6 L1268.3 1605.47 L1262.7 1605.47 Q1259.54 1605.47 1258.3 1606.74 Q1257.09 1608.01 1257.09 1611.32 L1257.09 1614.48 L1266.74 1614.48 L1266.74 1619.03 L1257.09 1619.03 L1257.09 1650.12 L1251.21 1650.12 L1251.21 1619.03 L1245.6 1619.03 L1245.6 1614.48 L1251.21 1614.48 L1251.21 1611.99 Q1251.21 1606.04 1253.97 1603.34 Q1256.74 1600.6 1262.76 1600.6 L1268.3 1600.6 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1319.57 1615.84 L1319.57 1621.32 Q1317.09 1619.95 1314.58 1619.28 Q1312.09 1618.58 1309.55 1618.58 Q1303.85 1618.58 1300.7 1622.21 Q1297.55 1625.81 1297.55 1632.33 Q1297.55 1638.86 1300.7 1642.48 Q1303.85 1646.08 1309.55 1646.08 Q1312.09 1646.08 1314.58 1645.41 Q1317.09 1644.71 1319.57 1643.34 L1319.57 1648.76 Q1317.12 1649.9 1314.48 1650.47 Q1311.87 1651.05 1308.91 1651.05 Q1300.86 1651.05 1296.12 1645.99 Q1291.37 1640.93 1291.37 1632.33 Q1291.37 1623.61 1296.15 1618.61 Q1300.95 1613.62 1309.29 1613.62 Q1312 1613.62 1314.58 1614.19 Q1317.15 1614.73 1319.57 1615.84 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1329.76 1600.6 L1335.61 1600.6 L1335.61 1650.12 L1329.76 1650.12 L1329.76 1600.6 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1364.07 1632.2 Q1356.97 1632.2 1354.23 1633.83 Q1351.5 1635.45 1351.5 1639.37 Q1351.5 1642.48 1353.53 1644.33 Q1355.6 1646.15 1359.14 1646.15 Q1364.01 1646.15 1366.93 1642.71 Q1369.89 1639.24 1369.89 1633.51 L1369.89 1632.2 L1364.07 1632.2 M1375.75 1629.79 L1375.75 1650.12 L1369.89 1650.12 L1369.89 1644.71 Q1367.89 1647.96 1364.9 1649.52 Q1361.91 1651.05 1357.58 1651.05 Q1352.1 1651.05 1348.86 1647.99 Q1345.64 1644.9 1345.64 1639.75 Q1345.64 1633.73 1349.65 1630.68 Q1353.69 1627.62 1361.68 1627.62 L1369.89 1627.62 L1369.89 1627.05 Q1369.89 1623.01 1367.22 1620.81 Q1364.58 1618.58 1359.77 1618.58 Q1356.72 1618.58 1353.82 1619.31 Q1350.92 1620.05 1348.25 1621.51 L1348.25 1616.1 Q1351.47 1614.86 1354.49 1614.25 Q1357.51 1613.62 1360.38 1613.62 Q1368.11 1613.62 1371.93 1617.63 Q1375.75 1621.64 1375.75 1629.79 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1410.54 1615.53 L1410.54 1621.06 Q1408.06 1619.79 1405.38 1619.15 Q1402.71 1618.52 1399.84 1618.52 Q1395.48 1618.52 1393.29 1619.85 Q1391.12 1621.19 1391.12 1623.87 Q1391.12 1625.9 1392.68 1627.08 Q1394.24 1628.23 1398.95 1629.28 L1400.96 1629.72 Q1407.2 1631.06 1409.81 1633.51 Q1412.45 1635.93 1412.45 1640.29 Q1412.45 1645.25 1408.5 1648.15 Q1404.59 1651.05 1397.71 1651.05 Q1394.85 1651.05 1391.73 1650.47 Q1388.64 1649.93 1385.2 1648.82 L1385.2 1642.77 Q1388.45 1644.46 1391.6 1645.32 Q1394.75 1646.15 1397.84 1646.15 Q1401.98 1646.15 1404.21 1644.74 Q1406.43 1643.31 1406.43 1640.73 Q1406.43 1638.35 1404.81 1637.07 Q1403.22 1635.8 1397.78 1634.62 L1395.74 1634.15 Q1390.3 1633 1387.88 1630.64 Q1385.46 1628.26 1385.46 1624.12 Q1385.46 1619.09 1389.02 1616.35 Q1392.59 1613.62 1399.14 1613.62 Q1402.39 1613.62 1405.26 1614.09 Q1408.12 1614.57 1410.54 1615.53 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1444.5 1615.53 L1444.5 1621.06 Q1442.02 1619.79 1439.34 1619.15 Q1436.67 1618.52 1433.81 1618.52 Q1429.45 1618.52 1427.25 1619.85 Q1425.08 1621.19 1425.08 1623.87 Q1425.08 1625.9 1426.64 1627.08 Q1428.2 1628.23 1432.91 1629.28 L1434.92 1629.72 Q1441.16 1631.06 1443.77 1633.51 Q1446.41 1635.93 1446.41 1640.29 Q1446.41 1645.25 1442.46 1648.15 Q1438.55 1651.05 1431.67 1651.05 Q1428.81 1651.05 1425.69 1650.47 Q1422.6 1649.93 1419.16 1648.82 L1419.16 1642.77 Q1422.41 1644.46 1425.56 1645.32 Q1428.71 1646.15 1431.8 1646.15 Q1435.94 1646.15 1438.17 1644.74 Q1440.39 1643.31 1440.39 1640.73 Q1440.39 1638.35 1438.77 1637.07 Q1437.18 1635.8 1431.74 1634.62 L1429.7 1634.15 Q1424.26 1633 1421.84 1630.64 Q1419.42 1628.26 1419.42 1624.12 Q1419.42 1619.09 1422.98 1616.35 Q1426.55 1613.62 1433.11 1613.62 Q1436.35 1613.62 1439.22 1614.09 Q1442.08 1614.57 1444.5 1615.53 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1486.23 1630.84 L1486.23 1633.7 L1459.3 1633.7 Q1459.68 1639.75 1462.93 1642.93 Q1466.21 1646.08 1472.03 1646.08 Q1475.41 1646.08 1478.56 1645.25 Q1481.74 1644.43 1484.86 1642.77 L1484.86 1648.31 Q1481.71 1649.65 1478.4 1650.35 Q1475.09 1651.05 1471.68 1651.05 Q1463.15 1651.05 1458.15 1646.08 Q1453.19 1641.12 1453.19 1632.65 Q1453.19 1623.9 1457.9 1618.77 Q1462.64 1613.62 1470.66 1613.62 Q1477.86 1613.62 1482.03 1618.26 Q1486.23 1622.88 1486.23 1630.84 M1480.37 1629.12 Q1480.31 1624.31 1477.67 1621.45 Q1475.06 1618.58 1470.73 1618.58 Q1465.83 1618.58 1462.87 1621.35 Q1459.94 1624.12 1459.49 1629.15 L1480.37 1629.12 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1518.57 1615.53 L1518.57 1621.06 Q1516.08 1619.79 1513.41 1619.15 Q1510.74 1618.52 1507.87 1618.52 Q1503.51 1618.52 1501.31 1619.85 Q1499.15 1621.19 1499.15 1623.87 Q1499.15 1625.9 1500.71 1627.08 Q1502.27 1628.23 1506.98 1629.28 L1508.98 1629.72 Q1515.22 1631.06 1517.83 1633.51 Q1520.48 1635.93 1520.48 1640.29 Q1520.48 1645.25 1516.53 1648.15 Q1512.61 1651.05 1505.74 1651.05 Q1502.87 1651.05 1499.75 1650.47 Q1496.67 1649.93 1493.23 1648.82 L1493.23 1642.77 Q1496.48 1644.46 1499.63 1645.32 Q1502.78 1646.15 1505.87 1646.15 Q1510 1646.15 1512.23 1644.74 Q1514.46 1643.31 1514.46 1640.73 Q1514.46 1638.35 1512.84 1637.07 Q1511.24 1635.8 1505.8 1634.62 L1503.77 1634.15 Q1498.32 1633 1495.9 1630.64 Q1493.48 1628.26 1493.48 1624.12 Q1493.48 1619.09 1497.05 1616.35 Q1500.61 1613.62 1507.17 1613.62 Q1510.42 1613.62 1513.28 1614.09 Q1516.15 1614.57 1518.57 1615.53 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip922)" style="stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  102.74,1183.2 2352.76,1183.2 
  "/>
<polyline clip-path="url(#clip922)" style="stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  102.74,845.394 2352.76,845.394 
  "/>
<polyline clip-path="url(#clip922)" style="stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  102.74,507.591 2352.76,507.591 
  "/>
<polyline clip-path="url(#clip922)" style="stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  102.74,169.789 2352.76,169.789 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  102.74,1505.26 102.74,62.9921 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  102.74,1183.2 129.74,1183.2 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  102.74,845.394 129.74,845.394 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  102.74,507.591 129.74,507.591 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  102.74,169.789 129.74,169.789 
  "/>
<path clip-path="url(#clip920)" d="M 0 0 M-15.4816 1165.92 L2.87476 1165.92 L2.87476 1169.85 L-11.1992 1169.85 L-11.1992 1178.32 Q-10.1807 1177.98 -9.16221 1177.81 Q-8.1437 1177.63 -7.12519 1177.63 Q-1.33818 1177.63 2.04143 1180.8 Q5.42104 1183.97 5.42104 1189.39 Q5.42104 1194.97 1.94884 1198.07 Q-1.52337 1201.15 -7.84278 1201.15 Q-10.0187 1201.15 -12.2872 1200.78 Q-14.5326 1200.41 -16.9399 1199.67 L-16.9399 1194.97 Q-14.8566 1196.1 -12.6344 1196.66 Q-10.4122 1197.21 -7.93537 1197.21 Q-3.93076 1197.21 -1.59281 1195.11 Q0.745141 1193 0.745141 1189.39 Q0.745141 1185.78 -1.59281 1183.67 Q-3.93076 1181.56 -7.93537 1181.56 Q-9.81036 1181.56 -11.6853 1181.98 Q-13.5372 1182.4 -15.4816 1183.28 L-15.4816 1165.92 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M24.6339 1168.99 Q21.0228 1168.99 19.1941 1172.56 Q17.3886 1176.1 17.3886 1183.23 Q17.3886 1190.34 19.1941 1193.9 Q21.0228 1197.44 24.6339 1197.44 Q28.2681 1197.44 30.0737 1193.9 Q31.9024 1190.34 31.9024 1183.23 Q31.9024 1176.1 30.0737 1172.56 Q28.2681 1168.99 24.6339 1168.99 M24.6339 1165.29 Q30.4441 1165.29 33.4996 1169.9 Q36.5783 1174.48 36.5783 1183.23 Q36.5783 1191.96 33.4996 1196.56 Q30.4441 1201.15 24.6339 1201.15 Q18.8237 1201.15 15.7451 1196.56 Q12.6895 1191.96 12.6895 1183.23 Q12.6895 1174.48 15.7451 1169.9 Q18.8237 1165.29 24.6339 1165.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M54.7958 1168.99 Q51.1847 1168.99 49.356 1172.56 Q47.5504 1176.1 47.5504 1183.23 Q47.5504 1190.34 49.356 1193.9 Q51.1847 1197.44 54.7958 1197.44 Q58.43 1197.44 60.2356 1193.9 Q62.0643 1190.34 62.0643 1183.23 Q62.0643 1176.1 60.2356 1172.56 Q58.43 1168.99 54.7958 1168.99 M54.7958 1165.29 Q60.6059 1165.29 63.6615 1169.9 Q66.7402 1174.48 66.7402 1183.23 Q66.7402 1191.96 63.6615 1196.56 Q60.6059 1201.15 54.7958 1201.15 Q48.9856 1201.15 45.9069 1196.56 Q42.8514 1191.96 42.8514 1183.23 Q42.8514 1174.48 45.9069 1169.9 Q48.9856 1165.29 54.7958 1165.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-4.94927 843.53 Q-8.0974 843.53 -9.94925 845.683 Q-11.7779 847.836 -11.7779 851.586 Q-11.7779 855.312 -9.94925 857.488 Q-8.0974 859.641 -4.94927 859.641 Q-1.80114 859.641 0.027552 857.488 Q1.87939 855.312 1.87939 851.586 Q1.87939 847.836 0.027552 845.683 Q-1.80114 843.53 -4.94927 843.53 M4.33308 828.877 L4.33308 833.137 Q2.57383 832.303 0.768289 831.863 Q-1.01411 831.424 -2.77336 831.424 Q-7.40296 831.424 -9.85665 834.549 Q-12.2872 837.674 -12.6344 843.993 Q-11.2687 841.979 -9.20851 840.914 Q-7.14834 839.826 -4.6715 839.826 Q0.536809 839.826 3.54605 842.998 Q6.57844 846.146 6.57844 851.586 Q6.57844 856.91 3.43031 860.127 Q0.28218 863.345 -4.94927 863.345 Q-10.9446 863.345 -14.1159 858.761 Q-17.2872 854.155 -17.2872 845.428 Q-17.2872 837.234 -13.3983 832.373 Q-9.50943 827.489 -2.95854 827.489 Q-1.19929 827.489 0.583105 827.836 Q2.38865 828.183 4.33308 828.877 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M24.6339 831.192 Q21.0228 831.192 19.1941 834.757 Q17.3886 838.299 17.3886 845.428 Q17.3886 852.535 19.1941 856.099 Q21.0228 859.641 24.6339 859.641 Q28.2681 859.641 30.0737 856.099 Q31.9024 852.535 31.9024 845.428 Q31.9024 838.299 30.0737 834.757 Q28.2681 831.192 24.6339 831.192 M24.6339 827.489 Q30.4441 827.489 33.4996 832.095 Q36.5783 836.678 36.5783 845.428 Q36.5783 854.155 33.4996 858.761 Q30.4441 863.345 24.6339 863.345 Q18.8237 863.345 15.7451 858.761 Q12.6895 854.155 12.6895 845.428 Q12.6895 836.678 15.7451 832.095 Q18.8237 827.489 24.6339 827.489 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M54.7958 831.192 Q51.1847 831.192 49.356 834.757 Q47.5504 838.299 47.5504 845.428 Q47.5504 852.535 49.356 856.099 Q51.1847 859.641 54.7958 859.641 Q58.43 859.641 60.2356 856.099 Q62.0643 852.535 62.0643 845.428 Q62.0643 838.299 60.2356 834.757 Q58.43 831.192 54.7958 831.192 M54.7958 827.489 Q60.6059 827.489 63.6615 832.095 Q66.7402 836.678 66.7402 845.428 Q66.7402 854.155 63.6615 858.761 Q60.6059 863.345 54.7958 863.345 Q48.9856 863.345 45.9069 858.761 Q42.8514 854.155 42.8514 845.428 Q42.8514 836.678 45.9069 832.095 Q48.9856 827.489 54.7958 827.489 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-16.7085 490.311 L5.51363 490.311 L5.51363 492.302 L-7.0326 524.871 L-11.9168 524.871 L-0.111336 494.246 L-16.7085 494.246 L-16.7085 490.311 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M24.6339 493.39 Q21.0228 493.39 19.1941 496.955 Q17.3886 500.496 17.3886 507.626 Q17.3886 514.732 19.1941 518.297 Q21.0228 521.839 24.6339 521.839 Q28.2681 521.839 30.0737 518.297 Q31.9024 514.732 31.9024 507.626 Q31.9024 500.496 30.0737 496.955 Q28.2681 493.39 24.6339 493.39 M24.6339 489.686 Q30.4441 489.686 33.4996 494.293 Q36.5783 498.876 36.5783 507.626 Q36.5783 516.353 33.4996 520.959 Q30.4441 525.542 24.6339 525.542 Q18.8237 525.542 15.7451 520.959 Q12.6895 516.353 12.6895 507.626 Q12.6895 498.876 15.7451 494.293 Q18.8237 489.686 24.6339 489.686 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M54.7958 493.39 Q51.1847 493.39 49.356 496.955 Q47.5504 500.496 47.5504 507.626 Q47.5504 514.732 49.356 518.297 Q51.1847 521.839 54.7958 521.839 Q58.43 521.839 60.2356 518.297 Q62.0643 514.732 62.0643 507.626 Q62.0643 500.496 60.2356 496.955 Q58.43 493.39 54.7958 493.39 M54.7958 489.686 Q60.6059 489.686 63.6615 494.293 Q66.7402 498.876 66.7402 507.626 Q66.7402 516.353 63.6615 520.959 Q60.6059 525.542 54.7958 525.542 Q48.9856 525.542 45.9069 520.959 Q42.8514 516.353 42.8514 507.626 Q42.8514 498.876 45.9069 494.293 Q48.9856 489.686 54.7958 489.686 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-5.52797 170.657 Q-8.86129 170.657 -10.7826 172.439 Q-12.6807 174.222 -12.6807 177.347 Q-12.6807 180.472 -10.7826 182.254 Q-8.86129 184.036 -5.52797 184.036 Q-2.19466 184.036 -0.273372 182.254 Q1.64791 180.448 1.64791 177.347 Q1.64791 174.222 -0.273372 172.439 Q-2.17151 170.657 -5.52797 170.657 M-10.2039 168.666 Q-13.2131 167.925 -14.9029 165.865 Q-16.5696 163.805 -16.5696 160.842 Q-16.5696 156.699 -13.6298 154.291 Q-10.6668 151.884 -5.52797 151.884 Q-0.365964 151.884 2.57383 154.291 Q5.51363 156.699 5.51363 160.842 Q5.51363 163.805 3.82383 165.865 Q2.15717 167.925 -0.828925 168.666 Q2.55069 169.453 4.42568 171.745 Q6.32381 174.036 6.32381 177.347 Q6.32381 182.37 3.24513 185.055 Q0.189588 187.74 -5.52797 187.74 Q-11.2455 187.74 -14.3242 185.055 Q-17.3798 182.37 -17.3798 177.347 Q-17.3798 174.036 -15.4816 171.745 Q-13.5835 169.453 -10.2039 168.666 M-11.9168 161.282 Q-11.9168 163.967 -10.2502 165.472 Q-8.56036 166.976 -5.52797 166.976 Q-2.51873 166.976 -0.828925 165.472 Q0.884029 163.967 0.884029 161.282 Q0.884029 158.597 -0.828925 157.092 Q-2.51873 155.587 -5.52797 155.587 Q-8.56036 155.587 -10.2502 157.092 Q-11.9168 158.597 -11.9168 161.282 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M24.6339 155.587 Q21.0228 155.587 19.1941 159.152 Q17.3886 162.694 17.3886 169.824 Q17.3886 176.93 19.1941 180.495 Q21.0228 184.036 24.6339 184.036 Q28.2681 184.036 30.0737 180.495 Q31.9024 176.93 31.9024 169.824 Q31.9024 162.694 30.0737 159.152 Q28.2681 155.587 24.6339 155.587 M24.6339 151.884 Q30.4441 151.884 33.4996 156.49 Q36.5783 161.074 36.5783 169.824 Q36.5783 178.55 33.4996 183.157 Q30.4441 187.74 24.6339 187.74 Q18.8237 187.74 15.7451 183.157 Q12.6895 178.55 12.6895 169.824 Q12.6895 161.074 15.7451 156.49 Q18.8237 151.884 24.6339 151.884 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M54.7958 155.587 Q51.1847 155.587 49.356 159.152 Q47.5504 162.694 47.5504 169.824 Q47.5504 176.93 49.356 180.495 Q51.1847 184.036 54.7958 184.036 Q58.43 184.036 60.2356 180.495 Q62.0643 176.93 62.0643 169.824 Q62.0643 162.694 60.2356 159.152 Q58.43 155.587 54.7958 155.587 M54.7958 151.884 Q60.6059 151.884 63.6615 156.49 Q66.7402 161.074 66.7402 169.824 Q66.7402 178.55 63.6615 183.157 Q60.6059 187.74 54.7958 187.74 Q48.9856 187.74 45.9069 183.157 Q42.8514 178.55 42.8514 169.824 Q42.8514 161.074 45.9069 156.49 Q48.9856 151.884 54.7958 151.884 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-117.161 1016.67 L-117.161 1010.81 L-67.6358 1010.81 L-67.6358 1016.67 L-117.161 1016.67 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-99.1778 984.741 Q-99.1778 989.452 -95.4857 992.189 Q-91.8254 994.926 -85.4279 994.926 Q-79.0304 994.926 -75.3383 992.221 Q-71.678 989.484 -71.678 984.741 Q-71.678 980.062 -75.3701 977.325 Q-79.0622 974.588 -85.4279 974.588 Q-91.7618 974.588 -95.4539 977.325 Q-99.1778 980.062 -99.1778 984.741 M-104.143 984.741 Q-104.143 977.102 -99.1778 972.742 Q-94.2126 968.381 -85.4279 968.381 Q-76.6751 968.381 -71.678 972.742 Q-66.7127 977.102 -66.7127 984.741 Q-66.7127 992.412 -71.678 996.772 Q-76.6751 1001.1 -85.4279 1001.1 Q-94.2126 1001.1 -99.1778 996.772 Q-104.143 992.412 -104.143 984.741 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-103.284 962.079 L-103.284 956.223 L-75.4656 948.902 L-103.284 941.614 L-103.284 934.707 L-75.4656 927.386 L-103.284 920.097 L-103.284 914.241 L-67.6358 923.567 L-67.6358 930.474 L-96.8544 938.144 L-67.6358 945.847 L-67.6358 952.754 L-103.284 962.079 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-86.9239 874.869 L-84.0593 874.869 L-84.0593 901.796 Q-78.0119 901.414 -74.829 898.168 Q-71.678 894.889 -71.678 889.065 Q-71.678 885.691 -72.5055 882.54 Q-73.3331 879.357 -74.9882 876.238 L-69.45 876.238 Q-68.1132 879.389 -67.413 882.699 Q-66.7127 886.009 -66.7127 889.415 Q-66.7127 897.945 -71.678 902.942 Q-76.6432 907.907 -85.1096 907.907 Q-93.8625 907.907 -98.9869 903.197 Q-104.143 898.454 -104.143 890.433 Q-104.143 883.24 -99.4961 879.07 Q-94.881 874.869 -86.9239 874.869 M-88.6426 880.726 Q-93.4487 880.789 -96.3133 883.431 Q-99.1778 886.041 -99.1778 890.37 Q-99.1778 895.271 -96.4088 898.231 Q-93.6397 901.16 -88.6108 901.605 L-88.6426 880.726 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-97.8092 844.6 Q-98.3821 845.587 -98.6368 846.765 Q-98.9232 847.91 -98.9232 849.311 Q-98.9232 854.276 -95.6767 856.95 Q-92.462 859.591 -86.4146 859.591 L-67.6358 859.591 L-67.6358 865.48 L-103.284 865.48 L-103.284 859.591 L-97.7456 859.591 Q-100.992 857.745 -102.552 854.785 Q-104.143 851.825 -104.143 847.592 Q-104.143 846.987 -104.048 846.255 Q-103.984 845.523 -103.825 844.632 L-97.8092 844.6 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-103.284 817.737 L-103.284 811.88 L-67.6358 811.88 L-67.6358 817.737 L-103.284 817.737 M-117.161 817.737 L-117.161 811.88 L-109.745 811.88 L-109.745 817.737 L-117.161 817.737 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-102.233 776.901 L-96.6952 776.901 Q-97.9684 779.384 -98.6049 782.057 Q-99.2415 784.731 -99.2415 787.595 Q-99.2415 791.956 -97.9047 794.152 Q-96.5679 796.316 -93.8943 796.316 Q-91.8573 796.316 -90.6796 794.757 Q-89.5338 793.197 -88.4835 788.486 L-88.0379 786.481 Q-86.7011 780.243 -84.2503 777.633 Q-81.8313 774.991 -77.4708 774.991 Q-72.5055 774.991 -69.6091 778.938 Q-66.7127 782.853 -66.7127 789.728 Q-66.7127 792.592 -67.2856 795.712 Q-67.8267 798.799 -68.9407 802.236 L-74.9882 802.236 Q-73.3012 798.99 -72.4419 795.839 Q-71.6143 792.688 -71.6143 789.6 Q-71.6143 785.463 -73.0148 783.235 Q-74.4471 781.007 -77.0252 781.007 Q-79.4123 781.007 -80.6855 782.63 Q-81.9586 784.221 -83.1363 789.664 L-83.6137 791.701 Q-84.7595 797.144 -87.1148 799.563 Q-89.502 801.982 -93.6397 801.982 Q-98.6686 801.982 -101.406 798.417 Q-104.143 794.852 -104.143 788.296 Q-104.143 785.049 -103.666 782.184 Q-103.188 779.32 -102.233 776.901 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-85.4279 719.355 Q-91.8891 719.355 -95.5494 722.029 Q-99.2415 724.67 -99.2415 729.317 Q-99.2415 733.964 -95.5494 736.638 Q-91.8891 739.28 -85.4279 739.28 Q-78.9667 739.28 -75.2746 736.638 Q-71.6143 733.964 -71.6143 729.317 Q-71.6143 724.67 -75.2746 722.029 Q-78.9667 719.355 -85.4279 719.355 M-97.8729 739.28 Q-101.056 737.434 -102.583 734.633 Q-104.143 731.8 -104.143 727.885 Q-104.143 721.392 -98.9869 717.35 Q-93.8306 713.276 -85.4279 713.276 Q-77.0252 713.276 -71.869 717.35 Q-66.7127 721.392 -66.7127 727.885 Q-66.7127 731.8 -68.2405 734.633 Q-69.8001 737.434 -72.983 739.28 L-67.6358 739.28 L-67.6358 745.168 L-117.161 745.168 L-117.161 739.28 L-97.8729 739.28 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-86.9239 673.076 L-84.0593 673.076 L-84.0593 700.003 Q-78.0119 699.621 -74.829 696.375 Q-71.678 693.096 -71.678 687.272 Q-71.678 683.898 -72.5055 680.747 Q-73.3331 677.564 -74.9882 674.445 L-69.45 674.445 Q-68.1132 677.596 -67.413 680.906 Q-66.7127 684.216 -66.7127 687.622 Q-66.7127 696.152 -71.678 701.149 Q-76.6432 706.114 -85.1096 706.114 Q-93.8625 706.114 -98.9869 701.404 Q-104.143 696.661 -104.143 688.64 Q-104.143 681.447 -99.4961 677.278 Q-94.881 673.076 -86.9239 673.076 M-88.6426 678.933 Q-93.4487 678.996 -96.3133 681.638 Q-99.1778 684.248 -99.1778 688.577 Q-99.1778 693.478 -96.4088 696.438 Q-93.6397 699.367 -88.6108 699.812 L-88.6426 678.933 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-113.405 657.671 L-103.284 657.671 L-103.284 645.608 L-98.7322 645.608 L-98.7322 657.671 L-79.3805 657.671 Q-75.02 657.671 -73.7787 656.494 Q-72.5374 655.284 -72.5374 651.624 L-72.5374 645.608 L-67.6358 645.608 L-67.6358 651.624 Q-67.6358 658.403 -70.1502 660.981 Q-72.6965 663.56 -79.3805 663.56 L-98.7322 663.56 L-98.7322 667.856 L-103.284 667.856 L-103.284 663.56 L-113.405 663.56 L-113.405 657.671 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-113.405 632.113 L-103.284 632.113 L-103.284 620.05 L-98.7322 620.05 L-98.7322 632.113 L-79.3805 632.113 Q-75.02 632.113 -73.7787 630.935 Q-72.5374 629.726 -72.5374 626.065 L-72.5374 620.05 L-67.6358 620.05 L-67.6358 626.065 Q-67.6358 632.845 -70.1502 635.423 Q-72.6965 638.001 -79.3805 638.001 L-98.7322 638.001 L-98.7322 642.298 L-103.284 642.298 L-103.284 638.001 L-113.405 638.001 L-113.405 632.113 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-86.9239 581.856 L-84.0593 581.856 L-84.0593 608.783 Q-78.0119 608.401 -74.829 605.154 Q-71.678 601.876 -71.678 596.051 Q-71.678 592.677 -72.5055 589.526 Q-73.3331 586.343 -74.9882 583.224 L-69.45 583.224 Q-68.1132 586.375 -67.413 589.685 Q-66.7127 592.996 -66.7127 596.401 Q-66.7127 604.931 -71.678 609.928 Q-76.6432 614.894 -85.1096 614.894 Q-93.8625 614.894 -98.9869 610.183 Q-104.143 605.441 -104.143 597.42 Q-104.143 590.227 -99.4961 586.057 Q-94.881 581.856 -86.9239 581.856 M-88.6426 587.712 Q-93.4487 587.776 -96.3133 590.418 Q-99.1778 593.027 -99.1778 597.356 Q-99.1778 602.258 -96.4088 605.218 Q-93.6397 608.146 -88.6108 608.592 L-88.6426 587.712 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M-97.8092 551.587 Q-98.3821 552.573 -98.6368 553.751 Q-98.9232 554.897 -98.9232 556.297 Q-98.9232 561.263 -95.6767 563.936 Q-92.462 566.578 -86.4146 566.578 L-67.6358 566.578 L-67.6358 572.466 L-103.284 572.466 L-103.284 566.578 L-97.7456 566.578 Q-100.992 564.732 -102.552 561.772 Q-104.143 558.812 -104.143 554.579 Q-104.143 553.974 -104.048 553.242 Q-103.984 552.51 -103.825 551.619 L-97.8092 551.587 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M377.767 12.096 L385.95 12.096 L385.95 72.576 L377.767 72.576 L377.767 12.096 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M439.624 45.1919 L439.624 72.576 L432.171 72.576 L432.171 45.4349 Q432.171 38.994 429.659 35.7938 Q427.148 32.5936 422.124 32.5936 Q416.089 32.5936 412.605 36.4419 Q409.121 40.2903 409.121 46.9338 L409.121 72.576 L401.627 72.576 L401.627 27.2059 L409.121 27.2059 L409.121 34.2544 Q411.795 30.163 415.4 28.1376 Q419.046 26.1121 423.785 26.1121 Q431.604 26.1121 435.614 30.9732 Q439.624 35.7938 439.624 45.1919 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M477.46 9.54393 L477.46 15.7418 L470.33 15.7418 Q466.32 15.7418 464.74 17.3622 Q463.201 18.9825 463.201 23.1955 L463.201 27.2059 L475.475 27.2059 L475.475 32.9987 L463.201 32.9987 L463.201 72.576 L455.706 72.576 L455.706 32.9987 L448.577 32.9987 L448.577 27.2059 L455.706 27.2059 L455.706 24.0462 Q455.706 16.471 459.231 13.0277 Q462.755 9.54393 470.411 9.54393 L477.46 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M501.279 32.4315 Q495.284 32.4315 491.8 37.1306 Q488.316 41.7891 488.316 49.9314 Q488.316 58.0738 491.759 62.7728 Q495.243 67.4314 501.279 67.4314 Q507.234 67.4314 510.718 62.7323 Q514.202 58.0333 514.202 49.9314 Q514.202 41.8701 510.718 37.1711 Q507.234 32.4315 501.279 32.4315 M501.279 26.1121 Q511.001 26.1121 516.551 32.4315 Q522.101 38.7509 522.101 49.9314 Q522.101 61.0714 516.551 67.4314 Q511.001 73.7508 501.279 73.7508 Q491.516 73.7508 485.967 67.4314 Q480.457 61.0714 480.457 49.9314 Q480.457 38.7509 485.967 32.4315 Q491.516 26.1121 501.279 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M560.746 34.1734 Q559.491 33.4443 557.992 33.1202 Q556.533 32.7556 554.751 32.7556 Q548.432 32.7556 545.029 36.8875 Q541.667 40.9789 541.667 48.6757 L541.667 72.576 L534.172 72.576 L534.172 27.2059 L541.667 27.2059 L541.667 34.2544 Q544.016 30.1225 547.784 28.1376 Q551.551 26.1121 556.939 26.1121 Q557.708 26.1121 558.64 26.2337 Q559.572 26.3147 560.706 26.5172 L560.746 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M602.43 35.9153 Q605.225 30.8922 609.114 28.5022 Q613.003 26.1121 618.269 26.1121 Q625.358 26.1121 629.207 31.0947 Q633.055 36.0368 633.055 45.1919 L633.055 72.576 L625.561 72.576 L625.561 45.4349 Q625.561 38.913 623.252 35.7533 Q620.943 32.5936 616.203 32.5936 Q610.41 32.5936 607.048 36.4419 Q603.686 40.2903 603.686 46.9338 L603.686 72.576 L596.192 72.576 L596.192 45.4349 Q596.192 38.8725 593.883 35.7533 Q591.574 32.5936 586.753 32.5936 Q581.041 32.5936 577.679 36.4824 Q574.317 40.3308 574.317 46.9338 L574.317 72.576 L566.823 72.576 L566.823 27.2059 L574.317 27.2059 L574.317 34.2544 Q576.869 30.082 580.434 28.0971 Q583.999 26.1121 588.9 26.1121 Q593.842 26.1121 597.286 28.6237 Q600.769 31.1352 602.43 35.9153 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M668.541 49.7694 Q659.507 49.7694 656.024 51.8354 Q652.54 53.9013 652.54 58.8839 Q652.54 62.8538 655.132 65.2034 Q657.766 67.5124 662.262 67.5124 Q668.46 67.5124 672.187 63.1374 Q675.954 58.7219 675.954 51.4303 L675.954 49.7694 L668.541 49.7694 M683.408 46.6907 L683.408 72.576 L675.954 72.576 L675.954 65.6895 Q673.402 69.8214 669.594 71.8063 Q665.786 73.7508 660.277 73.7508 Q653.31 73.7508 649.178 69.8619 Q645.086 65.9325 645.086 59.3701 Q645.086 51.7138 650.19 47.825 Q655.335 43.9361 665.503 43.9361 L675.954 43.9361 L675.954 43.2069 Q675.954 38.0623 672.551 35.2672 Q669.189 32.4315 663.072 32.4315 Q659.183 32.4315 655.497 33.3632 Q651.811 34.295 648.408 36.1584 L648.408 29.2718 Q652.499 27.692 656.348 26.9223 Q660.196 26.1121 663.842 26.1121 Q673.686 26.1121 678.547 31.2163 Q683.408 36.3204 683.408 46.6907 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M706.133 14.324 L706.133 27.2059 L721.486 27.2059 L721.486 32.9987 L706.133 32.9987 L706.133 57.6282 Q706.133 63.1779 707.632 64.7578 Q709.172 66.3376 713.83 66.3376 L721.486 66.3376 L721.486 72.576 L713.83 72.576 Q705.202 72.576 701.92 69.3758 Q698.639 66.1351 698.639 57.6282 L698.639 32.9987 L693.17 32.9987 L693.17 27.2059 L698.639 27.2059 L698.639 14.324 L706.133 14.324 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M731.289 27.2059 L738.743 27.2059 L738.743 72.576 L731.289 72.576 L731.289 27.2059 M731.289 9.54393 L738.743 9.54393 L738.743 18.9825 L731.289 18.9825 L731.289 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M771.92 32.4315 Q765.925 32.4315 762.441 37.1306 Q758.957 41.7891 758.957 49.9314 Q758.957 58.0738 762.4 62.7728 Q765.884 67.4314 771.92 67.4314 Q777.875 67.4314 781.359 62.7323 Q784.842 58.0333 784.842 49.9314 Q784.842 41.8701 781.359 37.1711 Q777.875 32.4315 771.92 32.4315 M771.92 26.1121 Q781.642 26.1121 787.192 32.4315 Q792.742 38.7509 792.742 49.9314 Q792.742 61.0714 787.192 67.4314 Q781.642 73.7508 771.92 73.7508 Q762.157 73.7508 756.608 67.4314 Q751.098 61.0714 751.098 49.9314 Q751.098 38.7509 756.608 32.4315 Q762.157 26.1121 771.92 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M842.811 45.1919 L842.811 72.576 L835.357 72.576 L835.357 45.4349 Q835.357 38.994 832.846 35.7938 Q830.334 32.5936 825.311 32.5936 Q819.275 32.5936 815.791 36.4419 Q812.308 40.2903 812.308 46.9338 L812.308 72.576 L804.813 72.576 L804.813 27.2059 L812.308 27.2059 L812.308 34.2544 Q814.981 30.163 818.586 28.1376 Q822.232 26.1121 826.972 26.1121 Q834.79 26.1121 838.8 30.9732 Q842.811 35.7938 842.811 45.1919 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M916.699 28.9478 L916.699 35.9153 Q913.54 34.1734 910.339 33.3227 Q907.18 32.4315 903.939 32.4315 Q896.688 32.4315 892.677 37.0496 Q888.667 41.6271 888.667 49.9314 Q888.667 58.2358 892.677 62.8538 Q896.688 67.4314 903.939 67.4314 Q907.18 67.4314 910.339 66.5807 Q913.54 65.6895 916.699 63.9476 L916.699 70.8341 Q913.58 72.2924 910.218 73.0216 Q906.896 73.7508 903.129 73.7508 Q892.88 73.7508 886.844 67.3098 Q880.808 60.8689 880.808 49.9314 Q880.808 38.832 886.885 32.472 Q893.002 26.1121 903.615 26.1121 Q907.058 26.1121 910.339 26.8413 Q913.621 27.5299 916.699 28.9478 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M955.953 34.1734 Q954.697 33.4443 953.198 33.1202 Q951.74 32.7556 949.957 32.7556 Q943.638 32.7556 940.235 36.8875 Q936.873 40.9789 936.873 48.6757 L936.873 72.576 L929.379 72.576 L929.379 27.2059 L936.873 27.2059 L936.873 34.2544 Q939.222 30.1225 942.99 28.1376 Q946.757 26.1121 952.145 26.1121 Q952.914 26.1121 953.846 26.2337 Q954.778 26.3147 955.912 26.5172 L955.953 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M963.771 27.2059 L971.225 27.2059 L971.225 72.576 L963.771 72.576 L963.771 27.2059 M963.771 9.54393 L971.225 9.54393 L971.225 18.9825 L963.771 18.9825 L963.771 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M994.193 14.324 L994.193 27.2059 L1009.55 27.2059 L1009.55 32.9987 L994.193 32.9987 L994.193 57.6282 Q994.193 63.1779 995.692 64.7578 Q997.231 66.3376 1001.89 66.3376 L1009.55 66.3376 L1009.55 72.576 L1001.89 72.576 Q993.261 72.576 989.98 69.3758 Q986.699 66.1351 986.699 57.6282 L986.699 32.9987 L981.23 32.9987 L981.23 27.2059 L986.699 27.2059 L986.699 14.324 L994.193 14.324 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1058.16 48.0275 L1058.16 51.6733 L1023.89 51.6733 Q1024.37 59.3701 1028.5 63.421 Q1032.68 67.4314 1040.09 67.4314 Q1044.38 67.4314 1048.39 66.3781 Q1052.45 65.3249 1056.42 63.2184 L1056.42 70.267 Q1052.4 71.9684 1048.19 72.8596 Q1043.98 73.7508 1039.64 73.7508 Q1028.79 73.7508 1022.43 67.4314 Q1016.11 61.1119 1016.11 50.3365 Q1016.11 39.1965 1022.1 32.6746 Q1028.14 26.1121 1038.35 26.1121 Q1047.5 26.1121 1052.81 32.0264 Q1058.16 37.9003 1058.16 48.0275 M1050.7 45.84 Q1050.62 39.7232 1047.26 36.0774 Q1043.94 32.4315 1038.43 32.4315 Q1032.19 32.4315 1028.42 35.9558 Q1024.7 39.4801 1024.13 45.8805 L1050.7 45.84 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1096.68 34.1734 Q1095.43 33.4443 1093.93 33.1202 Q1092.47 32.7556 1090.69 32.7556 Q1084.37 32.7556 1080.96 36.8875 Q1077.6 40.9789 1077.6 48.6757 L1077.6 72.576 L1070.11 72.576 L1070.11 27.2059 L1077.6 27.2059 L1077.6 34.2544 Q1079.95 30.1225 1083.72 28.1376 Q1087.49 26.1121 1092.87 26.1121 Q1093.64 26.1121 1094.57 26.2337 Q1095.51 26.3147 1096.64 26.5172 L1096.68 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1104.5 27.2059 L1111.95 27.2059 L1111.95 72.576 L1104.5 72.576 L1104.5 27.2059 M1104.5 9.54393 L1111.95 9.54393 L1111.95 18.9825 L1104.5 18.9825 L1104.5 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1148.17 49.7694 Q1139.13 49.7694 1135.65 51.8354 Q1132.17 53.9013 1132.17 58.8839 Q1132.17 62.8538 1134.76 65.2034 Q1137.39 67.5124 1141.89 67.5124 Q1148.09 67.5124 1151.81 63.1374 Q1155.58 58.7219 1155.58 51.4303 L1155.58 49.7694 L1148.17 49.7694 M1163.03 46.6907 L1163.03 72.576 L1155.58 72.576 L1155.58 65.6895 Q1153.03 69.8214 1149.22 71.8063 Q1145.41 73.7508 1139.9 73.7508 Q1132.94 73.7508 1128.8 69.8619 Q1124.71 65.9325 1124.71 59.3701 Q1124.71 51.7138 1129.82 47.825 Q1134.96 43.9361 1145.13 43.9361 L1155.58 43.9361 L1155.58 43.2069 Q1155.58 38.0623 1152.18 35.2672 Q1148.82 32.4315 1142.7 32.4315 Q1138.81 32.4315 1135.12 33.3632 Q1131.44 34.295 1128.04 36.1584 L1128.04 29.2718 Q1132.13 27.692 1135.97 26.9223 Q1139.82 26.1121 1143.47 26.1121 Q1153.31 26.1121 1158.17 31.2163 Q1163.03 36.3204 1163.03 46.6907 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1237.33 49.9314 Q1237.33 41.7081 1233.93 37.0496 Q1230.56 32.3505 1224.65 32.3505 Q1218.73 32.3505 1215.33 37.0496 Q1211.97 41.7081 1211.97 49.9314 Q1211.97 58.1548 1215.33 62.8538 Q1218.73 67.5124 1224.65 67.5124 Q1230.56 67.5124 1233.93 62.8538 Q1237.33 58.1548 1237.33 49.9314 M1211.97 34.0924 Q1214.32 30.0415 1217.88 28.0971 Q1221.49 26.1121 1226.47 26.1121 Q1234.74 26.1121 1239.88 32.6746 Q1245.07 39.2371 1245.07 49.9314 Q1245.07 60.6258 1239.88 67.1883 Q1234.74 73.7508 1226.47 73.7508 Q1221.49 73.7508 1217.88 71.8063 Q1214.32 69.8214 1211.97 65.7705 L1211.97 72.576 L1204.48 72.576 L1204.48 9.54393 L1211.97 9.54393 L1211.97 34.0924 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1276.3 76.7889 Q1273.14 84.8907 1270.14 87.3618 Q1267.14 89.8329 1262.12 89.8329 L1256.17 89.8329 L1256.17 83.5945 L1260.54 83.5945 Q1263.62 83.5945 1265.32 82.1361 Q1267.02 80.6778 1269.09 75.2496 L1270.42 71.8468 L1252.07 27.2059 L1259.97 27.2059 L1274.15 62.6918 L1288.33 27.2059 L1296.23 27.2059 L1276.3 76.7889 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1370.6 45.1919 L1370.6 72.576 L1363.15 72.576 L1363.15 45.4349 Q1363.15 38.994 1360.64 35.7938 Q1358.13 32.5936 1353.1 32.5936 Q1347.07 32.5936 1343.58 36.4419 Q1340.1 40.2903 1340.1 46.9338 L1340.1 72.576 L1332.61 72.576 L1332.61 27.2059 L1340.1 27.2059 L1340.1 34.2544 Q1342.77 30.163 1346.38 28.1376 Q1350.02 26.1121 1354.76 26.1121 Q1362.58 26.1121 1366.59 30.9732 Q1370.6 35.7938 1370.6 45.1919 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1384.7 54.671 L1384.7 27.2059 L1392.15 27.2059 L1392.15 54.3874 Q1392.15 60.8284 1394.67 64.0691 Q1397.18 67.2693 1402.2 67.2693 Q1408.24 67.2693 1411.72 63.421 Q1415.24 59.5726 1415.24 52.9291 L1415.24 27.2059 L1422.7 27.2059 L1422.7 72.576 L1415.24 72.576 L1415.24 65.6084 Q1412.53 69.7404 1408.92 71.7658 Q1405.36 73.7508 1400.62 73.7508 Q1392.8 73.7508 1388.75 68.8897 Q1384.7 64.0286 1384.7 54.671 M1403.46 26.1121 L1403.46 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1473.37 35.9153 Q1476.17 30.8922 1480.06 28.5022 Q1483.95 26.1121 1489.21 26.1121 Q1496.3 26.1121 1500.15 31.0947 Q1504 36.0368 1504 45.1919 L1504 72.576 L1496.51 72.576 L1496.51 45.4349 Q1496.51 38.913 1494.2 35.7533 Q1491.89 32.5936 1487.15 32.5936 Q1481.35 32.5936 1477.99 36.4419 Q1474.63 40.2903 1474.63 46.9338 L1474.63 72.576 L1467.14 72.576 L1467.14 45.4349 Q1467.14 38.8725 1464.83 35.7533 Q1462.52 32.5936 1457.7 32.5936 Q1451.99 32.5936 1448.62 36.4824 Q1445.26 40.3308 1445.26 46.9338 L1445.26 72.576 L1437.77 72.576 L1437.77 27.2059 L1445.26 27.2059 L1445.26 34.2544 Q1447.81 30.082 1451.38 28.0971 Q1454.94 26.1121 1459.84 26.1121 Q1464.79 26.1121 1468.23 28.6237 Q1471.71 31.1352 1473.37 35.9153 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1551.44 49.9314 Q1551.44 41.7081 1548.03 37.0496 Q1544.67 32.3505 1538.76 32.3505 Q1532.84 32.3505 1529.44 37.0496 Q1526.08 41.7081 1526.08 49.9314 Q1526.08 58.1548 1529.44 62.8538 Q1532.84 67.5124 1538.76 67.5124 Q1544.67 67.5124 1548.03 62.8538 Q1551.44 58.1548 1551.44 49.9314 M1526.08 34.0924 Q1528.43 30.0415 1531.99 28.0971 Q1535.6 26.1121 1540.58 26.1121 Q1548.84 26.1121 1553.99 32.6746 Q1559.17 39.2371 1559.17 49.9314 Q1559.17 60.6258 1553.99 67.1883 Q1548.84 73.7508 1540.58 73.7508 Q1535.6 73.7508 1531.99 71.8063 Q1528.43 69.8214 1526.08 65.7705 L1526.08 72.576 L1518.58 72.576 L1518.58 9.54393 L1526.08 9.54393 L1526.08 34.0924 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1610.34 48.0275 L1610.34 51.6733 L1576.07 51.6733 Q1576.55 59.3701 1580.68 63.421 Q1584.86 67.4314 1592.27 67.4314 Q1596.56 67.4314 1600.57 66.3781 Q1604.62 65.3249 1608.59 63.2184 L1608.59 70.267 Q1604.58 71.9684 1600.37 72.8596 Q1596.16 73.7508 1591.82 73.7508 Q1580.97 73.7508 1574.61 67.4314 Q1568.29 61.1119 1568.29 50.3365 Q1568.29 39.1965 1574.28 32.6746 Q1580.32 26.1121 1590.53 26.1121 Q1599.68 26.1121 1604.99 32.0264 Q1610.34 37.9003 1610.34 48.0275 M1602.88 45.84 Q1602.8 39.7232 1599.44 36.0774 Q1596.12 32.4315 1590.61 32.4315 Q1584.37 32.4315 1580.6 35.9558 Q1576.88 39.4801 1576.31 45.8805 L1602.88 45.84 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1648.86 34.1734 Q1647.6 33.4443 1646.11 33.1202 Q1644.65 32.7556 1642.86 32.7556 Q1636.55 32.7556 1633.14 36.8875 Q1629.78 40.9789 1629.78 48.6757 L1629.78 72.576 L1622.29 72.576 L1622.29 27.2059 L1629.78 27.2059 L1629.78 34.2544 Q1632.13 30.1225 1635.9 28.1376 Q1639.66 26.1121 1645.05 26.1121 Q1645.82 26.1121 1646.75 26.2337 Q1647.69 26.3147 1648.82 26.5172 L1648.86 34.1734 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1700.63 32.4315 Q1694.63 32.4315 1691.15 37.1306 Q1687.67 41.7891 1687.67 49.9314 Q1687.67 58.0738 1691.11 62.7728 Q1694.59 67.4314 1700.63 67.4314 Q1706.59 67.4314 1710.07 62.7323 Q1713.55 58.0333 1713.55 49.9314 Q1713.55 41.8701 1710.07 37.1711 Q1706.59 32.4315 1700.63 32.4315 M1700.63 26.1121 Q1710.35 26.1121 1715.9 32.4315 Q1721.45 38.7509 1721.45 49.9314 Q1721.45 61.0714 1715.9 67.4314 Q1710.35 73.7508 1700.63 73.7508 Q1690.87 73.7508 1685.32 67.4314 Q1679.81 61.0714 1679.81 49.9314 Q1679.81 38.7509 1685.32 32.4315 Q1690.87 26.1121 1700.63 26.1121 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1756.78 9.54393 L1756.78 15.7418 L1749.65 15.7418 Q1745.64 15.7418 1744.06 17.3622 Q1742.52 18.9825 1742.52 23.1955 L1742.52 27.2059 L1754.79 27.2059 L1754.79 32.9987 L1742.52 32.9987 L1742.52 72.576 L1735.02 72.576 L1735.02 32.9987 L1727.89 32.9987 L1727.89 27.2059 L1735.02 27.2059 L1735.02 24.0462 Q1735.02 16.471 1738.55 13.0277 Q1742.07 9.54393 1749.73 9.54393 L1756.78 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1822.04 28.9478 L1822.04 35.9153 Q1818.88 34.1734 1815.68 33.3227 Q1812.52 32.4315 1809.28 32.4315 Q1802.02 32.4315 1798.01 37.0496 Q1794 41.6271 1794 49.9314 Q1794 58.2358 1798.01 62.8538 Q1802.02 67.4314 1809.28 67.4314 Q1812.52 67.4314 1815.68 66.5807 Q1818.88 65.6895 1822.04 63.9476 L1822.04 70.8341 Q1818.92 72.2924 1815.55 73.0216 Q1812.23 73.7508 1808.47 73.7508 Q1798.22 73.7508 1792.18 67.3098 Q1786.14 60.8689 1786.14 49.9314 Q1786.14 38.832 1792.22 32.472 Q1798.34 26.1121 1808.95 26.1121 Q1812.39 26.1121 1815.68 26.8413 Q1818.96 27.5299 1822.04 28.9478 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1835 9.54393 L1842.45 9.54393 L1842.45 72.576 L1835 72.576 L1835 9.54393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1878.67 49.7694 Q1869.63 49.7694 1866.15 51.8354 Q1862.67 53.9013 1862.67 58.8839 Q1862.67 62.8538 1865.26 65.2034 Q1867.89 67.5124 1872.39 67.5124 Q1878.59 67.5124 1882.31 63.1374 Q1886.08 58.7219 1886.08 51.4303 L1886.08 49.7694 L1878.67 49.7694 M1893.53 46.6907 L1893.53 72.576 L1886.08 72.576 L1886.08 65.6895 Q1883.53 69.8214 1879.72 71.8063 Q1875.91 73.7508 1870.4 73.7508 Q1863.44 73.7508 1859.3 69.8619 Q1855.21 65.9325 1855.21 59.3701 Q1855.21 51.7138 1860.32 47.825 Q1865.46 43.9361 1875.63 43.9361 L1886.08 43.9361 L1886.08 43.2069 Q1886.08 38.0623 1882.68 35.2672 Q1879.32 32.4315 1873.2 32.4315 Q1869.31 32.4315 1865.62 33.3632 Q1861.94 34.295 1858.53 36.1584 L1858.53 29.2718 Q1862.63 27.692 1866.47 26.9223 Q1870.32 26.1121 1873.97 26.1121 Q1883.81 26.1121 1888.67 31.2163 Q1893.53 36.3204 1893.53 46.6907 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1937.81 28.5427 L1937.81 35.5912 Q1934.65 33.9709 1931.25 33.1607 Q1927.85 32.3505 1924.2 32.3505 Q1918.65 32.3505 1915.85 34.0519 Q1913.1 35.7533 1913.1 39.156 Q1913.1 41.7486 1915.09 43.2475 Q1917.07 44.7058 1923.07 46.0426 L1925.62 46.6097 Q1933.56 48.3111 1936.88 51.4303 Q1940.24 54.509 1940.24 60.0587 Q1940.24 66.3781 1935.22 70.0644 Q1930.24 73.7508 1921.49 73.7508 Q1917.84 73.7508 1913.87 73.0216 Q1909.94 72.3329 1905.57 70.9151 L1905.57 63.2184 Q1909.7 65.3654 1913.71 66.4591 Q1917.72 67.5124 1921.65 67.5124 Q1926.91 67.5124 1929.75 65.73 Q1932.59 63.9071 1932.59 60.6258 Q1932.59 57.5877 1930.52 55.9673 Q1928.49 54.3469 1921.57 52.8481 L1918.97 52.2405 Q1912.05 50.7821 1908.97 47.7845 Q1905.89 44.7463 1905.89 39.4801 Q1905.89 33.0797 1910.43 29.5959 Q1914.96 26.1121 1923.31 26.1121 Q1927.44 26.1121 1931.09 26.7198 Q1934.73 27.3274 1937.81 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M1981.03 28.5427 L1981.03 35.5912 Q1977.87 33.9709 1974.47 33.1607 Q1971.07 32.3505 1967.42 32.3505 Q1961.87 32.3505 1959.08 34.0519 Q1956.32 35.7533 1956.32 39.156 Q1956.32 41.7486 1958.31 43.2475 Q1960.29 44.7058 1966.29 46.0426 L1968.84 46.6097 Q1976.78 48.3111 1980.1 51.4303 Q1983.46 54.509 1983.46 60.0587 Q1983.46 66.3781 1978.44 70.0644 Q1973.46 73.7508 1964.71 73.7508 Q1961.06 73.7508 1957.09 73.0216 Q1953.16 72.3329 1948.79 70.9151 L1948.79 63.2184 Q1952.92 65.3654 1956.93 66.4591 Q1960.94 67.5124 1964.87 67.5124 Q1970.14 67.5124 1972.97 65.73 Q1975.81 63.9071 1975.81 60.6258 Q1975.81 57.5877 1973.74 55.9673 Q1971.72 54.3469 1964.79 52.8481 L1962.2 52.2405 Q1955.27 50.7821 1952.19 47.7845 Q1949.11 44.7463 1949.11 39.4801 Q1949.11 33.0797 1953.65 29.5959 Q1958.19 26.1121 1966.53 26.1121 Q1970.66 26.1121 1974.31 26.7198 Q1977.96 27.3274 1981.03 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M2034.14 48.0275 L2034.14 51.6733 L1999.87 51.6733 Q2000.36 59.3701 2004.49 63.421 Q2008.66 67.4314 2016.07 67.4314 Q2020.37 67.4314 2024.38 66.3781 Q2028.43 65.3249 2032.4 63.2184 L2032.4 70.267 Q2028.39 71.9684 2024.18 72.8596 Q2019.96 73.7508 2015.63 73.7508 Q2004.77 73.7508 1998.41 67.4314 Q1992.09 61.1119 1992.09 50.3365 Q1992.09 39.1965 1998.09 32.6746 Q2004.12 26.1121 2014.33 26.1121 Q2023.49 26.1121 2028.79 32.0264 Q2034.14 37.9003 2034.14 48.0275 M2026.69 45.84 Q2026.61 39.7232 2023.24 36.0774 Q2019.92 32.4315 2014.41 32.4315 Q2008.17 32.4315 2004.41 35.9558 Q2000.68 39.4801 2000.11 45.8805 L2026.69 45.84 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M2075.3 28.5427 L2075.3 35.5912 Q2072.14 33.9709 2068.74 33.1607 Q2065.33 32.3505 2061.69 32.3505 Q2056.14 32.3505 2053.34 34.0519 Q2050.59 35.7533 2050.59 39.156 Q2050.59 41.7486 2052.57 43.2475 Q2054.56 44.7058 2060.55 46.0426 L2063.11 46.6097 Q2071.05 48.3111 2074.37 51.4303 Q2077.73 54.509 2077.73 60.0587 Q2077.73 66.3781 2072.71 70.0644 Q2067.72 73.7508 2058.97 73.7508 Q2055.33 73.7508 2051.36 73.0216 Q2047.43 72.3329 2043.05 70.9151 L2043.05 63.2184 Q2047.19 65.3654 2051.2 66.4591 Q2055.21 67.5124 2059.14 67.5124 Q2064.4 67.5124 2067.24 65.73 Q2070.07 63.9071 2070.07 60.6258 Q2070.07 57.5877 2068.01 55.9673 Q2065.98 54.3469 2059.05 52.8481 L2056.46 52.2405 Q2049.53 50.7821 2046.46 47.7845 Q2043.38 44.7463 2043.38 39.4801 Q2043.38 33.0797 2047.91 29.5959 Q2052.45 26.1121 2060.8 26.1121 Q2064.93 26.1121 2068.57 26.7198 Q2072.22 27.3274 2075.3 28.5427 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip922)" style="stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none" points="
  166.42,297.775 469.656,1129.04 772.893,1050.13 1076.13,868.121 1379.37,707.502 1682.6,499.68 1985.84,281.987 2289.08,103.811 
  "/>
<polyline clip-path="url(#clip922)" style="stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none" points="
  166.42,429.602 469.656,1402.11 772.893,1464.44 1076.13,1423.68 1379.37,1404.3 1682.6,1337.72 1985.84,1261.27 2289.08,1224.34 
  "/>
<path clip-path="url(#clip920)" d="
M1951.18 292.508 L2277.76 292.508 L2277.76 111.068 L1951.18 111.068  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip920)" style="stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none" points="
  1951.18,292.508 2277.76,292.508 2277.76,111.068 1951.18,111.068 1951.18,292.508 
  "/>
<polyline clip-path="url(#clip920)" style="stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none" points="
  1976.18,171.548 2126.18,171.548 
  "/>
<path clip-path="url(#clip920)" d="M 0 0 M2155.86 172.323 L2155.86 184.985 L2163.36 184.985 Q2167.13 184.985 2168.94 183.434 Q2170.76 181.86 2170.76 178.643 Q2170.76 175.402 2168.94 173.874 Q2167.13 172.323 2163.36 172.323 L2155.86 172.323 M2155.86 158.11 L2155.86 168.527 L2162.78 168.527 Q2166.2 168.527 2167.87 167.254 Q2169.56 165.957 2169.56 163.319 Q2169.56 160.703 2167.87 159.407 Q2166.2 158.11 2162.78 158.11 L2155.86 158.11 M2151.18 154.268 L2163.13 154.268 Q2168.47 154.268 2171.37 156.49 Q2174.26 158.712 2174.26 162.809 Q2174.26 165.981 2172.78 167.856 Q2171.3 169.731 2168.43 170.194 Q2171.88 170.934 2173.77 173.295 Q2175.7 175.633 2175.7 179.152 Q2175.7 183.781 2172.55 186.305 Q2169.4 188.828 2163.59 188.828 L2151.18 188.828 L2151.18 154.268 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M2183.7 154.268 L2188.38 154.268 L2188.38 188.828 L2183.7 188.828 L2183.7 154.268 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M2223.57 156.93 L2223.57 161.86 Q2221.2 159.661 2218.52 158.573 Q2215.86 157.485 2212.85 157.485 Q2206.92 157.485 2203.77 161.12 Q2200.63 164.731 2200.63 171.582 Q2200.63 178.411 2203.77 182.045 Q2206.92 185.656 2212.85 185.656 Q2215.86 185.656 2218.52 184.568 Q2221.2 183.481 2223.57 181.281 L2223.57 186.166 Q2221.11 187.832 2218.36 188.666 Q2215.63 189.499 2212.57 189.499 Q2204.72 189.499 2200.21 184.707 Q2195.7 179.893 2195.7 171.582 Q2195.7 163.249 2200.21 158.458 Q2204.72 153.643 2212.57 153.643 Q2215.67 153.643 2218.4 154.476 Q2221.16 155.286 2223.57 156.93 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip920)" style="stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none" points="
  1976.18,232.028 2126.18,232.028 
  "/>
<path clip-path="url(#clip920)" d="M 0 0 M2167.01 219.354 L2160.67 236.553 L2173.38 236.553 L2167.01 219.354 M2164.38 214.748 L2169.68 214.748 L2182.85 249.308 L2177.99 249.308 L2174.84 240.442 L2159.26 240.442 L2156.11 249.308 L2151.18 249.308 L2164.38 214.748 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M2187.89 214.748 L2192.57 214.748 L2192.57 249.308 L2187.89 249.308 L2187.89 214.748 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip920)" d="M 0 0 M2227.76 217.41 L2227.76 222.34 Q2225.39 220.141 2222.71 219.053 Q2220.05 217.965 2217.04 217.965 Q2211.11 217.965 2207.96 221.6 Q2204.82 225.211 2204.82 232.062 Q2204.82 238.891 2207.96 242.525 Q2211.11 246.136 2217.04 246.136 Q2220.05 246.136 2222.71 245.048 Q2225.39 243.961 2227.76 241.761 L2227.76 246.646 Q2225.3 248.312 2222.55 249.146 Q2219.82 249.979 2216.76 249.979 Q2208.91 249.979 2204.4 245.187 Q2199.88 240.373 2199.88 232.062 Q2199.88 223.729 2204.4 218.938 Q2208.91 214.123 2216.76 214.123 Q2219.86 214.123 2222.59 214.956 Q2225.35 215.766 2227.76 217.41 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /></svg>
<p>We see that following the &quot;lowest AIC&quot; rule we would indeed choose three classes, while following the &quot;best AIC&quot; criteria we would have choosen only two classes. This means that there is two classes that, concerning the floreal measures used in the database, are very similar, and opur models are unsure about them. Perhaps the biologists will end up one day with the conclusion that it is indeed only one specie :-).</p><p>We could study this issue more in detail by analysing the <a href="../../Utils.html#BetaML.Utils.ConfusionMatrix"><code>ConfusionMatrix</code></a>, but the one used in BetaML does not account for the ignoreLabels option (yet).</p><h2 id="Benchmarking-computational-efficiency"><a class="docs-heading-anchor" href="#Benchmarking-computational-efficiency">Benchmarking computational efficiency</a><a id="Benchmarking-computational-efficiency-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmarking-computational-efficiency" title="Permalink"></a></h2><p>We now benchmark the time and memory required by the various models by using the <code>@btime</code> macro of the <code>BenchmarkTools</code> package:</p><pre><code class="language-none">@btime kmeans($xs,3);
# 261.540 μs (3777 allocations: 442.53 KiB)
@btime kmedoids($xs,3);
4.576 ms (97356 allocations: 10.42 MiB)
@btime gmm($xs,3,mixtures=[SphericalGaussian() for i in 1:3], verbosity=NONE);
# 5.498 ms (133365 allocations: 8.42 MiB)
@btime gmm($xs,3,mixtures=[DiagonalGaussian() for i in 1:3], verbosity=NONE);
# 18.901 ms (404333 allocations: 25.65 MiB)
@btime gmm($xs,3,mixtures=[FullGaussian() for i in 1:3], verbosity=NONE);
# 49.257 ms (351500 allocations: 61.95 MiB)
@btime Clustering.kmeans($xs&#39;, 3);
# 17.071 μs (23 allocations: 14.31 KiB)
@btime begin dGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:diag); GaussianMixtures.em!(dGMM, $xs) end;
# 530.528 μs (2088 allocations: 488.05 KiB)
@btime begin fGMM = GaussianMixtures.GMM(3, $xs; method=:kmeans, kind=:full); GaussianMixtures.em!(fGMM, $xs) end;
# 4.166 ms (58910 allocations: 3.59 MiB)</code></pre><p>(<em>note: the values reported here are of a local pc, not of the GitHub CI server, as sometimes - depending on data and random initialisation - <code>GaussainMixtures.em!</code><code>fails with a</code>PosDefException`. This in turln would lead the whole documentation to fail to compile</em>)</p><p>Like for supervised models, dedicated models are much better optimized than BetaML models, and are order of magnitude more efficient. However even the slowest BetaML clusering model (gmm using full gaussians) is realtively fast and can handle mid-size datasets (tens to hundreds of thousand records) without significant slow downs.</p><h2 id="Conclusions"><a class="docs-heading-anchor" href="#Conclusions">Conclusions</a><a id="Conclusions-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions" title="Permalink"></a></h2><p>We have shown in this tutorial how we can easily run clustering almgorithms in BetaML with just one line of code <code>choosenModel(x,k)</code>, but also how can we use cross-validation in order to help the model or parameter selection, with or whithout knowing the real classes. We retrieve here what we observed with supervised models. Globally the accuracy of BetaML models are comparable to those of leading specialised packages (in this case they are even better), but there is a significant gap in computational efficiency that restricts the pratical usage of BetaML to mid-size datasets. However we trade this relative inefficiency with very flexible model definition and utility functions (for example the BetaML gmm works with missing data, allowing it to be used as the backbone of the <a href="../../Clustering.html#BetaML.Clustering.predictMissing"><code>predictMissing</code></a> missing imputation function, or for collaborative reccomendation systems).</p><p><a href="https://github.com/sylvaticus/BetaML.jl/blob/master/docs/src/tutorials/Clustering - Iris/betaml_tutorial_cluster_iris.jl">View this file on Github</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Classification - cars/betaml_tutorial_classification_cars.html">« A classification task when labels are known - determining the country of origin of cars given the cars characteristics</a><a class="docs-footer-nextpage" href="../../Perceptron.html">Perceptron »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 19 April 2021 21:41">Monday 19 April 2021</span>. Using Julia version 1.6.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
