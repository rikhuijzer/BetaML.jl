var documenterSearchIndex = {"docs":
[{"location":"Notebooks.html#Notebook-example-1","page":"Notebooks","title":"Notebook example","text":"","category":"section"},{"location":"Notebooks.html#","page":"Notebooks","title":"Notebooks","text":"The following notebooks provide runnable examples of the package functionality:","category":"page"},{"location":"Notebooks.html#","page":"Notebooks","title":"Notebooks","text":"Pegasus classifiers\nNeural Networks\nClustering","category":"page"},{"location":"Notebooks.html#","page":"Notebooks","title":"Notebooks","text":"Note: the live, runnable computational environment is a temporary new copy made at each connection. The first time after a commit is done on this repository a new environment has to be set (instead of just being copied), and the server may take several minutes.","category":"page"},{"location":"Notebooks.html#","page":"Notebooks","title":"Notebooks","text":"This is only if you are the unlucky user triggering the rebuild of the environment after the commit.","category":"page"},{"location":"Nn.html#The-BetaML.Nn-Module-1","page":"Nn","title":"The BetaML.Nn Module","text":"","category":"section"},{"location":"Nn.html#","page":"Nn","title":"Nn","text":"Nn","category":"page"},{"location":"Nn.html#BetaML.Nn","page":"Nn","title":"BetaML.Nn","text":"BetaML.Nn module\n\nImplement the functionality required to define an artificial Neural Network, train it with data, forecast data and assess its performances.\n\nCommon type of layers and optimisation algorithms are already provided, but you can define your own ones subclassing respectively the Layer and OptimisationAlgorithm abstract types.\n\nThe module provide the following type or functions. Use ?[type or function] to access their full signature and detailed documentation:\n\nModel definition:\n\nDenseLayer: Classical feed-forward layer with user-defined activation function\nDenseNoBiasLayer: Classical layer without the bias parameter\nVectorFunctionLayer: Parameterless layer whose activation function run over the ensable of its nodes rather than on each one individually\nbuildNetwork: Build the chained network and define a cost function\ngetParams(nn): Retrieve current weigthts\ngetGradient(nn): Retrieve the current gradient of the weights\nsetParams!(nn): Update the weigths of the network\nshow(nn): Print a representation of the Neural Network\n\nEach layer can use a default activation function, one of the functions provided in the Utils module (relu, tanh, softmax,...) or you can specify your own function. The derivative of the activation function can be optionally be provided, in such case training will be quicker, altought this difference tends to vanish with bigger datasets. You can alternativly implement your own layers defining a new type as subtype of the abstract type Layer. Each user-implemented layer must define the following methods:\n\nA suitable constructor\nforward(layer,x)\nbackward(layer,x,nextGradient)\ngetParams(layer)\ngetGradient(layer,x,nextGradient)\nsetParams!(layer,w)\nsize(layer)\n\nModel training:\n\ntrainingInfo(nn): Default callback function during training\ntrain!(nn):  Training function\nsingleUpdate(θ,▽;optAlg): The parameter update made by the specific optimisation algorithm\nSGD: The default optimisation algorithm\n\nTo define your own optimisation algorithm define a subtype of OptimisationAlgorithm and implement the function singleUpdate(θ,▽;optAlg) specific for it. You can use gradSum, gradSub, gradDiv and gradMul functions to operate on the gradient structure at once.\n\nModel predictions and assessment:\n\npredict(nn): Return the output given the data\nloss(nn): Compute avg. network loss on a test set\nUtils.accuracy(nn): Categorical output accuracy\n\nAll high-level functions (except the low-level ones) expect x and y as (nRecords × nDimensions) matrices.\n\n\n\n\n\n","category":"module"},{"location":"Nn.html#Module-Index-1","page":"Nn","title":"Module Index","text":"","category":"section"},{"location":"Nn.html#","page":"Nn","title":"Nn","text":"Modules = [Nn]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Nn.html#Detailed-API-1","page":"Nn","title":"Detailed API","text":"","category":"section"},{"location":"Nn.html#","page":"Nn","title":"Nn","text":"Modules = [Nn]","category":"page"},{"location":"Nn.html#BetaML.Nn.DenseLayer","page":"Nn","title":"BetaML.Nn.DenseLayer","text":"DenseLayer\n\nRepresentation of a layer in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nwb: Biases (n)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.DenseNoBiasLayer","page":"Nn","title":"BetaML.Nn.DenseNoBiasLayer","text":"DenseNoBiasLayer\n\nRepresentation of a layer without bias in the network\n\nFields:\n\nw:  Weigths matrix with respect to the input from previous layer or data (n x n pr. layer)\nf:  Activation function\ndf: Derivative of the activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.NN","page":"Nn","title":"BetaML.Nn.NN","text":"NN\n\nRepresentation of a Neural Network\n\nFields:\n\nlayers:  Array of layers objects\ncf:      Cost function\ndcf:     Derivative of the cost function\ntrained: Control flag for trained networks\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.SGD","page":"Nn","title":"BetaML.Nn.SGD","text":"SGD\n\nStochastic Gradient Descent algorithm (default)\n\nFields:\n\nη: Learning rate, as a function of the current epoch [def: t -> 1/(1+t)]\nλ: Multiplicative constant to the learning rate [def: 2]\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.VectorFunctionLayer","page":"Nn","title":"BetaML.Nn.VectorFunctionLayer","text":"VectorFunctionLayer\n\nRepresentation of a (weightless) VectorFunction layer in the network. Vector function layer expects a vector activation function, i.e. a function taking the whole output of the previous layer in input rather than working on a single node as \"normal\" activation functions. Useful for example for the SoftMax function.\n\nFields:\n\nnₗ: Number of nodes of the previous layer\nn:  Number of nodes in output\nf:  Activation function (vector)\ndf: Derivative of the (vector) activation function\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#Base.size-Tuple{Layer}","page":"Nn","title":"Base.size","text":"size(layer)\n\nSGet the dimensions of the layers in terms of (dimensions in input , dimensions in output)\n\nNotes:\n\nYou need to use import Base.size before defining this function for your layer\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.backward-Tuple{Layer,Any,Any}","page":"Nn","title":"BetaML.Nn.backward","text":"backward(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer inputs\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.buildNetwork-Tuple{Any,Any}","page":"Nn","title":"BetaML.Nn.buildNetwork","text":"buildNetwork(layers,cf;dcf,name)\n\nInstantiate a new Feedforward Neural Network\n\nParameters:\n\nlayers: Array of layers objects\ncf:     Cost function\ndcf:    Derivative of the cost function [def: nothing]\nname:   Name of the network [def: \"Neural Network\"]\n\nNotes:\n\nEven if the network ends with a single output note, the cost function and its derivative should always expect y and ŷ as column vectors.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.forward-Tuple{Layer,Any}","page":"Nn","title":"BetaML.Nn.forward","text":"forward(layer,x)\n\nPredict the output of the layer given the input\n\nParameters:\n\nlayer:  Worker layer\nx:      Input to the layer\n\nReturn:\n\nAn Array{T,1} of the prediction (even for a scalar)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Tuple{Layer,Any,Any}","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(layer,x,nextGradient)\n\nCompute backpropagation for this layer\n\nParameters:\n\nlayer:        Worker layer\nx:            Input to the layer\nnextGradient: Derivative of the overaall loss with respect to the input of the next layer (output of this layer)\n\nReturn:\n\nThe evaluated gradient of the loss with respect to this layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getParams() and setParams() functions.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{Any,AbstractArray{T,2},AbstractArray{T2,2}}} where T2<:Number where T<:Number","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,xbatch,ybatch)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn:      Worker network\nxbatch:  Input to the network (n,d)\nybatch:  Label input (n,d)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getGradient-Union{Tuple{T2}, Tuple{T}, Tuple{NN,Union{AbstractArray{T,1}, T},Union{AbstractArray{T2,1}, T2}}} where T2<:Number where T<:Number","page":"Nn","title":"BetaML.Nn.getGradient","text":"getGradient(nn,x,y)\n\nRetrieve the current gradient of the weigthts (i.e. derivative of the cost with respect to the weigths)\n\nParameters:\n\nnn: Worker network\nx:   Input to the network (d,1)\ny:   Label input (d,1)\n\n#Notes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{Layer}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(layer)\n\nReturn the number of parameters of a layer.\n\nIt doesn't need to be implemented by each layer type, as it uses getParams().\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getNParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getNParams","text":"getNParams(nn) - Return the number of trainable parameters of the neural network.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{Layer}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(layer)\n\nGet the layers current value of its trainable parameters\n\nParameters:\n\nlayer:  Worker layer\n\nReturn:\n\nThe current value of the layer's trainable parameters as tuple of matrices. It is up to you to decide how to organise this tuple, as long you are consistent with the getGradient() and setParams() functions.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.getParams-Tuple{NN}","page":"Nn","title":"BetaML.Nn.getParams","text":"getParams(nn)\n\nRetrieve current weigthts\n\nParameters:\n\nnn: Worker network\n\nNotes:\n\nThe output is a vector of tuples of each layer's input weigths and bias weigths\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.loss-Tuple{NN,Any,Any}","page":"Nn","title":"BetaML.Nn.loss","text":"loss(fnn,x,y)\n\nCompute avg. network loss on a test set (or a single (1 × d) data point)\n\nParameters:\n\nfnn: Worker network\nx:   Input to the network (n) or (n x d)\ny:   Label input (n) or (n x d)\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{Layer,Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":" setParams!(layer,w)\n\nSet the trainable parameters of the layer with the given values\n\nParameters:\n\nlayer: Worker layer\nw:   The new parameters to set (tuple)\n\nNotes:\n\nThe format of the tuple with the parameters must be consistent with those of the getParams() and getGradient() functions.\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.setParams!-Tuple{NN,Any}","page":"Nn","title":"BetaML.Nn.setParams!","text":"setParams!(nn,w)\n\nUpdate weigths of the network\n\nParameters:\n\nnn: Worker network\nw:  The new weights to set\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.show-Tuple{NN}","page":"Nn","title":"BetaML.Nn.show","text":"show(nn)\n\nPrint a representation of the Neural Network (layers, dimensions..)\n\nParameters:\n\nnn: Worker network\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.singleUpdate-Tuple{Any,Any}","page":"Nn","title":"BetaML.Nn.singleUpdate","text":"singleUpdate(θ,▽;nEpoch,nBatch,batchSize,xbatch,ybatch,optAlg)\n\nPerform the parameters update based on the average batch gradient.\n\nParameters:\n\nθ:         Current parameters\n▽:         Average gradient of the bbatch\nnEpoch:    Numer of epochs\nnBatch:    Number of batches\nbatchSize: Size of each batch\nxbatch:    Data associated to the current batch\nybatch:    Labels associated to the current batch\noptAlg:    The Optimisation algorithm to use for the update\n\nNotes:\n\nThis function is overridden so that each optimisation algorithm implement their\n\nown version\n\nMost parameters are not used by any optimisation algorithm. They are provided\n\nto support the largest possible class of optimisation algorithms\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.train!-Tuple{NN,Any,Any}","page":"Nn","title":"BetaML.Nn.train!","text":"train!(nn,x,y;epochs,batchSize,sequential,optAlg,verbosity,cb)\n\nTrain a neural network with the given x,y data\n\nParameters:\n\nnn:         Worker network\nx:          Training input to the network (records x dimensions)\ny:          Label input (records x dimensions)\nepochs:     Number of passages over the training set [def: 100]\nbatchSize:  Size of each individual batch [def: min(size(x,1),32)]\nsequential: Wether to run all data sequentially instead of random [def: false]\noptAlg:     The optimisation algorithm to update the gradient at each batch [def: SGD]\nverbosity:  A verbosity parameter for the trade off information / efficiency [def: STD]\ncb:         A callback to provide information. [def: trainingInfo]\n\nReturn:\n\nA named tuple with the following information\nepochs: Number of epochs actually ran\nϵ_epochs: The average error on each epoch (if verbosity > LOW)\nθ_epochs: The parameters at each epoch (if verbosity > STD)\n\nNotes:\n\nCurrently supported algorithms:\nSGD (Stochastic) Gradient Descent\nLook at the individual optimisation algorithm (?[Name OF THE ALGORITHM]) for info on its parameter, e.g. ?SGD for the default Stochastic Gradient Descent.\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate(⋅) (type ?singleUpdate for details).\nYou can implement your own callback function, altought the one provided by default is already pretty generic (its output depends on the verbosity parameter). @see trainingInfo for informations on the cb parameters.\nBoth the callback function and the singleUpdate function of the optimisation algorithm can be used to stop the training algorithm, respectively returning true or returning .stop true.\nThe verbosity can be set to any of NONE,LOW,STD,HIGH,FULL.\nThe update is done computing the average gradient for each batch and then calling singleUpdate to let the optimisation algorithm perform the parameters update\n\n\n\n\n\n","category":"method"},{"location":"Nn.html#BetaML.Nn.OptimisationAlgorithm","page":"Nn","title":"BetaML.Nn.OptimisationAlgorithm","text":"OptimisationAlgorithm\n\nAbstract type representing an Optimisation algorithm.\n\nCurrently supported algorithms:\n\nSGD (Stochastic) Gradient Descent\n\nSee ?[Name OF THE ALGORITHM] for their details\n\nYou can implement your own optimisation algorithm using a subtype of OptimisationAlgorithm and implementing its constructor and the update function singleUpdate(⋅) (type ?singleUpdate for details).\n\n\n\n\n\n","category":"type"},{"location":"Nn.html#BetaML.Nn.trainingInfo-Tuple{Any,Any,Any}","page":"Nn","title":"BetaML.Nn.trainingInfo","text":"trainingInfo(nn,x,y;n,batchSize,epochs,verbosity,nEpoch,nBatch)\n\nDefault callback funtion to display information during training, depending on the verbosity level\n\nParameters:\n\nnn: Worker network\nx:  Batch input to the network (batchSize,d)\ny:  Batch label input (batchSize,d)\nn: Size of the full training set\nbatchSize : size of the specific batch just ran\nepochs: Number of epochs defined for the training\nverbosity: Verbosity level defined for the training (NONE,LOW,STD,HIGH,FULL)\nnEpoch: Counter of the current epoch\nnBatch: Counter of the current batch\n\n#Notes:\n\nReporting of the error (loss of the network) is expensive. Use verbosity=NONE for better performances\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#The-BetaML.Clustering-Module-1","page":"Clustering","title":"The BetaML.Clustering Module","text":"","category":"section"},{"location":"Clustering.html#","page":"Clustering","title":"Clustering","text":"Clustering\n","category":"page"},{"location":"Clustering.html#BetaML.Clustering","page":"Clustering","title":"BetaML.Clustering","text":"Clustering module (WIP)\n\nProvide clustering methods and missing values imputation / collaborative filtering / reccomendation systems using clustering methods as backend.\n\nThe module provides the following functions. Use ?[function] to access their full signature and detailed documentation:\n\ninitRepresentatives(X,K;initStrategy,Z₀): Initialisation strategies for Kmean and Kmedoids\n`kmeans(X,K;dist,initStrategy,Z₀)`: Classical KMean algorithm\n`kmedoids(X,K;dist,initStrategy,Z₀)`: Kmedoids algorithm\n`em(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy)`: EM algorithm over GMM\n`predictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance)`: Fill mixing values / collaborative filtering using EM as backbone\n\n{Spherical|Diagonal|Full}Gaussian mixtures for em / predictMissing are already provided. User defined mixtures can be used defining a struct as subtype of Mixture and implementing for that mixture the following functions:\n\ninitMixtures!(mixtures, X; minVariance, minCovariance, initStrategy)\nlpdf(m,x,mask) (for the e-step)\nupdateParameters!(mixtures, X, pₙₖ; minVariance, minCovariance) (the m-step)\n\n\n\n\n\n","category":"module"},{"location":"Clustering.html#Module-Index-1","page":"Clustering","title":"Module Index","text":"","category":"section"},{"location":"Clustering.html#","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Clustering.html#Detailed-API-1","page":"Clustering","title":"Detailed API","text":"","category":"section"},{"location":"Clustering.html#","page":"Clustering","title":"Clustering","text":"Modules = [Clustering]","category":"page"},{"location":"Clustering.html#BetaML.Clustering.em-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.em","text":"em(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance,initStrategy)\n\nCompute Expectation-Maximisation algorithm to identify K clusters of X data, i.e. employ a Generative Mixture Model as the underlying probabilistic model.\n\nX can contain missing values in some or all of its dimensions. In such case the learning is done only with the available data. Implemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (n x d) data to clusterise\nK  :           Number of cluster wanted\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: nothing]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: grid]\n\nReturns:\n\nA named touple of:\npⱼₓ: Matrix of size (N x K) of the probabilities of each point i to belong to cluster j\npⱼ : Probabilities of the categorical distribution (K x 1)\nμ  : Means (K x d) of the Gaussian\nσ² : Variance of the gaussian (K x 1). We assume here that the gaussian has the same variance across all the dimensions\nϵ  : Vector of the discrepancy (matrix norm) between pⱼₓ and the lagged pⱼₓ at each iteration\nlL : The log-likelihood (without considering the last mixture optimisation)\nBIC : The Bayesian Information Criterion (lower is better)\nAIC : The Akaike Information Criterion (lower is better)\n\nNotes:\n\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nReasonable choices for the minVariance/Covariance depends on the mixture. For example 0.25 seems a reasonable value for the SphericalGaussian, 0.05 seems better for the DiagonalGaussian, and FullGaussian seems to prefer either very low values of variance/covariance (e.g. (0.05,0.05) ) or very big but similar ones (e.g. (100,100) ).\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support either grid or kmeans. grid is faster (expecially if X contains missing values), but kmeans often provides better results.\n\nResources:\n\nPaper describing EM with missing values\nClass notes from MITx 6.86x (Sec 15.9)\nLimitations of EM\n\nExample:\n\njulia> clusters = em([1 10.5;1.5 0; 1.8 8; 1.7 15; 3.2 40; 0 0; 3.3 38; 0 -2.3; 5.2 -2.4],3,verbosity=HIGH)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.initMixtures!-Union{Tuple{T}, Tuple{Array{T,1},Any}} where T<:BetaML.Clustering.AbstractGaussian","page":"Clustering","title":"BetaML.Clustering.initMixtures!","text":"initMixtures!(mixtures::Array{T,1}, X; minVariance=0.25, minCovariance=0.0, initStrategy=\"grid\")\n\nTThe parameter initStrategy can be either grid or kmeans.  If the data contains missing values, a first run of predictMissing is done under init=grid to impute the missing values just to allow the kmeans algorithm. Then the em algorithm is used with the output of kmean as init values.\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.initRepresentatives-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.initRepresentatives","text":"initRepresentatives(X,K;initStrategy,Z₀)\n\nInitialisate the representatives for a K-Mean or K-Medoids algorithm\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ninitStrategy: Wheter to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\n\nReturns:\n\nA (K x D) matrix of initial representatives\n\nExample:\n\njulia> Z₀ = initRepresentatives([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.6 38],2,initStrategy=\"given\",Z₀=[1.7 15; 3.6 40])\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmeans-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.kmeans","text":"kmeans(X,K;dist,initStrategy,Z₀)\n\nCompute K-Mean algorithm to identify K clusters of X using Euclidean distance\n\nParameters:\n\nX: a (N x D) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Wheter to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach [default]\nshuffle: selecting randomly within the available points\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmeans([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.kmedoids-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.kmedoids","text":"kmedoids(X,K;dist,initStrategy,Z₀)\n\nCompute K-Medoids algorithm to identify K clusters of X using distance definition dist\n\nParameters:\n\nX: a (n x d) data to clusterise\nK: Number of cluster wonted\ndist: Function to employ as distance (see notes). Default to Euclidean distance.\ninitStrategy: Wheter to select the initial representative vectors:\nrandom: randomly in the X space\ngrid: using a grid approach\nshuffle: selecting randomly within the available points [default]\ngiven: using a provided set of initial representatives provided in the Z₀ parameter\nZ₀: Provided (K x D) matrix of initial representatives (used only together with the given initStrategy) [default: nothing]\n\nReturns:\n\nA tuple of two items, the first one being a vector of size N of ids of the clusters associated to each point and the second one the (K x D) matrix of representatives\n\nNotes:\n\nSome returned clusters could be empty\nThe dist parameter can be:\nAny user defined function accepting two vectors and returning a scalar\nAn anonymous function with the same characteristics (e.g. dist = (x,y) -> norm(x-y)^2)\nOne of the above predefined distances: l1_distance, l2_distance, l2²_distance, cosine_distance\n\nExample:\n\njulia> (clIdx,Z) = kmedoids([1 10.5;1.5 10.8; 1.8 8; 1.7 15; 3.2 40; 3.6 32; 3.3 38; 5.1 -2.3; 5.2 -2.4],3,initStrategy=\"grid\")\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{DiagonalGaussian,Any,Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::DiagonalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{FullGaussian,Any,Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::FullGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.lpdf-Tuple{SphericalGaussian,Any,Any}","page":"Clustering","title":"BetaML.Clustering.lpdf","text":"lpdf(m::SphericalGaussian,x,mask) - Log PDF of the mixture given the observation x\n\n\n\n\n\n","category":"method"},{"location":"Clustering.html#BetaML.Clustering.predictMissing-Tuple{Any,Any}","page":"Clustering","title":"BetaML.Clustering.predictMissing","text":"predictMissing(X,K;p₀,mixtures,tol,verbosity,minVariance,minCovariance)\n\nFill missing entries in a sparse matrix assuming an underlying Gaussian Mixture probabilistic Model (GMM) and implementing an Expectation-Maximisation algorithm.\n\nWhile the name of the function is predictMissing, the function can be used also for system reccomendation / collaborative filtering and GMM-based regressions.\n\nImplemented in the log-domain for better numerical accuracy with many dimensions.\n\nParameters:\n\nX  :           A (N x D) sparse matrix of data to fill according to a GMM model\nK  :           Number of mixtures desired\np₀ :           Initial probabilities of the categorical distribution (K x 1) [default: nothing]\nmixtures:      An array (of length K) of the mixture to employ (see notes) [def: [DiagonalGaussian() for i in 1:K]]\ntol:           Tolerance to stop the algorithm [default: 10^(-6)]\nverbosity:     A verbosity parameter regulating the information messages frequency [def: STD]\nminVariance:   Minimum variance for the mixtures [default: 0.05]\nminCovariance: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minVariance (see notes).\ninitStrategy:  Mixture initialisation algorithm [def: grid]\n\nReturns:\n\nA named touple of:\n̂X̂    : The Filled Matrix of size (N x D)\nnFill: The number of items filled\nlL   : The log-likelihood (without considering the last mixture optimisation)\nBIC :  The Bayesian Information Criterion (lower is better)\nAIC :  The Akaike Information Criterion (lower is better)\nNotes:\nThe mixtures currently implemented are SphericalGaussian(μ,σ²),DiagonalGaussian(μ,σ²) and FullGaussian(μ,σ²)\nFor initStrategy, look at the documentation of initMixtures! for the mixture you want. The provided gaussian mixtures support either grid or kmeans. grid is faster, but kmeans often provides better results.\n\nExample:\n\njulia>  cFOut = predictMissing([1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4],3)\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#The-BetaML.Perceptron-Module-1","page":"Perceptron","title":"The BetaML.Perceptron Module","text":"","category":"section"},{"location":"Perceptron.html#","page":"Perceptron","title":"Perceptron","text":"Perceptron","category":"page"},{"location":"Perceptron.html#BetaML.Perceptron","page":"Perceptron","title":"BetaML.Perceptron","text":"Perceptron module\n\nProvide linear and kernel classifiers.\n\nSee a runnable example on myBinder\n\nperceptron: Train data using the classical perceptron\nkernelPerceptron: Train data using the kernel perceptron\npegasus: Train data using the pegasus algorithm\npredict: Predict data using parameters from one of the above algorithms\n\n\n\n\n\n","category":"module"},{"location":"Perceptron.html#Module-Index-1","page":"Perceptron","title":"Module Index","text":"","category":"section"},{"location":"Perceptron.html#","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Perceptron.html#Detailed-API-1","page":"Perceptron","title":"Detailed API","text":"","category":"section"},{"location":"Perceptron.html#","page":"Perceptron","title":"Perceptron","text":"Modules = [Perceptron]","category":"page"},{"location":"Perceptron.html#BetaML.Perceptron.kernelPerceptron-Tuple{Any,Any}","page":"Perceptron","title":"BetaML.Perceptron.kernelPerceptron","text":"kernelPerceptron(x,y;K,T,α,nMsgs,rShuffle)\n\nTrain a Kernel Perceptron algorithm based on x and y\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\ny:        Associated labels of the training data, in the format of ⨦ 1\nK:        Kernel function to emplpy. See ?radialKernel or ?polynomialKernelfor details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radialKernel]\nT:        Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nα:        Initial distribution of the errors [def: zeros(length(y))]\nnMsg:     Maximum number of messages to show if all iterations are done\nrShuffle: Wheter to randomly shuffle the data at each iteration [def: false]\n\nReturn a named tuple with:\n\nx: the x data (eventually shuffled if rShuffle=true)\ny: the label\nα: the errors associated to each record\nerrors: the number of errors in the last iteration\nbesterrors: the minimum number of errors in classifying the data ever reached\niterations: the actual number of iterations performed\nseparated: a flag if the data has been successfully separated\n\nNotes:\n\nThe trained data can then be used to make predictions using the function predict(). If the option randomShuffle has been used, it is important to use there the returned (x,y,α) as these would have been shuffle compared with the original (x,y).\n\nExample:\n\njulia> kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.pegasus-Tuple{Any,Any}","page":"Perceptron","title":"BetaML.Perceptron.pegasus","text":"pegasus(x,y;θ,θ₀,λ,η,T,nMsgs,rShuffle,forceOrigin)\n\nTrain the peagasus algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant term [def: 0]\nλ:           Multiplicative term of the learning rate\nη:           Learning rate [def: (t -> 1/sqrt(t))]\nT:           Maximum number of iterations across the whole set (if the set is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nrShuffle:    Wheter to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Wheter to force θ₀ to remain zero [def: false]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> pegasus([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.perceptron-Tuple{Any,Any}","page":"Perceptron","title":"BetaML.Perceptron.perceptron","text":"perceptron(x,y;θ,θ₀,T,nMsgs,rShuffle,forceOrigin)\n\nTrain a perceptron algorithm based on x and y (labels)\n\nParameters:\n\nx:           Feature matrix of the training data (n × d)\ny:           Associated labels of the training data, in the format of ⨦ 1\nθ:           Initial value of the weights (parameter) [def: zeros(d)]\nθ₀:          Initial value of the weight (parameter) associated to the constant                term [def: 0]\nT:           Maximum number of iterations across the whole set (if the set                is not fully classified earlier) [def: 1000]\nnMsg:        Maximum number of messages to show if all iterations are done\nrShuffle:    Wheter to randomly shuffle the data at each iteration [def: false]\nforceOrigin: Wheter to force θ₀ to remain zero [def: false]\n\nReturn a named tuple with:\n\nθ:          The final weights of the classifier\nθ₀:         The final weight of the classifier associated to the constant term\navgθ:       The average weights of the classifier\navgθ₀:      The average weight of the classifier associated to the constant term\nerrors:     The number of errors in the last iteration\nbesterrors: The minimum number of errors in classifying the data ever reached\niterations: The actual number of iterations performed\nseparated:  Weather the data has been successfully separated\n\nNotes:\n\nThe trained parameters can then be used to make predictions using the function predict().\n\nExample:\n\njulia> perceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])\n\n\n\n\n\n","category":"method"},{"location":"Perceptron.html#BetaML.Perceptron.predict","page":"Perceptron","title":"BetaML.Perceptron.predict","text":"predict(x,θ,θ₀)\n\nPredict a binary label {-1,1} given the feature vector and the linear coefficients\n\nParameters:\n\nx:        Feature matrix of the training data (n × d)\nθ:        The trained parameters\nθ₀:       The trained bias barameter [def: 0]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"function"},{"location":"Perceptron.html#BetaML.Perceptron.predict-NTuple{4,Any}","page":"Perceptron","title":"BetaML.Perceptron.predict","text":"predict(x,xtrain,ytrain,α;K)\n\nPredict a binary label {-1,1} given the feature vector and the training data together with their errors (as trained by a kenrnel perceptron algorithm)\n\nParameters:\n\nx:      Feature matrix of the training data (n × d)\nxtrain: The feature vectors used for the training\nytrain: The labels of the training set\nα:      The errors associated to each record\nK:      The kernel function used for the training and to be used for the prediction [def: radialKernel]\n\nReturn :\n\ny: Vector of the predicted labels\n\nExample:\n\njulia> predict([1.1 2.1; 5.3 4.2; 1.8 1.7], [3.2,1.2])\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#The-BetaML.Utils-Module-1","page":"Utils","title":"The BetaML.Utils Module","text":"","category":"section"},{"location":"Utils.html#","page":"Utils","title":"Utils","text":"Utils\n","category":"page"},{"location":"Utils.html#BetaML.Utils","page":"Utils","title":"BetaML.Utils","text":"Utils module\n\nProvide shared utility functions for various machine learning algorithms. You don't usually need to import from this module, as each other module (Nn, Perceptron, Clusters,...) reexport it.\n\n\n\n\n\n","category":"module"},{"location":"Utils.html#Module-Index-1","page":"Utils","title":"Module Index","text":"","category":"section"},{"location":"Utils.html#","page":"Utils","title":"Utils","text":"Modules = [Utils]\nOrder   = [:constant, :type, :function, :macro]","category":"page"},{"location":"Utils.html#Detailed-API-1","page":"Utils","title":"Detailed API","text":"","category":"section"},{"location":"Utils.html#","page":"Utils","title":"Utils","text":"Modules = [Utils]","category":"page"},{"location":"Utils.html#Base.error-Tuple{Array{Int64,1},Array{Int64,1}}","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error (Int vs Int)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Array{T,1},Int64}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic prediction of a single datapoint (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.error-Union{Tuple{T}, Tuple{Array{T,2},Array{Int64,1}}} where T<:Number","page":"Utils","title":"Base.error","text":"error(ŷ,y) - Categorical error with probabilistic predictions of a dataset (PMF vs Int). \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#Base.reshape-Union{Tuple{T}, Tuple{T,Vararg{Any,N} where N}} where T<:Number","page":"Utils","title":"Base.reshape","text":"reshape(myNumber, dims..) - Reshape a number as a n dimensional Array \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Tuple{Array{Int64,1},Array{Int64,1}}","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y) - Categorical accuracy (Int vs Int)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Array{T,1},Int64}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol)\n\nCategorical accuracy with probabilistic prediction of a single datapoint (PMF vs Int).\n\nUse the parameter tol [def: 1] to determine the tollerance of the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.accuracy-Union{Tuple{T}, Tuple{Array{T,2},Array{Int64,1}}} where T<:Number","page":"Utils","title":"BetaML.Utils.accuracy","text":"accuracy(ŷ,y;tol,ignoreLabels)\n\nCategorical accuracy with probabilistic predictions of a dataset (PMF vs Int).\n\nParameters:\n\nŷ: An (N,K) matrix of probabilities that each hat y_n record with n in 1N  being of category k with k in 1K.\ny: The N array with the correct category for each point n.\ntol: The tollerance to the prediction, i.e. if considering \"correct\" only a prediction where the value with highest probability is the true value (tol = 1), or consider instead the set of tol maximum values [def: 1].\nignoreLabels: Wheter to ignore the specific label order in y. Useful for unsupervised learning algorithms where the specific label order don't make sense [def: false]\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.aic-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.aic","text":"aic(lL,k) -  Akaike information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.autoJacobian-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.autoJacobian","text":"autoJacobian(f,x;nY)\n\nEvaluate the Jacobian using AD in the form of a (nY,nX) madrix of first derivatives\n\nParameters:\n\nf: The function to compute the Jacobian\nx: The input to the function where the jacobian has to be computed\nnY: The number of outputs of the function f [def: length(f(x))]\n\nReturn values:\n\nAn Array{Float64,2} of the locally evaluated Jacobian\n\nNotes:\n\nThe nY parameter is optional. If provided it avoids having to compute f(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.batch-Tuple{Integer,Integer}","page":"Utils","title":"BetaML.Utils.batch","text":"batch(n,bSize;sequential=false)\n\nReturn a vector of bSize indeces from 1 to n. Randomly unless the optional parameter sequential is used.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.bic-Tuple{Any,Any,Any}","page":"Utils","title":"BetaML.Utils.bic","text":"bic(lL,k,n) -  Bayesian information criterion (lower is better)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.celu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.celu","text":"celu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.cosine_distance-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.cosine_distance","text":"Cosine distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dcelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dcelu","text":"dcelu(x; α=1) \n\nhttps://arxiv.org/pdf/1704.07483.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.delu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.delu","text":"delu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dmish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dmish","text":"dmish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dplu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dplu","text":"dplu(x;α=0.1,c=1) \n\nPiecewise Linear Unit derivative \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.drelu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.drelu","text":"drelu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsigmoid","text":"dsigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftmax","text":"dsoftmax(x; β=1) \n\nDerivative of the softmax function \n\nhttps://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dsoftplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dsoftplus","text":"dsoftplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.dtanh-Tuple{Any}","page":"Utils","title":"BetaML.Utils.dtanh","text":"dtanh(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.elu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.elu","text":"elu(x; α=1) with α > 0 \n\nhttps://arxiv.org/pdf/1511.07289.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.getScaleFactors-Tuple{Any}","page":"Utils","title":"BetaML.Utils.getScaleFactors","text":"getScaleFactors(x;skip)\n\nReturn the scale factors (for each dimensions) in order to scale a matrix X (n,d) such that each dimension has mean 0 and variance 1.\n\nParameters\n\nx: the (n × d) dimension matrix to scale on each dimension d\nskip: an array of dimension index to skip the scaling [def: []]\n\nReturn\n\nA touple whose first elmement is the shift and the second the multiplicative\n\nterm to make the scale.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l1_distance-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.l1_distance","text":"L1 norm distance (aka Manhattan Distance)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2_distance-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.l2_distance","text":"Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.l2²_distance-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.l2²_distance","text":"Squared Euclidean (L2) distance\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.logNormalFixedSd-Tuple{Any,Any,Any}","page":"Utils","title":"BetaML.Utils.logNormalFixedSd","text":"log-PDF of a multidimensional normal with no covariance and shared variance across dimensions\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.lse-Tuple{Any}","page":"Utils","title":"BetaML.Utils.lse","text":"LogSumExp for efficiently computing log(sum(exp.(x))) \n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.makeMatrix-Tuple{AbstractArray}","page":"Utils","title":"BetaML.Utils.makeMatrix","text":"Transform an Array{T,1} in an Array{T,2} and leave unchanged Array{T,2}.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.meanRelError-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.meanRelError","text":"meanRelError(ŷ,y;normDim=true,normRec=true,p=1)\n\nCompute the mean relative error (l-1 based by default) between ŷ and y.\n\nThere are many ways to compute a mean relative error. In particular, if normRec (normDim) is set to true, the records (dimensions) are normalised, in the sense that it doesn't matter if a record (dimension) is bigger or smaller than the others, the relative error is first computed for each record (dimension) and then it is averaged. With both normDim and normRec set to false the function returns the relative mean error; with both set to true (default) it returns the mean relative error (i.e. with p=1 the \"mean absolute percentage error (MAPE)\") The parameter p [def: 1] controls the p-norm used to define the error.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.mish-Tuple{Any}","page":"Utils","title":"BetaML.Utils.mish","text":"mish(x) \n\nhttps://arxiv.org/pdf/1908.08681v1.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.normalFixedSd-Tuple{Any,Any,Any}","page":"Utils","title":"BetaML.Utils.normalFixedSd","text":"PDF of a multidimensional normal with no covariance and shared variance across dimensions\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.oneHotEncoder","page":"Utils","title":"BetaML.Utils.oneHotEncoder","text":"oneHotEncoder(y,d;count)\n\nEncode arrays (or arrays of arrays) of integer data as 0/1 matrices\n\nParameters:\n\ny: The data to convert (integer, array or array of arrays of integers)\nd: The number of dimensions in the output matrik. [def: maximum(maximum.(Y))]\ncount: Wether to count multiple instances on the same dimension/record or indicate just presence. [def: false]\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.pca-Tuple{Any}","page":"Utils","title":"BetaML.Utils.pca","text":"pca(X;K,error)\n\nPerform Principal Component Analysis returning the matrix reprojected among the dimensions of maximum variance.\n\nParameters:\n\nX : The (N,D) data to reproject\nK : The number of dimensions to maintain (with K<=D) [def: nothing]\nerror: The maximum approximation error that we are willing to achieve [def: 0.05]\n\nReturn:\n\nA named tuple with:\nX: The reprojected (NxK) matrix\nK: The dimensions retieved\nerror: The actual proportion of variance not explained in the reprojected dimensions\nP: The (D,K) matrix of the eigenvalues associated to the K-largest eigenvalues used to reproject the data matrix\n\nNote that if K is indicated, the parameter error has no effect.\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.plu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.plu","text":"plu(x;α=0.1,c=1) \n\nPiecewise Linear Unit \n\nhttps://arxiv.org/pdf/1809.09534.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.polynomialKernel-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.polynomialKernel","text":"Polynomial kernel parametrised with c=0 and d=2 (i.e. a quadratic kernel). For other cᵢ and dᵢ use K = (x,y) -> polynomialKernel(x,y,c=cᵢ,d=dᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.radialKernel-Tuple{Any,Any}","page":"Utils","title":"BetaML.Utils.radialKernel","text":"Radial Kernel (aka RBF kernel) parametrised with γ=1/2. For other gammas γᵢ use K = (x,y) -> radialKernel(x,y,γ=γᵢ) as kernel function in the supporting algorithms\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.relu-Tuple{Any}","page":"Utils","title":"BetaML.Utils.relu","text":"relu(x) \n\nRectified Linear Unit \n\nhttps://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.scale","page":"Utils","title":"BetaML.Utils.scale","text":"scale(x,scaleFactors;rev)\n\nPerform a linear scaling of x using scaling factors scaleFactors.\n\nParameters\n\nx: The (n × d) dimension matrix to scale on each dimension d\nscalingFactors: A tuple of the constant and multiplicative scaling factor\n\nrespectively [def: the scaling factors needed to scale x to mean 0 and variance 1]\n\nrev: Wheter to invert the scaling [def: false]\n\nReturn\n\nThe scaled matrix\n\nNotes:\n\nAlso available scale!(x,scaleFactors) for in-place scaling.\nRetrieve the scale factors with the getScaleFactors() function\n\n\n\n\n\n","category":"function"},{"location":"Utils.html#BetaML.Utils.sigmoid-Tuple{Any}","page":"Utils","title":"BetaML.Utils.sigmoid","text":"sigmoid(x)\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softmax-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softmax","text":"softmax (x; β=1) \n\nThe input x is a vector. Return a PMF\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.softplus-Tuple{Any}","page":"Utils","title":"BetaML.Utils.softplus","text":"softplus(x) \n\nhttps://en.wikipedia.org/wiki/Rectifier(neuralnetworks)#Softplus\n\n\n\n\n\n","category":"method"},{"location":"Utils.html#BetaML.Utils.sterling-Tuple{BigInt,BigInt}","page":"Utils","title":"BetaML.Utils.sterling","text":"Sterling number: number of partitions of a set of n elements in k sets \n\n\n\n\n\n","category":"method"},{"location":"index.html#![BLogos](assets/BetaML_logo_30x30.png)-BetaML.jl-Documentation-1","page":"Index","title":"(Image: BLogos) BetaML.jl Documentation","text":"","category":"section"},{"location":"index.html#","page":"Index","title":"Index","text":"Welcome to the documentation of the Beta Machine Learning toolkit.","category":"page"},{"location":"index.html#Installation-1","page":"Index","title":"Installation","text":"","category":"section"},{"location":"index.html#","page":"Index","title":"Index","text":"The BetaML package is now included in the standard Julia register, install it with:","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"] add BetaML","category":"page"},{"location":"index.html#Loading-the-module(s)-1","page":"Index","title":"Loading the module(s)","text":"","category":"section"},{"location":"index.html#","page":"Index","title":"Index","text":"This package is split in several submodules. You can access its functionality either by using the specific submodule of interest and then directly the provided functionality (utilities are re-exported by each of the other submodules, so normally you don't need to implicitly import them) or by using the root module BetaML and then prefix with BetaML. each object/function you want to use, e.g.:","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"using BetaML.Nn\nmyLayer = DenseLayer(2,3)","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"or, equivalently,","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"using BetaML\nres = BetaML.kernelPerceptron([1.1 2.1; 5.3 4.2; 1.8 1.7], [-1,1,-1])","category":"page"},{"location":"index.html#Usage-1","page":"Index","title":"Usage","text":"","category":"section"},{"location":"index.html#","page":"Index","title":"Index","text":"Documentation for most algorithms can be retrieved using the inline Julia help system (just press the question mark ? and then, on the special help prompt help?>, type the function name).","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"For a list of supported algorithms please look at the individual modules:","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"BetaML.Perceptron: Perform classification tasks using the Perceptron, Kernel Perceptron or Pegasus algorithms\nBetaML.Nn: Implementation of Artificial Neural Networks\nBetaML.Clustering`: Clustering algorithms (Kmeans, Mdedoids, EM/GMM) and missing imputation / collaborative filtering / recommandation systems using clusters\nBetaML.Utils`: Various utility functions (scale, one-hot, distances, kernels, pca, accuracy/error measures..)","category":"page"},{"location":"index.html#Examples-1","page":"Index","title":"Examples","text":"","category":"section"},{"location":"index.html#","page":"Index","title":"Index","text":"Using an Artificial Neural Network for multinomial categorisation","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"# Load Modules\nusing BetaML.Nn, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(Base.find_package(\"BetaML\")),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\ny_oh     = oneHotEncoder(y) # Convert to One-hot representation (e.g. 2 => [0 1 0], 3 => [0 0 1])\n\n# Split the data in training/testing sets\nntrain    = Int64(round(size(x,1)*0.8))\nxtrain    = x[1:ntrain,:]\nytrain    = y[1:ntrain]\nytrain_oh = y_oh[1:ntrain,:]\nxtest     = x[ntrain+1:end,:]\nytest     = y[ntrain+1:end]\n\n# Define the Artificial Neural Network model\nl1   = DenseLayer(4,10,f=relu) # Activation function is ReLU\nl2   = DenseLayer(10,3)        # Activation function is identity by default\nl3   = VectorFunctionLayer(3,3,f=softMax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once\nmynn = buildNetwork([l1,l2,l3],squaredCost,name=\"Multinomial logistic regression Model Sepal\") # Build the NN and use the squared cost (aka MSE) as error function\n\n# Training it (default to SGD)\nres = train!(mynn,scale(xtrain),ytrain_oh,epochs=100,batchSize=6) # Use optAlg=SGD (Stochastic Gradient Descent) by default\n\n# Test it\nŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function\nŷtest         = predict(mynn,scale(xtest))\ntrainAccuracy = accuracy(ŷtrain,ytrain,tol=1) # 0.983\ntestAccuracy  = accuracy(ŷtest,ytest,tol=1)   # 1.0\n\n# Visualise results\ntestSize = size(ŷtest,1)\nŷtestChosen =  [argmax(ŷtest[i,:]) for i in 1:testSize]\ngroupedbar([ytest ŷtestChosen], label=[\"ytest\" \"ŷtest (est)\"], title=\"True vs estimated categories\") # All records correctly labelled !\nplot(0:res.epochs,res.ϵ_epochs, ylabel=\"epochs\",xlabel=\"error\",legend=nothing,title=\"Avg. error per epoch on the Sepal dataset\")","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"Using the Expectation-Maximisation algorithm for clustering","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"using BetaML.Clustering, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules\nRandom.seed!(123); # Fix the random seed (to obtain reproducible results)\n\n# Load the data\niris     = readdlm(joinpath(dirname(Base.find_package(\"BetaML\")),\"..\",\"test\",\"data\",\"iris.csv\"),',',skipstart=1)\niris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default\nx        = convert(Array{Float64,2}, iris[:,1:4])\nx        = scale(x) # normalise all dimensions to (μ=0, σ=1)\ny        = map(x->Dict(\"setosa\" => 1, \"versicolor\" => 2, \"virginica\" =>3)[x],iris[:, 5]) # Convert the target column to numbers\n\n# Get some ranges of minVariance and minCovariance to test\nminVarRange = collect(0.04:0.05:1.5)\nminCovarRange = collect(0:0.05:1.45)\n\n# Run the em algorithm for the various cases...\nsphOut  = [em(x,3,mixtures=[SphericalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE) for v in minVarRange, cv in minCovarRange[1:1]]\ndiagOut  = [em(x,3,mixtures=[DiagonalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange[1:1]]\nfullOut = [em(x,3,mixtures=[FullGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange]\n\n# Get the Bayesian information criterion (AIC is also available)\nsphBIC = [sphOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\ndiagBIC = [diagOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]\nfullBIC = [fullOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\n# Compare the accuracy with true categories\nsphAcc  = [accuracy(sphOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\ndiagAcc = [accuracy(diagOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]\nfullAcc = [accuracy(fullOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]\n\nplot(minVarRange,[sphBIC diagBIC fullBIC[:,1] fullBIC[:,15] fullBIC[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"BIC\", xlabel=\"minVariance\")\nplot(minVarRange,[sphAcc diagAcc fullAcc[:,1] fullAcc[:,15] fullAcc[:,30]], markershape=:circle, label=[\"sph\" \"diag\" \"full (cov=0)\" \"full (cov=0.7)\" \"full (cov=1.45)\"], title=\"Accuracies\", xlabel=\"minVariance\")","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"(Image: results)","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"Further examples","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"We also provide some Jupyter notebooks that can be run online without installing anything, so you can start playing with the library in minutes.","category":"page"},{"location":"index.html#Acknowledgements-1","page":"Index","title":"Acknowledgements","text":"","category":"section"},{"location":"index.html#","page":"Index","title":"Index","text":"The development of this package at the Bureau d'Economie Théorique et Appliquée (BETA, Nancy) was supported by the French National Research Agency through the Laboratory of Excellence ARBRE, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).","category":"page"},{"location":"index.html#","page":"Index","title":"Index","text":"(Image: BLogos)","category":"page"}]
}
